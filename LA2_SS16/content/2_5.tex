%!TEX root = ../LA2.tex
\section{Sesquilinearformen und selbstadjungierte Endomorphismen}
\label{sec:2.5}

In diesem Abschnitt sei wieder $\KK = \RR$ oder $\KK= \CC$.

\begin{definition}[Sesquilinearform, Bilinearform]
	\label{def:5.1}
	Seien $V$ und $W$ $\KK$-Vektorräume.
	Eine \Index{Sesquilinearform} auf $V \times W$ ist eine Abbildung $f \colon V \times W \rightarrow \KK$, die linear in der ersten und konjugiert linear (oder antilinear) in der zweiten Variablen ist.
	Das heißt, für $\lambda, \mu \in \KK$, $v_1,v_2,v \in V$ und $w_1,w_2,w \in W$ gilt:
	\begin{align*}
		f(\lambda v_1 + v_2, w) &= \lambda f(v_1,w) + f(v_2,w) \\
		f(v,\mu w_1 + w_2) &= \ol{\mu} f(v,w_1) + f(v,w_2)
	\end{align*}
	Ist $\KK= \RR$, so ist $f$ also linear in beiden Variablen.
	Wir sagen dann, $f$ ist eine \Index{Bilinearform},
\end{definition}

Sind $V$ und $W$ endlich dimensionale unitäre (bzw. orthogonale) $\KK$-Vektorräume, so besitzen die Sesquilinearformen auf $V \times W$ eine schöne Beschreibung:

\begin{satz}
	\label{satz:5.2}
	Seien $(V,\sk{\cdot,\cdot})$ und $(W,\sk{\cdot,\cdot})$ endlich dimensionale unitäre (bzw. orthogonale) $\KK$-Vektorräume und sei $f \colon V \times W \rightarrow \KK$ eine Sesquilinearform.
	Dann existiert genau eine lineare Abbildung $F \colon V \rightarrow W$ mit
	\[
		f(v,w) = \sk{F(v),w}
	\]
	für alle $v \in V$ und $w \in W$.
	Ist $B = \{v_1,\dots,v_n\}$ eine Orthonormalbasis von $V$ und $\widetilde{B} = \{w_1,\dots,w_m\}$ eine Orthonormalbasis von $W$, so gilt für die Darstellungsmatrix $\mat{A}{\wt{B}}{B}{F} = (a_{ij})$:
	\[
		a_{ij} = f(v_j,w_i)	\text{ für alle } 1\leq j \leq n, 1 \leq i \leq m.
	\]
\end{satz}

\begin{beweis}
	Seien $B$ und $\wt{B}$ wie im Satz und sei $A = (a_{ij}) \in M(n \times m,\KK)$ mit $a_{ij} = f(v_j,w_i)$.
	Dann existiert genau eine Abbildung $F\colon V \rightarrow W$ mit $\mat{A}{\wt{B}}{B}{F} = A$.
	Diese ist festgelegt durch
	\[
		F(v_j) = \sum_{i=1}^{m} a_{ij} w_i = \sum_{i=1}^{m} f(v_j,w_i) w_i
	\]
	und dann folgt für alle $1 \leq i \leq m, 1 \leq j \leq n$:
	\[
		\sk{F(v_j),w_i} = \sk{\sum_{l=1}^{m} a_{lj} w_l,w_i} = \sum_{l=1}^{m} a_{lj} \Underbrace{\sk{w_l,w_i}}{=\delta_{ij}} = a_{ij} = f(v_j,w_i).
	\]
	\newpage
	Für $v = \sum_{j=1}^{n} \lambda_j v_j$ und $w = \sum_{i=1}^{m} \mu_i w_i$ folgt dann
	\begin{align*}
		\sk{F(v),w} &= \sk{F \enb{\sum_{j=1}^{n} \lambda_j v_j}, \sum_{i=1}^{m} \mu_i w_i} = \sum_{j=1}^{n} \sum_{i=1}^{m} \lambda_j \ol{\mu_i} \sk{F(v_j),w_i} \\
		&= \sum_{j=1}^{n} \sum_{i=1}^{m} \lambda_j \ol{\mu_i} f(v_j,w_i) \stack{\text{Sesq.}}{=} f \enb{\sum_{j=1}^{n} \lambda_j v_j , \sum_{i=1}^{m} \mu_i w_i} = f(v,w).
	\end{align*}
	Nach Konstruktion von $F$ gilt natürlich $\mat{A}{\wt{B}}{B}{F} = (a_{ij})$ mit $a_{ij} = f(v_j,w_i)$ für alle $1 \leq j \leq n$ und $1 \leq i \leq m$.
	
	\textit{Zur Eindeutigkeit:} Sei $\wt{F} \colon V \rightarrow W$ eine weitere lineare Abbildung mit $f(v,w) = \sk{\wt{F}(v),w}$ für alle $v \in V, w \in W$.
	Dann folgt für alle $v \in V$ und $w \in W$:
	\[
		\sk{(F-\wt{F})(v),w} = \sk{F(v),w} - \sk{\wt{F}(v),w} = f(v,w) - f(v,w) = 0.
	\]
	Mit $w := (F-\wt{F})(v)$ folgt dann für alle $v \in V$:
	\[
		\no{(F-\wt{F})(v)}^2 = \sk{(F - \wt{F})(v),w} = 0,
	\]
	also $F - \wt{F}(v) = 0$ und damit $F = \wt{F}$. \qedhere
\end{beweis}

\begin{bemerkung}
	\label{bem:5.3}
	Natürlich gilt für jede lineare Abbildung $F \colon V \rightarrow W$, dass $(v,w) \mapsto \sk{F(v),w}$ eine Sesquilinearform auf $V \times W$.
	\autoref{satz:5.2} liefert daher eine Bijektion zwischen dem Raum $\Sesqui(V \times W)$ aller Sesquilinearformen auf $V \times W$ und $\Hom(V,W)$. 
	Dies ist sogar ein Vektorraum-Isomorphismus.
\end{bemerkung}

\begin{korollar}
	\label{kor:5.4}
	Seien $V,W$ unitäre (bzw. orthogonale) $\KK$-Vektorräume, $B = \{v_1,\dots,v_n\}$ eine Orthonormalbasis von $V$ und $\wt{B} = \{w_1,\dots,w_n\}$ eine Orthonormalbasis von $W$.
	Ist dann $F \colon V \times W$ linear, so gilt
	\[
		\mat{A}{\wt{B}}{B}{F} = (a_{ij})_{\substack{1 \leq i \leq m \\ 1 \leq j \leq n}} \text{ mit } a_{ij} = \sk{F(v_j),w_i}.
	\]
	Ferner gilt:
	Ist $v = \sum_{j=1}^{n} x_j v_j, w = \sum_{i=1}^{m} y_i w_i$ mit $x = (x_1,\dots,x_n)^T \in \KK^n$ und $y = (y_1,\dots,y_n)^T \in \KK^m$, so ist
	\[
		\sk{F(v),w)} = \sk{Ax,y}_S
	\]
	mit $A = \mat{A}{\wt{B}}{B}{F}$ und $\sk{\cdot,\cdot}_S$ das Standard-Skalarprodukt auf $\KK^m$.
\end{korollar}
\newpage
\begin{beweis}
	Die Formel für $\mat{A}{\wt{B}}{B}{F}$ folgt aus \autoref{satz:5.2} angewandt auf $f(v,w) := \sk{F(v),w}.$
	Für die Gleichheit setzen wir ein:
	\begin{align*}
		\sk{\sum_{j=1}^{n} x_j v_j, \sum_{i=1}^{m} y_i w_i} &= \sum_{j=1}^{n} \sum_{i=1}^{m} x_j \ol{y_i} \Underbrace{\sk{F(v_j),w_i}}{a_{ij}} = \sum_{j=1}^{n} \sum_{i=1}^{m} a_{ij} x_j \ol{y_i} \\
		&= \sum_{i=1}^{m} (Ax)_j \ol{y_i} = \sk{Ax,y}. \qedhere
	\end{align*}
\end{beweis}

\begin{satz}
	\label{satz:5.5}
	Seien $V,W$ endlich dimensionale unitäre (bzw. orthogonale) $\KK$-Vektorräume und sei $F \colon V \rightarrow W$ linear.
	Dann existiert genau eine lineare Abbildung $F^* \colon W \rightarrow V$ mit
	\[
		\sk{F(v),w}_W = \sk{v,F^*(w)}_V
	\]
	für alle $v \in V$ und $w \in W$.
	
	Sind dann $B = \{v_1,\dots,v_n\}$ und $\wt{B} = \{w_1,\dots,w_m\}$ Orthonormalbasen von $V$ und $W$, so gilt $\mat{A}{B}{\wt{B}}{F^*} = \enb{\mat{A}{\wt{B}}{B}{F}}^*$, wobei für $A \in M(n \times n, \KK)$ die Matrix $A^*$ definiert ist durch $A^* = \ol{A}^T$ (transponierte und komplex konjugierte Matrix).
\end{satz}

\begin{beweis}
	Betrachte $f^*\colon W \times V \rightarrow \KK$ mit $f^*(w,v) := \sk{w,F(v)}$.
	Dann ist $f^*$ eine Sesquilinearform und nach \autoref{satz:5.2} existiert genau eine lineare Abbildung $F^*\colon W \rightarrow V$ mit
	\[
		\sk{F^*(w),v}_V = f^*(w,v) = \sk{w,F(v)}_W
	\]
	für alle $v \in V$ und $w \in W$.
	Dann folgt auch für alle $v \in V$ und $w \in W$:
	\[
		\sk{F(v),w} = \ol{\sk{w,F(v)}} = \ol{\sk{F*(w),v}} = \sk{v,F*(w)}.
	\]
	Sind $B$ und $\wt{B}$ wie im Satz, so folgt ferner mit $\mat{A}{B}{\wt{B}}{F^*} = (a^*_{ij})$:
	\[
		a^*_{ij} \stack{\ref{kor:5.4}}{=} \sk{F^*(w_j),v_i} = \sk{w_j,F(v_i)} = \ol{\sk{F(v_i),w_j}} \stack{\ref{kor:5.4}}{=} \ol{a_{ji}}. \qedhere
	\]
\end{beweis}

\begin{definition}[Adjungierte]
	\label{def:5.6}
	Die Abbildung $F^*\colon W \rightarrow V$ aus \autoref{satz:5.5} heißt die zu $F \colon V \rightarrow W$ \textbf{adjungierte lineare Abbildung}.
	
	Analog: Ist $A \in M(m \times n, \KK)$ mit $A = (a_{ij})$ und ist $A^* \in (n \times m, \KK)$ mit $A^* = (a^*_{ij})$, sodass $a^*_{ij} := \ol{a_{ji}}$, so heißt $A^*$ die zu $A$ \textbf{adjungierte Matrix}. \index{Adjungierte}
\end{definition}

\begin{bemerkung}
	\label{bem:5.7}
	\mbox{} \\[-1.4cm]
	\begin{enumerate}[(a)]
		\item Ist $V = \KK^n$ und $W = \KK^m$ jeweils versehen mit dem Standard-Skalarprodukt, so gilt für alle $A \in M(m \times n,\KK)$ und für alle $x \in \KK^n, y \in \KK^m$:
		\begin{equation}
			\sk{Ax,y} = \sk{x,A^*y}. \label{eq:5.7.1}
		\end{equation}
		Dazu wende \autoref{satz:5.5} an auf $F_A \colon \KK^n \rightarrow \KK^m, x \mapsto Ax$.
		Dann gilt $\mat{A}{\mathcal{S}_m}{\mathcal{S}_n}{F_A} =A$ und $\mat{A}{\mathcal{S}_n}{\mathcal{S}_m}{F_A^*} = A^*$, also gilt $F_A^*(y) = A^*y$ für alle $y \in \KK^m$ und damit
		\[
			\sk{Ax,y} = \sk{F_A(x),y} = \sk{x,F_A^*(y)} = \sk{x,A^*y}.
		\]
		Man kann die Gleichung \eqref{eq:5.7.1} aber auch direkt nachrechnen.
		\item Die Abbildung
		\begin{align*}
			^*\colon \Hom(V,W) &\longrightarrow \Hom(W,V) \\
			F &\longmapsto F^*
		\end{align*}
		ist ein konjugiert linearer Isomorphismus mit Umkehrabbildung $^*\colon \Hom(W,V) \rightarrow \Hom(V,W)$, denn:
		
		Es gelten $(F+G)^* = F^* + G^*$ und $(\lambda F)^* = \ol{\lambda} F^*$.
		Ferner gilt $(F^*)^* = F$ für alle $F \in \Hom(V,W)$, denn für alle $w \in W$ und $v \in V$ gilt
		\[
			\sk{F^*(w),v} = \ol{\sk{v,F^*(w)}} = \ol{\sk{F(v),w}} = \sk{w,F(v)}.
		\]
		\item Analog zu (b) gilt:
		Die Abbildung $^*\colon M(m \times n, \KK) \rightarrow M(n \times m, \KK)$ mit $A \mapsto A^*$ ist ein konjugiert linearer Isomorphismus $(A^*)^* = A$, das heißt mit Umkehrabbildung $*\colon M(n \times m, \KK) \rightarrow M(m \times n, \KK)$.
	\end{enumerate}
\end{bemerkung}

Erinnerung: Ist $V$ ein $\KK$-Vektorraum, so ist ein Skalarprodukt auf $V$ eine Sesquilinearform $\sk{\cdot,\cdot}$ auf $V \times V$, die zusätzlich hermitesch (d.h. $\sk{v,w} = \ol{\sk{w,v}}$) und positiv definit ist (d.h. $\sk{v,v} \geq 0$ und $\sk{v,v} = 0 \Leftrightarrow v = 0$).
Ist $\dim(V) \leq \infty$, so können wir jetzt mit Hilfe eines Skalarprodukts auch alle anderen Skalarprodukte beschreiben.
Dafür benötigen wir:

\begin{definition}
	\label{def:5.8}
	Sei $(V,\sk{\cdot,\cdot})$ ein endlich dimensionaler unitärer oder euklidischer $\KK$-Vektorraum und sei $F \in \End(V)$.
	\begin{enumerate}[(i)]
		\item $F$ heißt \Index{selbstadjungiert}, falls $F^* = F$.
		\item $F$ heißt \Index{positiv definit}, falls $F$ selbstadjungiert ist und falls für alle $v \in V$ gilt: $\sk{F(v),v} \geq 0$ und $\sk{F(v),v} = 0 \Leftrightarrow v = 0$.
	\end{enumerate}
	
	Analog definieren wir für $A \in M(n \times n, \KK)$:
	\begin{enumerate}[(i)]
		\item $A$ heißt \Index{selbstadjungiert}, falls $FA* = A$.
		\item $A$ heißt \Index{positiv definit}, falls $A$ selbstadjungiert ist und falls für alle $x \in \KK^n$ gilt: $\sk{Ax,x} \geq 0$ und $\sk{Ax,x} = 0 \Leftrightarrow x = 0$.
	\end{enumerate}
	
	Gilt nur $\sk{Ax,x} \geq 0$, so sagt man auch, dass $A$ semi-positiv definit oder positiv semidefinit ist.
	Ist $\KK= \RR$, so ist $A$ selbstadjungiert, wenn $A$ symmetrisch ist, das heißt $A^T = A$.
\end{definition}

\begin{lemma}
	\label{lemma:5.9}
	Sei $(V,\sk{\cdot,\cdot})$ endlich dimensionaler unitärer oder euklidischer $\KK$-Vektorraum und sei $B = \{v_1,\dots,v_n\}$ eine Orthonormalbasis von $V$.
	Ferner sei $F \in \End(V)$.
	Dann gelten:
	\begin{enumerate}[(i)]
		\item $F$ ist selbstadjungiert $\Leftrightarrow A^B_F$ ist selbstadjungiert.
		\item $F$ ist positiv definit $\Leftrightarrow A^B_F$ ist positiv definit.
	\end{enumerate}
\end{lemma}

\begin{beweis}
	\mbox{} \\[-0.9cm]
	\begin{enumerate}[(i)]
		\item Das folgt aus $A^B_{F^*} = \mat{A}{B}{B}{F^*} \stack{\ref{satz:5.5}}{=} \enb{\mat{A}{B}{B}{F}}^* = \enb{A^B_F}^*$.
		\item Sei $\Phi_B\colon \KK^n \rightarrow V, x \mapsto \sum_{i=1}^{n} x_i v_i$ der zu $B$ gehörende Isomorphismus.
		Dann folgt mit \autoref{kor:5.4}:
		
		Ist $v = \Phi_B(x), x \in \KK^n$, so folgt $\sk{F(v),v} = \sk{Ax,x}$, also $\sk{F(v),v} > 0 \Leftrightarrow \sk{Ax,x} > 0$. \qedhere
	\end{enumerate}
\end{beweis}

\begin{korollar}
	\label{kor:5.10}
	\mbox{} \\[-1.4cm]
	\begin{enumerate}[(i)]
		\item Sei $\sk{\cdot,\cdot}$ ein Skalarprodukt auf dem endlich dimensionalen $\KK$-Vektorraum $V$.
		Ist dann $\sk{\sk{\cdot,\cdot}}$ ein weiteres Skalarprodukt auf $V$, so existiert eine eindeutig bestimmte positiv definite lineare Abbildung $F\colon V \rightarrow V$ mit $\sk{\sk{v,w}} = \sk{F(v),w}$ für alle $v,w \in V$.
		\item Ist $\sk{\sk{\cdot,\cdot}}$ ein Skalarprodukt auf $\KK^n$, so existiert genau eine positiv definite Matrix $A \in M(n \times n,\KK)$ mit $\sk{\sk{x,y}} = \sk{Ax,y}_S$ für alle $x,y \in \KK^n$, wenn $\sk{\cdot,\cdot}_S$ das Standard-Skalarprodukt auf $\KK^n$ bezeichnet.
	\end{enumerate}
\end{korollar}

\begin{beweis}
	\mbox{} \\[-.9cm]
	\begin{enumerate}[(i)]
		\item Nach \autoref{satz:5.2} existiert genau eine lineare Abbildung $F \colon V \rightarrow W$ mit $\sk{\sk{v,w}} = \sk{F(v),w}$.
		Da $\sk{\sk{\cdot,\cdot}}$ hermitesch ist, folgt $\sk{F(v),w} = \ol{\sk{F(w),v}}$, also gilt $F = F^*$.
		Da $\sk{\sk{\cdot,\cdot}}$ positiv definit ist, folgt $\sk{F(v),v} \geq 0$ für alle $v \in V$ und $\sk{F(v),v} = 0 \Leftrightarrow v = 0$, also ist $F$ positiv definit.
		\item Dies folgt direkt aus (i) und der Darstellung von linearen Abbildungen mit Hilfe von Matrixmultiplikation. \qedhere
	\end{enumerate}
\end{beweis}

\begin{problem}
	\label{prob:5.11}
	Wie stellen wir fest, ob eine gegebene selbstadjungierte Matrix (bzw. ein $F \in \End(V)$) positiv definit ist?
	Wir stellen zunächst fest:
	\begin{enumerate}[(i)]
		\item Ist $A$ positiv definit und ist $S \in M(n \times n,\KK)$ invertierbar, so gilt
		\[
			A \text{ positiv definit} \quad \Leftrightarrow \quad S^*AS \text{ positiv definit}
		\]
		\enquote{$\Rightarrow$} folgt aus $\sk{S^*ASx,x} = \sk{ASx,Sx} = \sk{Ay,y}$ mit $y = Sx$.
		Analog folgt \enquote{$\Leftarrow$} aus der obigen Gleichung mit $x = S^{-1}y$ für ein beliebiges $y \in \KK^n$.
		Dann ist $\sk{Ay,y} = \sk{ASx,Sx} = \sk{S^*ASx,x} > 0$ für $y \neq 0$.
		\item Sei $A \in M(n\times n,\KK$.
		Für $1 \leq k \leq n$ sei $A_k \in M(k \times k, \KK)$ der $k$-te \Index{Hauptminor} von $A$, das heißt ist $A = (a_{ij})_{1 \leq i,j \leq n}$, so ist $A_k = (a_{ij})_{1 \leq i,j \leq k}$.
		\[
			\enb{ \begin{BMAT}(e)[1pt]{ccccc}{ccccc}
				a_{11} & \cdots & a_{1k} & \cdots & a_{1n} \\
				\vdots & A_k & \vdots & & \vdots \\
				a_{k1} & \cdots & a_{kk} & \cdots & a_{kn} \\
				\vdots &  & \vdots & & \vdots \\
				a_{n1} & \cdots & a_{nk} & \cdots & a_{nn}
				\addpath{(0,2,|)rrruuu}
				\end{BMAT}}
		\]
		Dann gilt: Ist $A$ positiv definit, so sind auch alle $A_k$ positiv definit für $1 \leq k \leq n$.
		Denn: Ist $0 \neq x = (x_1, \dots, x_k)^T \in \KK^k$, so gilt mit $\wt{x} = (x_1, \dots, x_k,0,\dots,0)^T \in \KK^n$: $\sk{A_kx,x} = \sk{A\wt{x},\wt{x}}> 0$.
		\item Ist $A \in M(m \times n, \KK), B \in M(n \times l,\KK)$, so gilt $(AB)^* = B^*A^*$.
		Das folgt aus $(AB)^* = \ol{AB}^T = (\ol{A}\cdot \ol{B})^T = \ol{B}^T \ol{A}^T = B^* A^*$.
		Ist $A$ invertierbar, so auch $A^*$ mit $(A^*)^{-1} = (A^{-1})^*$, denn z.B. gilt $(A^{-1})^*A^* = (A \cdot A^{-1})^* = E_n^* = E_n$.
		\item Ist $A \in M(n \times n, \KK)$, so gilt $\det(A^*) = \ol{\det(A)}$, denn:
		\begin{align*}
			\det(A^*) &= \det(\ol{A}^T) = \det(\ol{A}) = \sum_{\sigma \in S_n} \sign(\sigma) \ol{a_{1,\sigma(1)}} \cdots \ol{a_{n,\sigma(n)}} \\
			&= \ol{ \sum_{\sigma \in S_n} \sign(\sigma) a_{1,\sigma(1)} \cdots a_{n,\sigma(n)}} = \ol{\det(A)}.
		\end{align*} 
		Damit folgt: Ist $A = A^*$, so gilt $\det(A) = \ol{\det(A)}$, also $\det(A) \in \RR$.
	\end{enumerate}
\end{problem}

\begin{satz}[Kriterium von \textsc{Sylvester}]
	\label{satz:5.12}
	Sei $A \in M(n \times n,\KK)$ selbstadjungiert, also $A^* = A$.
	Dann ist $A$ positiv definit genau dann, wenn für alle Hauptminoren $A_k$ von $A$ gilt: $\det(A_k) > 0$. \index{Kriterium von Sylvester}
\end{satz}

\begin{beweis}
	(vollständige Induktion nach $n$) \\
	Für $n = 1$ ist die Aussage trivial:
	Sei $A = (a)$, so gilt:
	\begin{align*}
		&A \text{ positiv definit} \\
		\Leftrightarrow \quad &\text{Für alle} x \in \KK \setminus \setzero \text{ gilt } 0 < \sk{Ax,x} = ax\ol{x} = a\abs{x}^2 \\
		\Leftrightarrow \quad &\det(A) = a > 0.
	\end{align*}
	Sei die Aussage wahr für $n$.
	Sei $A \in M((n+1)\times(n+1),\KK)$ mit $A = A^*$.
	Wir schreiben $A$ in Blockform wie folgt:
	\[
		A = \enb{ \begin{BMAT}(e)[1pt]{ccc|c}{ccc|c}
		& & & \\
		& A_n & & v \\
		& & & \\
		& v^* & & a
		\end{BMAT}}
	\]
	mit $v \in \KK^n$ und $v^* = \ol{v}^T$.
	Nach Induktionsvoraussetzung folgt:
	$A_n$ ist positiv definit genau dann, wenn $\det(A_k) > 0$ für alle $ 1 \leq k \leq n$.
	Insbesondere folgt dann, dass $A_n$ invertierbar ist.
	Sei dann
	\[
	S = \enb{ \begin{BMAT}(r)[1pt]{ccc|c}{ccc|c}
		& & & \\
		& E_n & & A_n^{-1}v \\
		& & & \\
		& 0 & & 1
		\end{BMAT}}.
	\]
	Dann gilt $\det(S) = 1$, also ist auch $S$ invertierbar.
	Für ein $b \in \RR$ betrachten wir dann das Produkt
	\begin{align*}
		S^* \wt{A} S &=
			\enb{\begin{BMAT}(r)[1pt]{ccc|c}{ccc|c}
					& & & \\
					& E_n & & 0 \\
					& & & \\
					& v^* A_n^{-1} & & 1
					\end{BMAT}} \cdot
			\enb{\begin{BMAT}(b)[1pt]{ccc|c}{ccc|c}
				& & & \\
				& A_n & & 0 \\
				& & & \\
				& 0 & & b
				\end{BMAT}} \cdot
			\enb{\begin{BMAT}(r)[1pt]{ccc|c}{ccc|c}
				& & & \\
				& E_n & & A_n^{-1}v \\
				& & & \\
				& 0 & & 1
				\end{BMAT}} \\
			&= \enb{\begin{BMAT}(b)[1pt]{ccc|c}{ccc|c}
				& & & \\
				& A_n & & 0 \\
				& & & \\
				& v^* & & b
				\end{BMAT}} \cdot
			\enb{\begin{BMAT}(r)[1pt]{ccc|c}{ccc|c}
				& & & \\
				& E_n & & A_n^{-1}v \\
				& & & \\
				& 0 & & 1
				\end{BMAT}} \\
			&= \enb{\begin{BMAT}(r)[1pt]{ccc|c}{ccc|c}
				& & & \\
				& A_n & & v \\
				& & & \\
				& v^* & & b + v^* A_n^{-1}v
				\end{BMAT}} = A,\text{ falls } b = a-v^* A_n^{-1} v.
	\end{align*}
	Nach \autoref{prob:5.11}(i) ist $A$ positiv definit genau dann, wenn $\wt{A}$ positiv definit ist, und wegen $\sk{\wt{A}x,x} = \sk{A_n \wt{x},\wt{x}} + b \abs{x_{n+1}}^2$ mit $\wt{x} = (x_1,\dots,x_n)^T$ ist dies der Fall genau dann, wenn $A_n$ positiv definit und $b>0$.
	
	Sei nun $A$ positiv definit.
	Nach \autoref{prob:5.11}(ii) ist dann auch $A_n$ positiv definit und dann folgt nach Induktionsvoraussetzung $\det(A_1),\cdots,\det(A_n) > 0$.
	Nach obiger Rechnung ist dann auch $b > 0$ und dann folgt auch
	\begin{align*}
		\det(A_{n+1}) &= \det(A) = \det(S^* \wt{A} S) \\
		&= \Underbrace{\det(S^*)}{=1} \cdot \det(\wt{A}) \cdot \Underbrace{\det(S)}{=1} = \det(\wt{A}) = \det(A_n) \cdot b >0,
	\end{align*}
	da $\det(A_n) > 0$ und $b > 0$.
	Also gilt $\det(A_k) > 0$ für alle $1 \leq k \leq n+1$.
	
	Gilt umgekehrt $\det(A_k) > 0$ für alle $1 \leq k \leq n+1$, so folgt aus dieser Rechnung, dass $0 < \det(A_{n+1}) = \det(A) = \det(A_n) \cdot b$, und da $\det(A_n) > 0$, folgt auch $b > 0$.
	Nach Induktionsvoraussetzung ist also $A_n$ positiv definit und $b > 0$, also folgt, dass $A$ positiv definit ist. \qedhere
\end{beweis}
\newpage
\begin{beispiel}
	\label{bsp:5.13}
	Sei
	\[
		A = \begin{pmatrix}
			2 & i \\ -i & 3
		\end{pmatrix}.
	\]
	Dann gilt $A^* = A$ und $\det(A_1) = 2 >0, \det(A_2) = 5 > 0$.
	Also ist $A$ positiv definit.
	Damit wird durch $\sk{\sk{z,w}} := \sk{Az,w}, z,w \in \CC^2$ ein neues Skalarprodukt auf $\CC^2$ definiert.
\end{beispiel}

\begin{bemerkung}
	\label{bem:5.14}
	Positiv definite Matrizen spielen auch in der Analysis II eine wichtige Rolle.
\end{bemerkung}
\newpage