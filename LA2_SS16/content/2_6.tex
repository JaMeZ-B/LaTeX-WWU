%!TEX root = ../LA2.tex
\section{Orthogonale und unitäre Abbildungen}
\label{sec:2.6}

In diesem Abschnitt wollen wir lineare Abbildungen auf unitären (bzw. orthogonalen) $\KK$-Vektorräumen betrachten, die das Skalarprodukt (und damit die Geometrie) erhalten.
Einfache Beispiele für solche Abbildungen sind Drehungen der Ebene (vgl. Blatt 3, Aufgabe 1) oder Spiegelungen.
Allgmein werden wir in diesem Abschnitt auch alle Abbildungen $\RR^n \rightarrow \RR^n$ untersuchen, die Abstände zwischen zwei Punkten erhalten.

\begin{definition}[Unitäre und orthogonale Abbildungen]
	\label{def:6.1}
	Sei $V$ ein endlich dimensionaler $\KK$-Vektorraum mit Skalarprodukt $\sk{\cdot,\cdot}$.
	Eine lineare Abbildung $F \colon V \rightarrow V$ heißt \Index{unitär} (bzw. \Index{orthogonal}, falls $\KK= \RR$), falls $F^* \circ F = \id_V = F \circ F^*$, also $F^* = F^{-1}$.
	
	Analog: Eine Matrix $U \in M(n \times n,\KK)$ heißt \Index{unitär} (bzw. \Index{orthogonal}), falls $U^*U = E_n = UU^*$, also $U^* = U^{-1}$.
\end{definition}

\begin{lemma}
	\label{lemma:6.2}
	Sei $B = \{v_1,\dots,v_n\}$ eine Orthonormalbasis von $V$ und sei $A = A^B_F$ die Darstellungsmatrix von $F \in \End(V)$.
	Dann gilt: 
	\[
		F \text{ unitär bzw. orthogonal} \quad \Leftrightarrow \quad A \text{ unitär bzw. orthogonal}.
	\]
	Ferner gilt:
	Sind $F,G \in \End(V)$ unitär bzw. orthogonal, so auch $F \circ G$, und sind $A,B \in M(n \times n, \KK)$ unitär bzw. orthogonal, so auch $AB$.
\end{lemma}

\begin{beweis}
	Wegen $A_{F^* \circ F}^B = A_{F^*}^B \cdot A_F^B$ und $A_{\id}^B = E_n$ folgt $F^* \circ F = \id_V \Leftrightarrow A_{F^*}^B \cdot A_F^B = A^*A = E_n$.
	Sind $F,G$ unitär, so auch $F \circ G$, denn zunächst gilt $(F \circ G)^* = G^* \circ F^*$, denn für alle $v,w \in V$ gilt
	\[
		\sk{(F \circ G)(v),w} = \sk{G(v),F^*(w)} = \sk{v,G^*(F^*(w))} = \sk{v,G^*\circ F^*(w)}.
	\]
	Dann folgt $(F \circ G)^* \circ (F \circ G) = G^* \circ (F^* \circ F) \circ G ) = \id_V$.
	Analog folgt: $A,B$ unitär $\Rightarrow AB$ unitär. \qedhere
\end{beweis}

\begin{bemerkung}
	\label{bem:6.3}
	Im Beweis haben wir die wichtige Formel $(F \circ G)^* = G^* \circ F^*$ hergeleitet.
	Analog gilt für Matrizen $A$ und $B$: $(AB)^* = B^* A^*$.
\end{bemerkung}

\begin{beispiel}
	\label{bsp:6.4}
	Für alle $\alpha \in \RR$ ist
	\[
		O(\alpha) = \begin{pmatrix}
			\cos(\alpha) & -\sin(\alpha) \\
			\sin(\alpha) & \cos(\alpha)
		\end{pmatrix}
	\]
	orthogonal, denn
	\[
		O(\alpha)^* O(\alpha) = \begin{pmatrix}
			\cos(\alpha)  & \sin(\alpha) \\
			-\sin(\alpha) & \cos(\alpha)
		\end{pmatrix} \begin{pmatrix}
		\cos(\alpha) & -\sin(\alpha) \\
		\sin(\alpha) & \cos(\alpha)
		\end{pmatrix} = \begin{pmatrix}
			1 & 0 \\
			0 & 1
		\end{pmatrix},
	\]
	da $\cos^2(\alpha) + \sin^2(\alpha) = 1$.
	Die zugehörige lineare Abbildung $F_\alpha \colon \RR^2 \rightarrow \RR^2, F_\alpha(x) = O_\alpha \cdot x$ ist die Drehung mit Winkel $\alpha$ in mathematisch positiver Orientierung (gegen den Uhrzeigersinn).
	Analog ist die Spiegelung an der Diagonalen
	\[
		S = \begin{pmatrix}
		 0 & 1 \\
		 1 & 0
		\end{pmatrix}
	\]
	orthogonal, da $S^*S = S^2 = \id$.
\end{beispiel}

\begin{definition}[Unitäre und orthogonale Gruppe]
	\label{def:6.5}
	Sei $(V,\sk{\cdot,\cdot})$ ein endlich dimensionaler unitärer $\CC$-Vektorraum.
	Dann heißt
	\[
		\UU(V) := \{F \in \End(V) : F \text{ unitär}\}
	\]
	versehen mit der Verknüpfung $(F,G) \mapsto F \circ G$ die \Index{unitäre Gruppe} von $V$.
	
	Analog: Ist $(V,\sk{\cdot,\cdot})$ ein endlich dimensionaler orthogonaler $\RR$-Vektorraum, so heißt
	\[
		\oh(n) := \{F \in \End(V) : F \text{ orthogonal}\}
	\]
	die \Index{orthogonale Gruppe} von $V$.
	
	Ist $n \in \NN$ so, heißen $\UU(n) := \{U \in M(n\times n,\CC) : U \text{ unitär}\}$ und $\oh(n) := \{O \in M(n \times n,\RR) : O \text{ orthogonal}\}$, jeweils versehen mit der Matrixmultiplikation, die $n$-te \textbf{unitäre} bzw \textbf{orthogonale Gruppe}.
\end{definition}

\begin{bemerkung}
	\mbox{} \\[-1.4cm]
	\begin{enumerate}[(i)]
		\item Man prüft leicht nach, dass $\UU(V), \oh(V), \UU(n), \oh(n)$ Gruppen sind.
		Ist $(V,\sk{\cdot,\cdot})$ ein $n$-dimensionaler unitärer (bzw. orthogonaler) $\CC$-Vektorraum (bzw. $\RR$-Vektorraum), und ist $B = \{v_1,\dots,v_n\}$ eine Orthonormalbasis von $V$, so ist
		\begin{align*}
			\UU(V) &\longrightarrow \UU(n) \\
			F &\longmapsto A_F^B
		\end{align*}
		(bzw. $\oh(V) \rightarrow \oh(n)$) ein Isomorphismus.
		\item Ist $A \in M(n \times n,\KK)$, so gilt $\det(A^*) = \ol{\det(A)}$ (vgl. \autoref{prob:5.11}(iv)).
		Ist also $U \in \UU(n)$ so folgt $\abs{\det(U)} = 1$, denn
		\[
			\abs{\det(U)}^2 = \ol{\det(U)} \cdot \det(U) = \det(U^*) \cdot \det(U) = \det(U^*U) = \det(E_n) = 1.
		\]
		Da für $F \in \End(V)$ gilt $\det(F) := \det(A_F^B)$ für eine beliebige Basis $B = \{v_1,\dots,v_n\}$ von $V$, folgt auch $\abs{\det(F)}=1$ für alle $F \in \UU(V)$.
		Analoges gilt für $\oh(V)$ und $\oh(n)$.
		\newpage
		Wir definieren die \textbf{spezielle unitäre} bzw. \textbf{spezielle orthogonale Gruppe} durch: \index{spezielle orthogonale Gruppe} \index{spezielle unitäre Gruppe}
		\begin{align*}
			\SU(V) &:= \{F \in \UU(V) : \det(F) = 1\} \\
			\SU(n) &:= \{U \in \UU(n) : \det(U) = 1\} \\
			\SO(V) &:= \{F \in \oh(V) : \det(F) = 1\} \\
			\SO(n) &:= \{O \in \oh(n) : \det(O) = 1\} 
		\end{align*}
		Diese Gruppen spielen in der Mathematik eine herausragende Rolle!
	\end{enumerate}
\end{bemerkung}

\begin{beispiel}
	\label{bsp:6.7}
	Für die Drehmatrizen $O(\alpha)$ aus \autoref{bsp:6.4} gilt $\det(O(\alpha)) = 1$, also gilt $O(\alpha) \in \SO(2)$.
	Für die Spiegelung $S$ gilt $\det(S) = -1$, also gilt $S \notin \SO(2)$.
	
	Da für $O \in \oh(n)$ stets $\det(O) \in \RR$ gilt, gilt stets $\det(O) = \pm 1$ für alle $O \in \oh(n)$.
	Für Matrizen $U \in \UU(n)$ kann $\det(U)$ ein beliebiges Element in $\mathbb{T} = \{z \in \CC: \abs{z}=1\}$ sein.
\end{beispiel}

Wir wollen noch mehr über unitäre und orthogonale Endomorphismen und Matrizen lernen.

\begin{satz}
	\label{satz:6.8}
	Sei $(V,\sk{\cdot,\cdot})$ ein endlich dimensionaler unitärer (bzw. orthogonaler) $\KK$-Vektorraum und sei $F \in \End(V)$.
	Dann sind äquivalent:
	\begin{enumerate}[(i)]
		\item $F$ ist unitär (bzw. orthogonal)
		\item Es gilt $\sk{F(v),F(w)} = \sk{v,w}$ für alle $v,w \in V$.
		\item Ist $B = \{v_1,\dots,v_n\}$ eine Orthonormalbasis von $V$, so ist auch $\{F(v_1),\dots,F(v_n)\}$ eine Orthonormalbasis von $V$.
		\item Es gilt $\no{F(v)} = \no{v}$ für alle $v \in V$ (mit $\no{v} := \sqrt{\sk{v,v}}$).
	\end{enumerate}
	Die Richtungen (ii) $\Rightarrow$ (iii) und (ii) $\Rightarrow$ (iv) gelten für jede Abbildung $F \colon V \rightarrow V$.
\end{satz}

\begin{beweis} \mbox{} \\[-.9cm]
	\begin{description}
		\item[(i) $\Rightarrow$ (ii):] Ist $F$ unitär, so gilt für alle $v,w \in V$:
		\[
			\sk{F(v),F(w)} = \sk{v,F^*(F(w))} = \langle v,\Underbrace{F^*\circ F}{\mathclap{=\id_V}}(w)\rangle = \sk{v,w}.
		\]
		\item[(ii) $\Rightarrow$ (iii):] Ist $B = \{v_1,\dots,v_n\}$ eine Orthonormalbasis, so folgt mit (ii):
		\[
			\sk{F(v_i),F(v_j)} = \sk{v_i,v_j} = \delta_{ij},
		\]
		also ist $\{F(v_1),\dots,F(v_n)\}$ ein Orthonormalsystem mit $n = \dim(V)$ Elementen und damit eine Orthonormalbasis von $V$.
		\item[(iii) $\Rightarrow$ (i):] Sei $B = \{v_1,\dots,v_n\}$ eine Orthonormalbasis von $V$.
		Nach (iii) ist dann auch $\wt{B} = \{F(v_1),\dots,F(v_n)\}$ eine Orthonormalbasis von $V$, und es gilt dann $\mat{A}{\wt{B}}{B}{F} = E_n$ sowie $\mat{A}{B}{\wt{B}}{F^*} = \enb{\mat{A}{\wt{B}}{B}{F}}^* = E_n^* = E_n$.
		Damit folgt:
		\[
			A^B_{F^* \circ F} = \mat{A}{B}{B}{F^* \circ F} = \mat{A}{B}{\wt{B}}{F^*} \cdot \mat{A}{\wt{B}}{B}{F} = E_n^2 = E_n = A^B_{\id_V}.
		\]
		Weiter folgt $F^* \circ F = \id_V$ und analog $F \circ F^* = \id_V$.
		\item[(ii) $\Rightarrow$ (iv):] Dies folgt mit
		\[
			\no{F(v)}^2 = \sk{F(v),F(v)} = \sk{v,v} = \no{v}^2.
		\]
		\item[(iv) $\Rightarrow$ (ii):] Nach Blatt 3, Aufgabe 2 gilt
		\begin{align*}
			4 \cdot \sk{F(v),F(w)} &= \sum_{k=0}^{3} i^k \no{F(v) + i^k F(w)}^2 
			= \sum_{k=0}^{3} i^k \no{F(v+i^kw)}^2 \\
			&= \sum_{k=0}^{3} i^k \no{v+i^kw}^2 = 4 \cdot \sk{v,w}. \qedhere
		\end{align*}
	\end{description}	
\end{beweis}

Für Matrizen erhalten wir:

\begin{satz}
	\label{satz:6.9}
	Sei $A \in M(n \times n,\CC$ (bzw. $A \in M(n \times n, \RR)$).
	Dann sind äquivalent:
	\begin{enumerate}[(i)]
		\item $A$ ist unitär (bzw. orthogonal).
		\item Die Spalten $\{a_1,\dots,a_n\}$ von $A$ bilden eine Orthonormalbasis von $\KK^n$ bezüglich des Standard-Skalarprodukts.
		\item Sind $\{a^1,\dots,a^n\}$ die Zeilen von $A$, so ist $\penb{(a^1)^T,\dots,(a^n)^T}$ eine Orthonormalbasis von $\KK^n$ bezüglich des Standard-Skalarprodukts.
	\end{enumerate}
\end{satz}

\begin{beweis} \mbox{} \\[-.9cm]
	\begin{description}
		\item[(i) $\Leftrightarrow$ (ii):] Seien $\{a_1,\dots,a_n\}$ die Spalten von $A$.Dann gilt mit $A^*A = (\gamma_{ij})_{1 \leq i,j \leq n}$:
		\[
			\gamma_{ij} = \ol{a_i}^T \cdot a_j = \sum_{k=1}^{n} \ol{a_{ik}} a_{kj} = \sk{a_j,a_i}.
		\]
		Damit folgt
		\begin{align*}
			A^*A = E_n \quad &\Leftrightarrow \quad \gamma_{ij} = \delta_{ij} \text{ für alle } 1 \leq i,j \leq n \\
			&\Leftrightarrow \quad \sk{a_j,a_i} = \delta_{ij} \text{ für alle } 1 \leq i,j \leq n \\
			&\Leftrightarrow \quad \{a_1,\dots,a_n\} \text{ ist Orthonormalbasis von } \KK^n.
		\end{align*}
		\newpage
		Gilt $A^*A = E_n$, so gilt schon $A^* = A^{-1}$, denn wegen $\det(A^*) \cdot \det(A) = \det(A^*A) = \det(E_n) = 1$ ist $\det(A) \neq 0$, also ist $A$ invertierbar, und dann folgt $A^{-1} = (A^*A)A^{-1} = A^*(AA^{-1}) = A^*$.
		\item[(i) $\Leftrightarrow$ (iii):] Dies folgt analog mit $AA^*$ anstelle von $A^*A$. \qedhere
	\end{description}
\end{beweis}

Wir wollen uns nun wieder der Eingangsfrage widmen.
Wir beschränken und dafür (zunächst) auf den Fall eines orthogonalen $\RR$-Vektorraums $(V,\sk{\cdot,\cdot})$.

\begin{definition}
	\label{def:6.10}
	Eine \Index{Isometrie} $T \colon V \rightarrow V$ ist eine Abbildung, die die Abstände erhält, das heißt es gilt
	\[
		\no{T(v) - T(w)} = \no{v-w}
	\]
	für alle $v,w \in V$.
\end{definition}

Jede orthogonale Abbildung $F \colon V \rightarrow V$ ist isometrisch, da dann $\no{F(v) - F(w)} = \no{F(v-w)} \stackrel{\ref{def:5.8}}{=} \no{v-w}$ gilt.
Andere Beispiele sind Translationen:
Ist $v_0 \in V$ fest, so ist
\begin{align*}
	T_{v_0}\colon V &\longrightarrow V \\
	v &\longmapsto v + v_0
\end{align*}
eine Isometrie, denn für alle $v,w \in V$ gilt
\[
	\no{T_{v_0}(v) - T_{v_0}(w)} = \no{v+v_0 - (w+ v_0)} = \no{v-w}.
\]
Wir wollen nun einsehen, dass jede Isometrie von $V$ eine Kombination einer Translation mit einer orthogonalen Abbildung ist.

\begin{satz}
	\label{satz:6.11}
	Sei $(V,\sk{\cdot,\cdot})$ ein endlich dimensionaler orthogonaler $\RR$-Vektorraum und sei $T \colon V \rightarrow V$ eine Isometrie.
	Dann gelten:
	\begin{enumerate}[(i)]
		\item Gilt zusätzlich $T(0) = 0$, so ist $T$ orthogonal.
		\item Im Allgemeinen existiert eine orthogonale Abbildung $O \colon V \rightarrow V$ und ein $v_0 \in V$, mit $T(v) = O(v) + v_0$ für alle $v \in V$, also $T = T_{v_0} \circ O$.
	\end{enumerate}
\end{satz}

\begin{beweis} \mbox{} \\[-.9cm]
	\begin{enumerate}[(i)]
		\item Es genügt zu zeigen, dass $T$ unter der zusätzlichen Voraussetzung $T(0) = 0$ automatisch linear ist.
		Da für alle $v \in V$ gilt
		\[
			\no{T(v)} = \no{T(v) - 0} = \no{T(v) - T(0)} = \no{v-0} = \no{v},
		\]
		folgt dann aus \autoref{satz:6.8}, dass $T$ orthogonal ist.
		
		Sei also $T \colon V \rightarrow V$ isometrisch mit $T(0) = 0$.
		Wegen
		\begin{align*}
			\no{v}^2 + \no{w}^2 - \no{v-w}^2 &= \no{v}^2 + \no{w}^2 - \sk{v-w,v-w} \\
			&= \sk{w,v} + \sk{v,w} = 2 \cdot \sk{v,w}
		\end{align*}
		erhalten wir $\sk{v,w} = \frac{1}{2} (\no{v}^2 + \no{w}^2 - \no{v-w}^2)$, und damit
		\begin{equation}
			\begin{aligned}
				\sk{T(v),T(w)} &= \frac{1}{2} (\no{T(v)}^2 + \no{T(w)}^2 - \no{T(v)-T(w)}^2) \\
				&= \frac{1}{2} (\no{v}^2 + \no{w}^2 - \no{v-w}^2) \quad (T \text{ isometrisch, } T(0)=0) \\
				&= \sk{v,w}.
			\end{aligned}
			\label{eq:6.11.1}
		\end{equation}
		Sei nun $B = \{v_1,\dots,v_n\}$ eine Orthonormalbasis von $V$.
		Wegen \autoref{satz:6.8} ist dann auch $\{T(v_1),\dots,T(v_n)\}$ eine Orthonormalbasis von $V$.
		Dann folgt für alle $v \in V$ mit \autoref{satz:4.12}:
		\[
			T(v) = \sum_{j=1}^{n} \sk{T(v),T(v_j)} T(v_j) \stackrel{\eqref{eq:6.11.1}}{=} \sum_{j=1}^{n} \sk{v,v_j} T(v_j),
		\]
		und damit ist $T(v)$ linear, denn für $\lambda,\mu \in \RR$ und $v,w \in V$ gilt
		\begin{align*}
			T(\lambda v+\mu w) &= \sum_{j=1}^{n} \sk{\lambda v + \mu w,v_j} T(v_j) \\
			&= \lambda \enb{\sum_{j=1}^{n} \sk{v,v_j} v_j} + \mu \enb{\sum_{j=1}^{n} \sk{w,v_j} v_j} \\
			&= \lambda T(v) + \mu T(w).
		\end{align*}
		\item Sei nun $T \colon V \rightarrow V$ eine beliebige Isometrie.
		Setze $v_0 := T(0)$ und $O\colon V \rightarrow V$ mit $O(v) := T(v) - v_0$.
		Mit $T$ ist dann auch $O$ eine Isometrie, und es gilt $O(0) = T(0) - v_0 = 0$.
		Nach (i) ist $O$ eine orthogonale Abbildung und für alle $v \in V$ gilt $T(v) = O(v) + v_0$. \qedhere
	\end{enumerate}
\end{beweis}

\begin{anwendung}
	\label{anw:6.12}
	Wir wollen alle Isometrien des $\RR^2$ bezüglich Standard-Skalarprodukt verstehen.
	Nach \autoref{satz:6.11} genügt es dazu, alle orthogonalen Matrizen zu betrachten.
	
	Sei also $A = \begin{pmatrix}
	 a & b \\ c & d
	\end{pmatrix} \in \oh(2)$.
	Nach \autoref{satz:6.9} ist dann $\penb{\binom{a}{b},\binom{c}{d}}$ eine Orthonormalbasis von $\RR^2$, das heißt es gilt
	\[
		\no{\binom{a}{b}}^2 = a^2 + b^2 = 1 = c^2 + d^2 = \no{\binom{c}{d}}^2
	\]
	und $\sk{\binom{a}{b},\binom{c}{d}} = ac + bd = 0$.
	Hieraus folgt $\binom{c}{d} = \binom{-b}{a}$ oder $\binom{c}{d} = \binom{b}{-a}$, also $A = \begin{pmatrix}
	a & -b \\ b & a
	\end{pmatrix}$ oder $A = \begin{pmatrix}
	a & b \\ b & -a
	\end{pmatrix}$.
	\begin{description}
		\item[1. Fall:] $A = \begin{pmatrix}
		a & -b \\ b & a
		\end{pmatrix}$.
		
		Da $\no{\binom{a}{b}} = \sqrt{a^2+b^2}=1$, existiert genau ein $\alpha \in [0,2\pi)$ mit $\binom{a}{b} = \binom{\cos(\alpha)}{\sin(\alpha)}$ (Polarkoordinaten).
		Dann folgt:
		\[
		 A = O(\alpha) \begin{pmatrix}
		 \cos(\alpha) & -\sin(\alpha) \\ \sin(\alpha) & \cos(\alpha)
		 \end{pmatrix}
		\]
		ist Drehung um den Ursprung mit Winkel $\alpha$.
		\item[2. Fall:] $A = \begin{pmatrix}
		a & -b \\ b & a
		\end{pmatrix} = \begin{pmatrix}
		\cos(\alpha) & \sin(\alpha) \\ \sin(\alpha) & -\cos(\alpha)
		\end{pmatrix}$.
		
		Sei $\beta = \frac{\alpha}{2}$.
		Wir behaupten, dass $F_A\colon \RR^2 \rightarrow \RR^2, x \mapsto Ax$ gerade die Spiegelung $S_\beta$ an der Geraden $G_\beta := \RR \cdot (\cos(\beta),\sin(\beta))^T \subseteq \RR^2$ ist.
		
		\begin{figure}[h]
			\centering
			\begin{tikzpicture}[scale=2.5,>=Latex]
				\draw [very thick,->] (-1.5,0) -- (3,0);
				\draw [very thick,->] (0,-1.5) -- (0,2);
				
				\coordinate (O) at (0,0);
				\coordinate (u1) at (0.8660254,0.5);
				\coordinate (u12) at (1.7320508,1);
				\coordinate (u14) at (2.5980762,1.5);
				\coordinate (u13) at (-1.7320508,-1);
				\coordinate (u2) at (-0.5,0.8660254);
				\coordinate (u22) at (0.5,-0.8660254);
				\coordinate (xy) at (1.2320508,1.8660254);
				\coordinate (S) at (2.2320508,0.1339746);
				\coordinate (x) at (1,0);
				
				\draw [thick,darkgray] (O) circle (1);
				\draw (-.1,1) -- (.1,1);
				\draw (-.1,-1) -- (.1,-1);
				\draw (1,-.1) -- (1,.1);
				\draw (-1,-.1) -- (-1,.1);
				
				\draw [thick] (u13) -- (u14) node[right]{$G(\beta)$};
				\draw [thick,dashed] (u2) node[anchor=south east]{$u_\beta^\perp$} -- (xy) node[right]{$\binom{x}{y}$} -- (S) node[right]{$S_\beta \binom{x}{y}$} -- (u22) -- cycle;
				\draw (u1) node[anchor=north west]{$u_\beta$};
				\draw (O) node[anchor=north west]{$0$};
				\draw (0,1) node[right]{$1$};
				\draw (0,-1) node[right]{$-1$};
				\draw (-1,0) node[below]{$-1$};
				\draw (1,0) node[below]{$1$};
				
				\foreach \x in {(u1),(u12),(u2),(u22),(xy),(S)} {
					\draw \x node[fill,circle,inner sep=1pt]{};	
				};
				
				\draw pic["$\beta$",draw=black,angle eccentricity=.75,angle radius=1cm]{angle=x--O--u1};
				\draw pic["$\bullet$",draw=black,angle eccentricity=.5,angle radius=.6cm]{angle=S--u22--O};
				\draw pic["$\bullet$",draw=black,angle eccentricity=.5,angle radius=.6cm]{angle=xy--u12--O};
				\draw pic["$\bullet$",draw=black,angle eccentricity=.5,angle radius=.6cm]{angle=O--u12--S};
			\end{tikzpicture}
		\end{figure}
		
		
		Zur Berechnung von $S_\beta$ sei $u_\beta = (\cos (\beta), \sin (\beta))^T, u_\beta^\perp = (-\sin (\beta), \cos (\beta))^T$.
		Dann ist $S_\beta$ gegeben durch
		\begin{align*}
			S_\beta \binom{x}{y} &= \binom{x}{y} - 2 \cdot \overbrace{\sk{\binom{x}{y},u_\beta^\perp} u_\beta^\perp}^{\mathclap{\text{orthogonale Projektion auf } G_\beta^\perp}} \\
			&= \binom{x}{y} + 2 \cdot (x \cdot \sin(\beta) - y \cdot \cos(\beta)) \cdot \binom{-\sin(\beta)}{\cos(\beta)}.
		\end{align*}
		Einsetzen von $e_1 = \binom{1}{0}, e_2 = \binom{0}{1}$ liefert mit den Additionstheoremen und $\alpha = 2\beta$:
		\begin{align*}
			S_\beta \binom{1}{0} &= \binom{1-2 \sin^2(\beta)}{2 \sin(\beta) \cos(\beta)} = \binom{\cos^2 (\beta) - \sin^2 (\beta)}{2 \sin (\beta) \cos (\beta)} = \binom{\cos(\alpha)}{\sin(\alpha)} \\
			S_\beta \binom{0}{1} &= \binom{2 \cos (\beta) \sin (\beta)}{1-2\cos^2 (\beta)} = \binom{2 \sin (\beta) \cos (\beta)}{\sin^2 (\beta) - \cos^2 (\beta)} = \binom{\sin(\alpha)}{-\cos(\alpha)},
		\end{align*}
		das heißt die Darstellungsmatrix von $S_\beta$ ist
		\[
			A = \begin{pmatrix}
				\cos(\alpha) & \sin(\alpha) \\
				\sin(\alpha) & -\cos(\alpha)
			\end{pmatrix}
		\]
	\end{description}
	Fazit: Jede Isometrie von $\RR^2$ ist eine Kombination einer Translation mit einer Drehung oder Spiegelung.
\end{anwendung}
\newpage