%!TEX root = ./WTG.tex

\section{Verzweigungsprozesse, Perkolation und Momentenmethode}

\subsection{Erste Momentenmethode}

Erinnerung an Galton-Watson-Prozesse. Es geht im die Aussterbewahrscheinlichkeit einer \enquote{idealisierten} Population. Dazu seinen $\enb{L^{(n)}_i}_{i,n}$ Zufallsvariablen mit Werten in $\NN_0$, i.i.d., sodass $L^{(n)}_i \not\equiv 1$ $\p$-f.s. 

Es sei $Z^0 = 1$ und für $1 \geq 1$
\begin{align}
	Z^{(n)} = \sum\limits_{i = 1}^{Z^{(n-1)}} L_i^{(n)} && \text{wobei } Z^{(n-1)} = 0, \text{falls } Z{(n)}  = 0
\end{align}
Frage: Wann überlebt $Z^{(n)}$, d.h. wann ist $\prop{Z^{(n)} > 0\text{ für alle }n} > 0$?

\underline{Es gilt:} \\
\begin{satz}
	Es sei 
	\begin{align}
		q = \propE{Z^{(n)} \text{ stirbt aus} } = \propE{Z^{(n)} = 0, \forall n \geq n_0}
	\end{align}
	Dann ist 
	\begin{enumerate}[a]
	
		\item
			\begin{enumerate}
				\item $q = \lim\limits_{n \to \infty} f^{(n)}(0)$, wobei $f^{(n)} = \EW{s^{Z_n}}$
				\item $q$ ist der kleinste Fixpunkt von $f(s) = \EW{s^L}$, $L = \overset{d}{=} L^(n)$
				\item $q = 1 \Leftrightarrow \EW{L} \leq 1$ \\ $q<1$, falls $\EW{L} > 1$
			\end{enumerate}
		\item $\frac{Z^{(n)}}{m^n}$  ist für $m = \EW{L}$ ein Martingal
		\item $\frac{Z^{(n)}}{m^n}$ konvergiert genau dann gegen einen nicht-entarteten Limes, falls $\EW{L \log L^+} < \infty$
	\end{enumerate}
\end{satz}

Wir wollen nun Galotn-Watson-Prozesse mit Perkolation verbinden. Was ist Perkolation? Ein Modell für ein poröses Medium, d.h. $G=(V,E)$ sei ein unedlicher Graph. Bei der Knotenperkolation entfernt man zufällig Knoten aus $V$, bei der Kantenperkolation entfernt man zufällig Kanten. Wir bbetrachten Kantenperkolation und zumeist Bernoulli-Perkolation mit Parameter $p \in (0,1)$. Man entfernt hierbei Kanten unabhängig voneinander mit Wahrscheinlichkeit $1-p$. \\
\underline{Sprache:} Die Kanten, die man behält, heißen \emph{offen}, die anderen \emph{geschlossen}. Sei $w \subseteq G$ das Produkt eines Perkolationsprozesses, dann heißen die Zusammenhangskomponenten von $w$ (offene) \emph{Cluster}.
\begin{definition}
	Sei $x \in V$. Dann bezeichnen wir mit $k(x)$ das offene Cluster mit $x\in V$. $k(x)$ kann auch einelementig sein. 
\end{definition}

\underline{Zentrale Frage:} Wamm gibt es ein $x$, sodass $k(x)$ unendlichen Durchmesser hat? \\
Diese Frage ist im Allgemeinen schwer, wir versuchen Schranken an die $p$ zu geben, bei denen in der $\Ber(p)$-Perkolation Phasenübergänge stattfinden. Für die obere Schranke benötigen wir nur, dass Ereignisse der Form $\set{w \given x \in w}$ oder $\set{w \given e \in w} = \set{w \given X_e(w) = 1}$ für alle $x \in V, e \in E$ messbar sind, wobei wir $x \notin w$ schreiben, falls $X_e = 0\ \forall e$, deren einer Endpunkt $x$ ist.
\begin{definition}
	Für $x,y \in V$ und $e \in E$ schreiben wir
	\begin{align}
		\benb{x \leftrightarrow e} := \set{e^-,e^+ \in k(x)}  := \set{e \in k(x)} && \text{für }e = \set{e^-,e^+} \\
		\benb{x \leftrightarrow y} = \set{k(x) = k(y)} = \set{w \given y \in k(x)} \\
		\benb{x \leftrightarrow \infty} := \set{w \given k(x) \text{ hat einen unendlichen Durchmesser}}
	\end{align}

\end{definition}

\begin{uebung}
	Diese Ereignisse sind messbar.
\end{uebung}
Um die Wahrscheinlichkeit von $\benb{x \leftrightarrow \infty}$ beschränken zu können, benötigen wir eine Version des Cutsets. Sei $\Pi \subseteq E$. Wir sagen, dass $\Pi$ $x$ von $\infty$ trennt, falls das Löschen der Kanten von $\Pi$ aus $E$ dazu führt, dass $k(x)$ endlich ist. Wir geben eine erste Schranke an $\propE{x \leftrightarrow \infty}$.
\begin{satz}
	\label{satz:6-3}
	Für alle Perkolationsprozesse auf einem unendlichen Graphen $G = (V,E)$ gilt
	\begin{align}
		\propE{x \leftrightarrow \infty} \leq \inf \set{\sum\limits_{e \in \Pi} \prop{x \leftrightarrow e} \given \text{$\Pi$ trennt $x$ von $\infty$} }
	\end{align}
\end{satz}
\begin{beweis}
	Sei $\Pi$ so, dass $\Pi$ $x$ von $ \infty$ trennt, dann gilt: $\benb{x \leftrightarrow \infty} \subseteq \bigcup\limits_{e \in \Pi} \benb{x \leftrightarrow e}$. Also
	\begin{align}
		\propE{x \leftrightarrow \infty} \leq \propE{\bigcup\limits_{e \in \Pi}\benb{x \leftrightarrow e}} \leq \sum\limits_{e \in \Pi}\propE{x \leftrightarrow e}
	\end{align}
	Da das für jede Menge $\Pi$ gilt, die $x$ von $\infty$ trennt, gilt es auch für das Infimum über all solche $\Pi$
\end{beweis}
Diese einfache Schranke heißt \emph{erste Momentenmethode} und liefert manchmal erstaunlich gute Ergebnisse.

Wir betrachten nun eine $\Ber(p)$-Kantenperkolation 
\begin{align}
	\propE{\text{w hat einen $\infty$ Cluster}}[][p]
\end{align}
In diesem Fall ist $\propE{\text{w hat einen $\infty$ Cluster}}[][p] \in \set{0,1}$, da $\set{w \text{ hat einen $\infty$ Cluster}}$ nicht von endlich vielen Indikatoren $X_e(w), e \in E$ abhängt, folgt dies aus dem Kolmogorovschen-$0$-$1$-Gesetz. \marginnote{$X_e(w) = \begin{cases}
	0 &,e \in w \\
	1 &,e \notin w
	\end{cases}$} \todo{häßlich}
Es ist intuitiv klar, dass $p \to \propE{\text{w hat einen $\infty$ Cluster}}[][p]$ monoton wachsend ist. Mathematisch folgt das aus einem Kopplungsargument. Dafür seien für jedes $e \in E$ $U(e) \sim \mathcal{U}[0,1]$ i.i.d.. Für ein gegebenes $p$ und eine Realisierung der $\big(U(e) \big)_{e \in E}$ sei nun $X(e)= 1$ für $U(e) \leq p$ und $0$ sonst. $X(e)$ ergibt eine Bernoulli-Kantenperkolation mit Parameter $p$. Ist nun $p' > p$ realisiere zu jeder festen Realisierung der $\big(U(e)\big)$ und
\begin{align}
	X'(e) = \begin{cases}
				1, & U(e) = p' \\
				0, & sonst
			\end{cases}
\end{align}
$\big(X'(e)\big)$ ist eine $\Ber(p')$-Perkolation. Hat nun die Realisierung der $\big(X(e)\big)$ ein $\infty$ Cluster, so auch die der $\big(X'(e)\big)$. Daher können wir 
\begin{align}
	p_c(G) := \sup\set{p \given \propE{w\text{ hat ein $\infty$ Cluster}} = 0 }
\end{align}
als die kritische Perkolationswahrscheinlichkeit definieren.

\begin{uebung}
	Wenn $G$ zusammenhängend ist, dann ist 
	\begin{align}
		p_c(G):= \sup\set{p \given \prop{\abs{k(x)} = \infty } = 0}
	\end{align}
	Im Allgemeinen ist $p_c(G)$ sehr schwierig zu berechnen. Bekannt ist $p_c(\ZZ)$ und $P_c(\ZZ^2) = \frac{1}{2}.$ Für $P_c(\ZZ^3)$ existiert noch nicht einmal eine Vermutung (der Physiker).
\end{uebung}

Daher betrachten wir Perkolation auf Bäumen. Hier kann man versuchen $\propE{0 \leftrightarrow \infty} \leq \inf\limits_{\Pi \text{ trennt }0 \leftrightarrow \infty}\sum\limits_{e \in \Pi} \propE{0 \leftrightarrow e}[p]$ auszurechnen. \todo{häßlich}
Der Vorteil eines Baumes ist, dass $\prop{0 \leftrightarrow e}[T] = p^{\abs{e}}$
Daher: Ist $p < \frac{1}{br(T)}$, wobei 
\begin{align} 
	br(T) = \sup\limits_{\lambda}\benb{\inf\limits_{\Pi} \sum\limits_{e \in \Pi} \lambda^{-\abs{e}} > 0 }. 
\end{align}
Dann folgt
\begin{align}
	\inf\limits_{\Pi} \sum\limits_{e \in \Pi} p^{\abs{e}} = 0,
\end{align}
weil $\frac{1}{p} > br(T) \Rightarrow \inf\limits_{\pi} \sum\limits_{e \in \Pi} (\frac{1}{p})^{-\abs{e}} = 0$. Dann ist also $\prop{0 \leftrightarrow \infty}[][p] = 0$, womit aus \autoref{satz:6-3} 
\begin{align}
	p_c(T) \geq \frac{1}{br(T)} \label{eqn:6-1}
\end{align}

Es gilt in \eqref{eqn:6-1} sogar Gleichheit, dazu benötigen wir allerdings etwas mehr Technik. Besonders übersichtilich ist die Situatuin, wenn $T$ $n$-regulär ist, d.h. jeder Knoten hat $n$ Kinder. Lässt man auf einem $n$-regulären Baum $\Ber(p)$-Kantenperkolation los, so ergibgt sich ein Galton-Watson-Baum mit $B(n,p)$-verteilter Kinderzahl. Also (da $\EW{B(n,p)} = np$) folgt, dass $p_L (T) = \frac{1}{n}$. Sehr ähnlich zeigt man
\begin{satz}[Lyons '90]
	\label{satz:6-4}
	Sei T ein superkritischer Galton-Watson-Baum mit $\EW{L} = m > 1$. Bedingt darauf, dass $T$ unendlich ist, ist $p_L(T) = \frac{1}{m}$ fast sicher. 
\end{satz}
\begin{beweis} Nur Beweisidee: Wenn man GW und Perkolation simultan laufen lässt, so ist dies wieder ein G-W-Prozess mit Wahrscheinlichkeit $pm$. Also überlebt er genau dann, wenn $p > \frac{1}{m}$. Da die endlichen $T$'s kein endliches unendliches Cluster erzeugen können, folgt mit ein paar Berechnungen die Behauptung. 
\end{beweis}

\marginnote{Vorlesungsbeginn 03.05.2016}

%Wir betrachten Perkolation auf unendlichen Graphen $G= (V,E)$, d.h. eine Teilmenge $E'(w) \subseteq E$ wird gemäß eines Zufallsmechanismus entfernt. \\
%\underline{Zentrale Frage:} Gibt es in $(V,E\backslash E')$ ein unendlich großes Cluster?
%
%Wir betrachten meist $Ber(p)$-Perkolation, d.h. man behält Kanten unabhängig voneinander mit Wahrscheinlichkeit $o \Rightarrow \prop{\exists \infty \text{ Cluster}}[][p] \in \set{0,1}$. Da $p \rightarrow \prop{\exists \infty \text{ Cluster}}[][p]$ monoton, existiert
%\begin{align}
%	p_c = \sup\set{p \given \prop{\exists \infty \text{ Cluster}}[][p] = 0}
%\end{align}
%
%Wir haben gezeigt:
%\begin{satz}
%	\begin{align}
%		\propE{x \leftrightarrow \infty} \leq \inf\set{\sum\limits_{e \in \Pi} \propE{x \leftrightarrow e} \given \Pi \text{ trennt $x$ von $\infty$}  }		
%	\end{align}
%	Daraus haben wir gefolgert: Ist $G$ ein Baum, 
%
%\end{satz}

\begin{korollar}
	Sei $T$ die Realisierung eines Galton-Watson-Prozesses. Dann gilt, bedingt auf das Überleben 
	\begin{align}
		br(T) = m, f.s.
	\end{align}
\end{korollar}
\begin{beweis}
	Aus \autoref{satz:6-4} folgt
	\begin{align}
		br(T) \geq \frac{1}{p_c(T)} = m
	\end{align}
	Aus der folgenden Übung ergibt sich $br(T) \leq gr(T) = m$
\end{beweis}
\begin{uebung}
	Man zeige für einen Galton-Watson-Baum $T$ mit $m = \EW{L} > 1$, dass bedingt auf Überleben $gr(T) = m$ ist.
\end{uebung}

\subsection*{Zweite Momentenmethode} \todo{nummerierung}
Die erste Momentenmethode erlaubt eine (ganz einfache) obere Schranke an $\propE{x \leftrightarrow \infty}$ zu geben. Für untere Schranken benötigt man typischerweise Kenntnisse über das Fluktuationsverhalten der mitspielenden Zufallsvariablen.
So etwas erhält man typischerweise aus Chebyshev-Ungleichungen bzw. der 2. Momentenmethode. Wir betrachten hier eine \enquote{gewichtete} 2. Momentenmethode, d.h. für eine Menge $\Pi$, die $x$ von $\infty$ trennt, werden wir $\prop{o \leftrightarrow e}, e\in \Pi$ mit dem anderen Gewocht $\mu(e)$ abschätzen. Wir werden von nun an wieder $x = o$ wählen und $k(o)$ sei das Cluster von $o$. Sei weiter $\Pi$ ein $o \leftrightarrow \infty$-Cutset und $\mu \in M^1(\Pi)$. Wir betrachten
\begin{align}
	X(y) = \sum\limits_{e \in \Pi} \mu(e) \mathds{1}_{\set{e \in k(o)}} \frac{1}{\prop{c \in k(o)}}
\end{align}
Nebenrechnung: $\prop{o \leftrightarrow \infty} = \inf \prop{o \leftrightarrow \Pi \given \Pi \text{ ist ein $o$-$\infty$-Cutset}}$

\begin{bemerkung}
	\begin{align}
		\EW{X(\mu)} = \EW{\sum\limits_{e \in \Pi} \mu(e) \frac{\mathds{1}_{\set{e \in k(o)}}}{\prop{e \in k(o)}}} = \sum\limits_{e \in \Pi} \mu(e) \frac{\prop{e \in k(o)}}{\prop{e \in k(o)}} = 1
	\end{align}
Sei $\set{o \leftrightarrow \Pi} := \set{\omega \given \exists \ e \in \Pi: o \leftrightarrow e}$. Dann ist 
\begin{align}
	\set{\omega \given X(\mu) > 0} \subseteq \set{\omega \given o \leftrightarrow \Pi} \Rightarrow \prop{X(\mu) > 0} \leq \prop{0 \leftrightarrow \Pi}
\end{align}
\end{bemerkung} 
Da man $\prop{o \leftrightarrow \infty}$ über das Infimum von $\prop{o \leftrightarrow \Pi}$ bestimmen kann, ergibt eine untere Schranke an $\prop{X(\mu) > 0}$ auch eine untere Schranke an $\prop{o \leftrightarrow \infty}$. Das funktioniert wie folgt.

\begin{satz}
	\label{satz:6-8}
	Es sei ein allgemeines Perkolationsmodell auf einem Graphen $G$ gegeben. Dann gilt für $\mu \in M^1(\Pi)$ und jedes $o$-$\infty$-Cutset $\Pi$
	\begin{align}
		\propE{o \leftrightarrow \infty} > \frac{1}{\EW{X^2(\mu)}}
	\end{align}
\end{satz}
\begin{beweis}
	Sei $\Pi$ ein Cutset und $\mu \in M^1(\Pi)$. Dann gilt mit Cauchy-Schwarz
	\begin{align}
		1 = \enb{\EW{X(\mu)}}^2 = \EWE{X(\mu) \mathds{1}_{\set{X(\mu) > o}}}^2 &\leq \EW{X^2(\mu)} \prop{X(\mu) > o} \\
							& \EW{X^2(\mu)} \prop{o \leftrightarrow \infty} 
	\end{align}
	Der Trick ist jetzt: 1. Wähle das \enquote{richige} $\mu$. 2. Schätze $\EW{X^2(\mu)}$ ab.
\end{beweis}

Die Schranken aus \autoref{satz:6-8} ist nur dann effektiv, wenn wir $\EW{X^2(\mu)}$ bestimmen können. Offenbar
\begin{align}
	\EW{X^2(\mu)} = \sum\limits_{e_1,e_2 \in \Pi} \mu(e_1) \mu(e_2) \frac{\prop{e_1,e_2 \in k(o)}}{\prop{e_1 \in k(o)} \prop{e \in k(o)}}
\end{align}

\begin{definition}
	\begin{align}
		\mathcal{E}(\mu) := \sum\limits_{e_1,e_2 \in \Pi} \mu(e_1) \mu(e_2) \frac{\prop{e_1,e_2 \in k(o)}}{\prop{e_1 \in k(o)} \prop{e \in k(o)}}
	\end{align}
	heißt \emph{Energie} von $\mu$
\end{definition}

\begin{korollar}
	\label{kor:6-10}
	\begin{align}
		\prop{o \leftrightarrow \infty} \geq \sup\limits_{\Pi\text{ Cutset von } o\leftrightarrow \infty} \Bigg\{\frac{1}{\inf\limits_{\mu \in M^1(\Pi)} \mathcal{E}(\mu)}\Bigg\}
	\end{align}
\end{korollar}
\begin{beweis}
	Wir haben das schon für alle $\Pi$ und $\mu \in M^1(\Pi)$ gesehen.
\end{beweis}
Damit wir weiterkommen benötigen wir nun Abschätzungen an $\mathcal{E}(\mu)$. Im Allgemeinen kann das schwierig sein, für $G=T$ (Bäume) ist es hingegen einfacher. Sei nun $G =  T = (V,E)$ ein Baum und darauf unabhängige $\Ber(p)$-Kantenperkolationen. Wir identifizieren die Kante in einem Vertex $x$ oft mit diesem Vertex und schreiben $e(x)$ für diese Kante. Für $\mu(e(x))$ schreibe auch $\mu(x)$. Sei nun $\Pi$ ein Cutset in einem Baum $T$ und $\mu \in M^1(\Pi)$. Dann gilt:
\begin{align}
	\mathcal{E}(\mu) &= \sum\limits_{e(x),e(y) \in \Pi} \mu(x)\mu(y) \frac{\prop{x,y\in k(o)}}{\prop{x \in k(o)} \prop{y \in k(o)}}  \\ \marginnote{wobei $x \land y$ der jüngste gemeinsame Vorfahre von $x$ und $y$ ist.}
					&= \sum\limits_{e(x),e(y) \in \Pi} \mu(x) \mu(y) \frac{\prop{x \land y \in k(o)} \prop{x \land y \leftrightarrow x} \prop{x \land y \leftrightarrow y} }{\prop{x \land y \in k(o)} \prop{x \land y \leftrightarrow x} \prop{x \land y \in k(o)} \prop{x \land y \leftrightarrow y} } \\
					&=\sum\limits_{e(x),e(y) \in \Pi} \mu(x) \mu(y) \frac{1}{\prop{o \leftrightarrow x \land y}}
\end{align}

\todo{Mehr erklärung?}
Das hat eine gewisse Ähnlichkeit mit der Rechnung für Flüsse.
\begin{lemma}
	Sei $\Theta$ ein Fluss auf einem endlichen Baum $T$ von der Wurzel $o$ in die Blätter $\delta T$. Dann gilt
	\begin{align}
		\mathcal{E}(\Theta) = \sum\limits_{x,y \in \delta T}\Theta(x) \Theta(y) \mathcal{R}(o \leftrightarrow x\land y) \marginnote{$\Theta(x) = \Theta(e(x))$}
	\end{align}
\end{lemma}
\begin{beweis}
	Benutze $\sum\limits_{x \in \delta T: e \leq x} \Theta(x) = \Theta(e), \forall e$. Dann
	\begin{align}
		\sum\limits_{x,y \in \delta T} \Theta(x) \Theta(y) \mathcal{R}(o \leftrightarrow x \land y) &= \sum\limits_{x,y \in \delta T} \Theta(x) \Theta(y) \sum\limits_{e \leq x\land y} r(e) \\
			&= \sum\limits_{e} r(e) \sum\limits_{\substack{x,y \in \delta T \\ x,y \geq e}} \Theta(x) \Theta(y)\\
			&= \sum\limits_{e} r(e) \Theta^2(e) = \mathcal{E}(\Theta)
	\end{align}
\end{beweis}
Schön wäre es nun diese Beobachtungen zu verknüpfen: Man wählt für $\Theta$ den durch $\mu$ induzierten Fluss $\Theta(e):= \sum\limits_{e \leq x \overline{\Pi}}\mu(x).$\marginnote{$\overline{\Pi} = \set{x \given x(r) \in \Pi}$} Dann ist $\Theta(e(x)) = \mu(e(x)), \forall x \in \delta T$. Kann man auch noch Leitfähigkeiten wählen, sodass $\prop{o \leftrightarrow x}  = \mathcal{C}(o \leftrightarrow x) (*)$ gilt, so folgt $\mathcal{E}(\mu) = \mathcal{E}(\Theta)$. Das geht leider nicht: $(*)$ ist z.B. für $o = x$ falsch. \\
\underline{2. Versuch:} Können wir Leitfähigkeiten $c$ so bestimmten, dass 
\begin{align}
	\frac{1}{\prop{o \leftrightarrow x}} = 1+\mathcal{R}(o \leftrightarrow x) 
\end{align}
so gilt:
\begin{align}
	\mathcal{E}(\mu) = \mathcal{E}(\Theta) + 1 
\end{align}
was dafür, Schranken an $\mathcal{E}(\mu)$ zu erhalten, ebenso gut ist. Falls $c$ so gewählt ist, dass \autoref{kor:6-10} richtig ist, heißt $c$ an das \emph{Perkolationsproblem angepasst}. 

Sei von nun an $G$ ein Baum. Beobachtung: Jetzt ist \marginnote{Vorlesungsbeginn 02.06.2016}
\begin{align}
	\mathcal{E}(\mu)  = \sum\limits_{e(x),e(y)}\mu(x)\mu(y) \frac{1}{\propE{o \leftrightarrow x \land y}}
\end{align}
Andererseits gilt für Flüsse $\Theta$ auf $G$ von $o$ in $\delta T$ (wenn $G=T$ endlich ist)
\begin{align}
	\mathcal{E}(\Theta) = \sum\limits_{x,y \in \delta T}\Theta(x) \Theta(y) \mathcal{R}(o \leftrightarrow x\land y)
\end{align}
Suche nun Leitfähigkeiten $c$, sodass $\frac{1}{\propE{o \leftrightarrow x}} = \mathcal{R}\enb{o \leftrightarrow x} +1, \forall x$. Dann $\mathcal{E}(\mu) = 1 + \mathcal{E}(\Theta)$ für den durch $\mu$ induzierten Fluss, d.h. man kann statt mit $\mathcal{E}(\mu)$ mit $\mathcal{E}(\Theta)$ arbeiten. 
Wie findet man diese Leitfähigkeiten? Da es zu jedem $x$ in einem Baum genau einen Pfad von $o$ nach $x$ gibt, ist $\mathcal{R}\enb{o \leftrightarrow x}$ nichts anderes als der Widerstand einer Reihenschaltung. Sei nun $e(x) = \enb{\overleftarrow{x},x}$, dann ist 
\begin{align} 
		r(e(x)) &= \mathcal{R} (o \leftrightarrow x) - \mathcal{R}\enb{o \leftrightarrow \overleftarrow{x}} = \mathcal{R} \enb{o \leftrightarrow x} + 1 - \enb{\mathcal{R}(o \leftrightarrow \overleftarrow 
		{x}) +1} \\
		&= \frac{1}{\prop{o \leftrightarrow x}} - \frac{1}{\prop{o \leftrightarrow \overleftarrow{x}}} = \frac{1}{\prop{o \leftrightarrow x}} - \frac{p_x}{\prop{o \leftrightarrow \overleftarrow{x}} p_x } = \frac{1-p_x}{\prop{o \leftrightarrow x}} \marginnote{$p_x = \prop{e(x) = 1}$} \\
		\Rightarrow c(e(x)) = \frac{\prop{o \leftrightarrow x}}{1-p_x}
\end{align}
Insbesondere gilt ür $\Ber(p)$-Perkolation $c(e(x)) = \frac{p^{\abs{x}}}{1-p}$. Das sind gerade die Leitfähigkeiten des $\text{RW}_{\lambda}$-randomwalks auf $T$ mit $\lambda = \frac{1}{p}$. Wir können jetzt zeigen:
\begin{satz}[Lyons '90]
	Für unabhängige Perkolation mit Wahrscheinlichkeit $p$ auf dem Baum gilt
	\begin{align}
		\propE{o \leftrightarrow \infty} \geq \frac{\mathcal{C}(o \leftrightarrow \infty)}{1+ \mathcal{C}\enb{o \leftrightarrow \infty}}
	\end{align}	
\end{satz}
\begin{beweis}
	Erinnerung: es gilt $\propE{o \leftrightarrow \infty} \geq \inf\limits_{\substack{\Pi \text{ ist } o\leftrightarrow \infty \\ \text{Cutset}}} \set{\frac{1}{\inf\limits_{\mu \in M^1(\Pi)} \mathcal{E}(\mu)}}$.
	Wir schätzen das Infimum rechts ab. Gegeben sei ein bezüglich \enquote{$\subseteq$} minimales Cutset. Sei $\mu$ eine Wahrscheinlichkeit auf $\Pi$ mit minimaler Energie. Sei $\Theta$ der induzierte Fluss von $o$ nach $\overline{\Pi} = \set{x \given \enb{\overleftarrow{x},x} \in \Pi}$. Das ist auch ein Fluss minimaler Energie. 
	Seien nun die Leitfähigkeiten an das Perkolationsproblem angepasst. Dann gilt
	\begin{align}
		\mathcal{E}(\mu) = 1 + \mathcal{E}(\Theta) = 1+ \mathcal{R}\enb{o \leftrightarrow \overline{\Pi}}
	\end{align}
	d.h. 
	\begin{align}
		\propE{o \leftrightarrow \infty} \geq \inf\limits_{\Pi} \frac{1}{\mathcal{E(\mu)}} = \inf\limits_{\Pi} \frac{1}{1+\mathcal{R}\enb{o \leftrightarrow \overline{\Pi}}} = \frac{1}{1+ \mathcal{o \leftrightarrow \infty}} = \frac{1}{1 + \frac{1}{e \enb{o \leftrightarrow \infty}}} = \frac{e\enb{o \leftrightarrow \infty}}{1 + e(o \leftrightarrow \infty)}
	\end{align}
\end{beweis}
	
\subsection{Rekurrenz und Transienz von Perkolationsclustern}
$p_c(G)$ ist für viele Graphen insbesondere aber für $G = \ZZ^d$ schwer zu berechnen. Vielleicht ist die folgende Frage einfacher. Ist \underline{das} $\infty$-Perkolationscluster von $\ZZ^d$ rekurrent oder transient?. Die Frage ist für $d=1,2$ trivial, da das Cluster ein Teilgraph eines rekurrenten Graphen ist, folgt die Behauptung aus dem Prinzip von Raleigh (\ref{satz:Rayleigh}).

Sei $d \geq 3$. Wir brauchen ein technisches Hilfsmittel, das aber allgemein interessant ist. 

\begin{satz}[Pakey-Zygmund-Ungleichung]
	Sei $X$ eine Zuvallsvariable mit $\EW{X} = 1$ und existiere $\EW{X^2}$. Dann gilt für alle t < 1
	\begin{align}
		\prop{X > t} \geq \frac{\enb{1-t}^2}{\EW{X^2}}
	\end{align}
\end{satz}
\begin{beweis}
	Das Bemerkenswerte an dieser Ungleichung ist, dass sie eine nicht-triviale untere Schranke an die Wahrscheinlichkeit legt. Sei $A = \set{w \given X(w) \geq t}$. Dann gilt 
	\begin{align}
		\EW{X^2} \prop{A} = \EW{X^2} \EW{\mathds{1}^2_A} \geq \enb{\EW(X\mathds{1}_A)}^2 = \enb{1 - \EW{X\mathds{1}_{A^c}}}^2 \geq \enb{1 - t}^2, \marginnote{$1 = \EW{X} = \EW{X\mathds{1}_A} + \EW{X\mathds{1}_{A^c}}$}
	\end{align}
	wobei die erste Ungleichung aus Cauchy-Schwarz folgt und die Zweite gilt da, die linke Seit für $w\in A,$ gleich $1^2$ und für $w\in A^c$ größer gleich $(1-t)^2$ ist.
\end{beweis}
		
Um diese Ungleichung anzuwenden müssen wir Wahrscheinlichkeiten definieren. Wir wollen hierfür Wahrscheinlichkeiten auf den Pfaden von einem Konten $a$ nach $t$ (oder $\infty$) in einen Graphen $G = (V,E)$ definieren. Anders als vorher sind dies keine Wahrscheinlichkeiten auf Cutsets. Die Kernidee wird sein, dass solche Pfade Flüssen identifizieren und die Frage ist, ob wir die Wahrscheinlichkeiten auf der Pfadmenge so wählen können, dass die assoziierten Flüsse endliche Energie haben (Weil wir gesehen haben, dass die Existenz von Flüssen endlicher Energie die Transienz des Graphen impliziert). Wir beginnen in einer endlichen Situation. 

Es sei $\Gamma= \set{\gamma \text{ ist ein Pfad von $a$ nach $t$} }, a,z \in \infty$-Perkolationscluster.

Sei $\mu$ ein Wahrscheinlichkeitsmaß auf $\Gamma$ und sei $\p$ das Maß der $\Ber(p)$-kantenperkolation auf $\ZZ^d$. Gegeben eine Realisierung $\omega$ der Perkolation, definiere ein positives Maß $Y_{\mu}(\omega)$ auf $\Gamma$ vermöge
\begin{gather}
	Y_{\mu}(\omega)[\gamma] = \begin{cases}
								\frac{\mu(\gamma)}{\propE{\gamma \text{ offen}}}, & \text{falls } \gamma \subseteq \omega \\
								0, & \text{sonst}
						\end{cases}		
\end{gather} 
Diesem positiven Maß kann man nun einen Fluss $\Theta_{\mu} (\omega)$ zuordnen, wobei 
\begin{align}
	\Theta_{\mu}(\omega) = Y_{\mu}(\omega)[e \in \gamma] - Y_{\mu}(\omega)[-e \in \gamma]
\end{align}
Das $Y$ ist zwar ein positives Maß, aber nicht notwendig ein Wahrscheinlichkeitsmaß, d.h. $\Theta_{\mu}(\omega)$ ist nicht notwendig ein Einheitsfluss. Man berechnet 
\begin{align}
	S (\Theta_{\mu}(\omega)) = X_{\mu}(\omega) \sum\limits_{\gamma \subseteq \omega} \frac{\mu(\gamma)}{\propE{\gamma \subseteq \omega}} 
\end{align}
\underline{N.B.} S ist eine Zufallsvariable. Es gilt $\EW{X_{\mu}}(\omega) = \sum \mu(\gamma) = 1$ und
\begin{align}
\EW{X^2_{\mu}}(\omega) &= \sum\limits_{\gamma_1,\gamma_2}\mu(\gamma_1) \mu(\gamma_2) \frac{\prop{\gamma_1 \cup \gamma_2 \subseteq \omega}}{\prop{\gamma_1 \text{ offen}} \prop{\gamma_2 \text{ offen}}} \\
	&= \sum\limits_{\gamma_1,\gamma_2} \mu(\gamma_1)\mu(\gamma_2) \frac{p^{\abs{\gamma_1 \cap \gamma_2}} p^{\abs{\gamma_2 \backslash \enb{\gamma_1 \cap \gamma_2}}} p^{\abs{\gamma_1 \backslash \gamma_1 \cap \gamma_2}} } {p^{\abs{\gamma_1 \cap \gamma_2}} p^{\abs{\gamma_2 \backslash \enb{\gamma_1 \cap \gamma_2}}} p^{\abs{\gamma \cap \gamma 2}}} {p^{\abs{\gamma_1 \backslash \gamma_1 \cap \gamma_2}}} \\ 
	&= \sum\limits_{\gamma_1, \gamma_2} \mu(\gamma_1)\mu(\gamma_2) p^{-\abs{\gamma_1 \cap \gamma_2}}
\end{align}
was eine erstaunliche Ähnlichkeit zu den Formeln oben hat. \\
Ähnlich
\begin{align}
	\mathcal{E}\enb{\Theta_{\mu}(\omega)} = \sum\limits_{e} \Theta^2_{\mu} (\omega)[e] \underbrace{r(e)}_{=1} \leq \sum\limits_{e}\enb{ \sum\limits_{\gamma \ni e} \frac{\mu(\gamma) \mathds{1}_{[\gamma \subseteq \omega]}}{\propE{\gamma \subseteq \omega}}}^2
\end{align}		
und damit 
\begin{align}
	\mathcal{E}(\Theta_{\mu}(w)) &= \sum\limits_{e} \sum\limits_{\gamma_1,\gamma_2 \ni e} \mu(\gamma_1)^\mu(\gamma_2) p^{-\abs{\gamma_1 \cap \gamma_2}} \\
		&= \sum\limits_{\gamma_1,\gamma_2}\abs{\gamma_1 \cap \gamma_2} \mu(\gamma_1) \mu(\gamma_2) p^{-\abs{\gamma_1 \cap \gamma_2}} \\
		&= \sum\limits_{n \geq 1} np^{-n} (\mu \times \mu) \benb{\abs{\gamma_1 \cap \gamma_2} = n}
\end{align}
Strategie: finde $\mu$, sodass das endlich ist. 
		
\marginnote{Vorlesungsbeginn 06.06.2016}
Eine kurze Rekapitulation: Wir waren dabei zu studieren, ob und wann Perkolationscluster in $\ZZ^d$ rekurrent oder transient sind. Ohne Beschränkung ist $d \geq 3$, da für $d=1,2$ die Irrfahrt schon rekurrent ist (Rayleigh). Wir haben auch schon den Satz von Poley-Zigmund gesehen.

Für einen endlichen Graphen $G = (V,E)$ mit $a,z \in V$ sei $\Gamma = \set{\gamma\given \gamma \text{ist Pfad von $a$ nach $z$}}$ und $\mu \in M^1(\Gamma)$. Gegeben eine Realisierung $\omega$ der Perkolation, sei $Y(\omega)$ das folgende positive Maß auf $\Gamma$:
\begin{align} 
	Y_{\mu} (\omega) \land \benb{\gamma} = \begin{cases}
	\frac{\mu(\gamma)}{\prop{\gamma \subseteq \omega}} & , \gamma \subseteq \omega \\
	0 & , \text{ sonst}
	\end{cases}
\end{align}
$Y_{\mu}(\omega)$ induziert einen Fluss $\Theta_{\mu}(\omega)$ der Stärke 
\begin{align}
	X_{\mu}(\omega) = \sum\limits_{\gamma \subseteq \omega} \frac{\mu(\gamma)}{\prop{\gamma \text{ offen}}} && \EW{X_{\mu}} = 1 && \EW{X_{\mu}^2} = \sum\limits_{\gamma, \gamma'} \mu(\gamma) \mu(\gamma') p^{-\abs{\gamma \cap \gamma'}} 
\end{align}
Für die Energie des Flusses haben wir gefunden, dass 
\begin{align*}
	\EW{\mathcal{E}(\Theta_{\mu})} \leq \sum\limits_{n \geq 1} n p^{-h} \enb{\mu \times \mu}(\abs{\gamma \cap \gamma'} = n) \tag{$*$} \label{eq:6-3}
\end{align*}
Wir haben noch die Freiheit $\mu \in M^1(\Gamma)$ zu wählen und wir versuchen $\mu$ so zu wählen, dass die rechte Seite von \eqref{eq:6-3} ist. Hierfür hilft es, wenn $(\mu \times \mu)(\abs{\gamma \cap \gamma'} = n)$ schnell (in $n$) abfällt. Wir sagen, dass $\mu$ einen \emph{Exponential-Intersection-Teil} mit Parameter $\xi < 1$ hat, falls
\begin{align}
	\enb{\mu \times \mu}\enb{\abs{\gamma \cap \gamma'} = n} \leq C \cdot \xi^n \tag{EIT($\xi$)}\label{eq:EIT}
\end{align} 
Wir zeigen 
\begin{enumerate}
	\item \ref{eq:EIT} $\Rightarrow$ Transienz von Perkolationsclustern für hinreichend großes $p$.
	\item In $\ZZ^d, d \geq 4$ gibt es Maße mit \ref{eq:EIT}
\end{enumerate}

\begin{satz}
	Falls $\mu \in M^1(\Gamma)$ mit \ref{eq:EIT}$, \xi < 1 $ existiert, dann ist der $\Ber(p)$-Perkolationscluster für $1 > p > \xi$ $\p$-f.s transient.
\end{satz}
\begin{beweis}
	Die Wahrscheinlichkeit, dass ein $\Ber(p)$-Perkolationscluster transient ist, ist entweder 0 oder 1, denn wenn $\omega$ transient, dann hängt es nur von den Variablen außerhalb jedes Würfels ab. Die Kernidee ist nun, den effektiven Widerstand von $o$ nach $\infty$ abzuschätzen. Dazu verkleben wir wieder alle Punkte außerhalb von $\benb{-r,r}^d$ zu einem Punkt $z_r$ und erhalten einen endlichen Graphen. In diesem schätzen wir $\mathcal{R}(o \leftrightarrow z_r)$ ab. Es gilt für einen beliebigen Einheitsfluss $\tau$
	\begin{align}
		\mathcal{R}\enb{o \leftrightarrow z_r} \leq \mathcal{E}(\tau)
	\end{align}
	und somit
	\begin{align}
		\mathcal{R}(o \leftrightarrow z_r;\infty) \leq \mathcal{E}(\overline{\Theta}_{\mu,r}) = \frac{\mathcal{E}(\Theta_r)}{X^2_r} && \text{mit \quad } \overline{\Theta}_{\mu,r} =: \frac{\Theta_{\mu,r}}{X_{\mu,r}}=:\frac{\Theta_r}{X_r}
	\end{align}
	Dies wollen wir abschätzen.	Berechnet man 
	\begin{align}
		\EW{X^2_r} = \sum\limits_{\gamma,\gamma'}\mu(\gamma)\mu(\gamma')p^{-\abs{\gamma \cap \gamma'}} = \sum\limits_{n \geq 1} p^{-n} \enb{\mu \times \mu}\enb{\abs{\gamma \cap \gamma'} = n} \leq C \sum\limits_{n \geq 1} \enb{\frac{\xi}{p}}^n = C \frac{\xi}{p - \xi}
	\end{align}
	Erinnerung:
	\begin{align}
		\EW{X_r} = 1 \Rightarrow \prop{X_r > \frac{1}{2}} \geq \frac{p - \xi}{4C\xi} =: \delta > 0
	\end{align}
	Andererseits:
	\begin{align}
		\EWE{ \mathcal{E}(\Theta_r)} \leq C \sum\limits_{n \geq 1} n \enb{\frac{\xi}{p}}^n = \frac{Cp\xi}{\enb{p-\xi}^2}=: N
	\end{align}
	\begin{align}
		\propE{\mathcal{E}(\Theta_r) \geq \beta } \leq \frac{n}{\beta} \text{ nach Markov}. 
	\end{align}
	Somit 
	\begin{align}
		\propE{\mathcal{R}(o \leftrightarrow z_r,w) < 4\beta} \geq \propE{X_r > \frac{1}{2}, \mathcal{E}(\Theta_r) \leq \beta} = \propE{X_r > \frac{1}{2}} - \propE{\mathcal{E}(\Theta_e)> \beta} \geq \delta-\frac{n}{p} = \frac{\delta}{2} 
	\end{align}
	für $\frac{n}{\beta} = \frac{\delta}{2}$. Hängt nicht von $r$ ab.

	\begin{align}
		r &\mapsto \mathcal{R}(o \leftrightarrow z_r,w) \text{ ist wachsend} \\
		r &\mapsto \propE{\mathcal{R}(o \leftrightarrow z_r,w) \leq \beta} \text{ ist somit fallend.}
	\end{align}
	Allerdings sind die Wahrscheinlichkeiten uniform nach unten beschränkt. Also ist die Wahrscheinlichkeit $\propE{\mathcal{R}(o\leftrightarrow \infty,w) \leq 4\beta} = \propE{\bigcap_n \mathcal{R}(o \leftrightarrow z_r,w) \leq 4\beta} \geq \frac{\delta}{2}>0$ (Stetigkeit von Wahrscheinlichkeiten) Also ist $\mathcal{R}(o \leftrightarrow \infty,w)$ mit positiver Wahrscheinlichkeit endlich und d.h. mit positiver Wahrscheinlichkeit ist $C(o)$ transient, also gibt es mit Wahrscheinlichkeit 1 ein transientes, unendliches Cluster. 
\end{beweis}

Nun zur Transienz von Perkolationsclustern in $\ZZ^d, d\geq 4$.
\begin{satz}
	Für $d \geq 4$ gibt es auf $\ZZ^d$ Maße $\mu$ auf $\Gamma = \set{\gamma \given \gamma \text{ ist } 0 \leftrightarrow \infty \text{-Pfad}}$ mit EIT$(\xi)$ und $\xi<1$.
\end{satz}
\begin{beweis}
	ObdA sei $d=4$, denn auf $\ZZ^4$ . Sei $\mu$ das Maß auf $0 \leftrightarrow \infty$-Pfade, sodass diese Zuwächse der Form $(1,0,0,0), (0,1,0,0),(0,0,1,0),(0,0,0,1)$ mit Wahrscheinlichkeit $\frac{1}{4}$ haben. Wir wollen zeigen, dass dieses Maß EIT hat. Wir betrachten also $(\mu \times \mu)(\abs{\gamma \cap \gamma'} = n)$. Nun ist $\abs{\gamma \cap \gamma'} \leq \abs{\set{n \given \gamma(n) = \gamma'(n)}}$, da man keine Kante rückwärts durchläuft und sich immer von der $0$ wegbewegt, müssen $\gamma$ und $\gamma'$ die gleiche Kante auch zur gleichen Zeit benutzen. Damit müssen sie aber auch zur gleichen Zeit im Ursprungspunkt der Kante sein. 
	
	Studiere also die Tails von $\abs{\set{n \given: \gamma(n) = \gamma'(n)}}$. Dazu betrachte $\gamma'' = \gamma - \gamma'$. Dann gilt $\gamma(n) = \gamma'(n) \Leftrightarrow \gamma''(n) = 0$.
	Aufgrund der Konstruktion von $\gamma$ und $\gamma'$ ist $\gamma''$ eine Irrfahrt auf $V_3 = \set{(x_1,\dots, x_4) \in \ZZ^4 \given \sum\limits_{i =  1}^{4} x_i = 0}$.
	Das Netzwerk auf $V_3$ ist \enquote{beinahe dasselbe} wie $\ZZ^3$. Insbesondere ist $\gamma''$ auf $V_3$ transient. Insbesondere ist die Anzahl der Rückkehren in die $0$ geometrisch verteilt zu $\xi := \propE{\exists: \gamma(n) = 0}$. Also hat dieses $\mu$ EIT zu genau diesem $\xi$. 
\end{beweis}

\begin{korollar}
	Das Perkolationscluster in $\ZZ^d$ ist für $d \geq 4$ und $p > p_0 = \xi$ transient und unendlich. 
\end{korollar}
\begin{bemerkung}
	Die Aussage stimmt auch für $\ZZ^3$, der Beweis allerdings nicht.
\end{bemerkung}
\marginnote{\enquote{Es ist generell von unschätzbarem Vorteil bei mir Stochastik gehört zu haben}, Löwe 06.06.2016}
\section{Cutsets, Pfade und Mischzeiten}
Wir betrachten für dieses Kapitel ergodische Markovketten auf einem endlichen Zustandsraum. \underline{Erinnerung:} Eine Markovkette mit Übergangsmatrix $Q$ heßt ergodisch, wenn \underline{entweder}:
\begin{enumerate}
	\item $Q$ ist irreduzibel, d.h. $\forall x,y \exists n: Q^n(x,y)>0$ und
	\item $Q$ ist apersiondisch, d.h. $ggT\set{n \given Q^n(x,x) > 0} = 1$
\end{enumerate}
\underline{oder} $\exists N: Q^N \gg 0$, d.h. $Q^N(x,y)>0 \forall x,y$ \\
Wichtigster Satz über endliche Markovketten:
\begin{satz}[Ergodensatz für Markovketten]
	\label{satz:7-1}
	Sei $X_0,X_1,\dots$ eine ergodische Markovkette mit Startverteilung $v: \prop{X_0 = a} = v(a), \forall a$ und Übergangsmatrix $Q$ 
	auf einem endlichen Zustandsraum $\Omega$. Dann existiert genau ein $\mu \in M^1(\Omega)$ mit 
	\begin{enumerate}
		\item $\mu Q = \mu$
		\item $\prop{X_n = a}[][v] \to \mu(a) $
	\end{enumerate}
\end{satz}
\begin{bemerkung}
	Der Satz ist deshalb wichtig, weil er auch die Grundlage für Simulationsverfahren ist. Gegeben sei ein Wahrscheinlichkeitsmaß $\mu$ auf $\Omega$, welches man simulieren soll. Der Monte-Carlo-Ansatz ist, das Intervall $[0,1]$ in Teile der Länge $\mu(w_1), \dots, \mu(w_n), \abs{\Omega} = n$. Dann wählt man eine Zufallszahl $x \in [0,1]$ und wenn $x$ im $k$'-ten Intervall liegt, so sagt man, dass man $w_k$ als Ergebnis der Simulation erhält. Dieses Verfahren hat zwei mögliche Probleme. 
	\begin{enumerate}[a)]
		\item $\Omega$ kann endlich, aber groß sein, z.B. Curie-Weiss-Modell: $\Omega = \set{+- 1}^N$ mit $N \approx 10^{23}$. Somit ist $\abs{\Omega} \approx {2^{10}}^{23}$
		\item $\Omega$ kann endlich sein, aber $\abs{\Omega}$ ist unbekannt. Zum Beispiel im Knipsack-Problem: Gewichte $0 \leq a_1, \dots, a_N$ sollen in einen Rucksack der Kapazität $b$ gepackt werden. Wie viele Packungen gibt es?
	\end{enumerate}
\end{bemerkung}

\marginnote{Vorlesungsbeginn 09.06.2016}
	\underline{Beweisskizze zu \autoref{satz:7-1}} 
	\begin{enumerate}
		\item Die Existenz von $\pi$ glauben wir
		\item Mit etwas Zahlentheorie zeigt man: Wenn $Q$ ergodisch ist, dann existiert ein $N$, sodass $Q^N \gg 0$, d.h. $Q^N(x,y) > 0, \forall x,y \in E$
		\item Kopplungsargument: Wenn man mit der Startverteilung $\pi$ beginnt, so ist stets die Verteilung der Markovkette wieder $\pi$ (zu allen Zeiten). Man startet mit zwei Markovketten mit Übergangsmatrix $Q$, die eine $X_n$ mit Startverteilung $\nu$, die andere $X'_n$ mit Startverteilung $\pi$. Falls $X_m \neq X'_m, \forall m \leq n$, so laufen die Ketten unabhänig, vom Treffzeitpunkt an nehmen sie den gleichen Weg. Nachdem sich die Ketten treffen ist also auch $X_n$ nach $\pi$-verteilt. Zu zeigen ist also: Die Ketten treffen sich irgendwann. Da nun $Q^N \gg 0, \exists \alpha := \min\limits_{x,y} Q^N(x,y) > 0$. Dann ist die $\propE{X_N = X'_N} \geq \alpha^2$. Also $\propE{X_N \neq X'_N} \leq 1- \alpha^2$, und somit $\propE{X_n \neq X'_n, \forall n = 1, \dots, mN} \leq \enb{1- \alpha^2}^m \to 0.$
	\end{enumerate}

\begin{korollar}[Bemerkung \& Korollar]
	Dieser Satz lässt sich auch in anderen Metriken formulieren, typischerweise nimmt der Abstand der Totalvariation $d_{TV}$. Seien $\mu,\nu \in M^1(E)$, dann
	\begin{align}
		d_{TV} (\mu,\nu) &:= \sup\limits_{A \subseteq E}\abs{\nu(A) - \mu(A)} \\
		&= \frac{1}{\epsilon} \sum\limits_{x \in E}(\mu(\set{x}) - \nu(\set{x})  \marginnote{Da $E$ diskret}
	\end{align}
	Dann ließt sich der Ergodensatz für Markovketten als: Sei $(X_n)_n$ eine ergodische Markovkette auf $E$ mit Übergangsmatrix $Q$, dann gilt für alle $\nu \in M^1(E)$
	\begin{align}
		d_{TV}(\nu Q^n,\pi) = d(\nu Q^n,\pi) \to 0
	\end{align}	
\end{korollar}
Für alle Anwendungen ist allerdings die Frage der Konvergenzgeschwindigkeit essentiell. Die Konvergenz gegen das Gleichgewichtsmaß $\pi$ sollte wesentlich schneller sein als ein Monte-Carlo-Sample von $\pi$.
\begin{align}
	\nu Q^n = \nu S^{-1} D^n S = \nu S^{-1} \begin{psmallmatrix}
	1 & & & \\
	  & \lambda_2 &0 & \\
	  & 0 & \ddots & \\
	  & & &\lambda_{\abs{E}-1}
	\end{psmallmatrix}^n S
\end{align}
\begin{definition}
	Es sei $\epsilon > 0$, dann ist 
	\begin{gather}
		t_{mix}(\epsilon) = \min\set{t \given d_{TV}(\nu Q^t, \pi) \leq \epsilon} \\
		t_{mix} := t_{mix}(\frac{1}{4})
	\end{gather} 
	die $\epsilon$-Mischzeit bzw. die Mischzeit der Markovkette. Weiter sei 
	\begin{align}
		d(t) &= \max\limits_x \set{d\enb{\delta_x Q^t, \pi}} \\
		\overline{d}(t) &= \max_{x,y} \set{d\enb{\delta_x Q^t, \delta_y Q^t}}
	\end{align}
\end{definition}
\begin{bemerkung}
	Die Wahl von $\epsilon = \frac{1}{4}$ ist willkürlich, jedes $\epsilon < \frac{1}{2}$ liefert sinnvolle Ergebnisse, wie wir gleich sehen werden. 
\end{bemerkung}

\begin{lemma}
	Es gilt stets
	\begin{align}
		d(t) \leq \overline{d}(t) \leq 2d(t).
	\end{align}
\end{lemma}
\begin{beweis}
	Es gilt: 
	\begin{align}
		\overline{d}(t) &= \max\limits_{x,y} d(\delta_x Q^t, \delta_y Q^t) \\
			&\leq \max d\enb{\delta_x Q^t, \pi} + \max\limits_y \enb{\delta_y Q^t,\pi} \marginnote{Dreiecksungleichung} \\
			&= 2d(t)
	\end{align}
	Für die andere Richtung bemerke, dass mit $Q(y,A) = \sum\limits_{x \in A} Q(y,x), A \subseteq E$ gilt:
	\begin{align}
		\pi(A) = \pi(y) Q^{(t)}(y,A)
	\end{align}
	Also
	\begin{align}
		d(\delta_x Q^t,\pi) &= \max\limits_{A \subseteq \Omega} \abs{\delta_x Q^t (A) = \pi (A)} \\
			&= \max\limits_{A \subseteq \Omega} \abs{\sum\limits_{y \in \Omega} \pi(y) \benb{Q(x,A) - Q^t(y,A)} } \\
			&\leq \sum\limits_{y} \pi(y) \max\limits_{A} \abs{Q^t\enb{x,A} - Q^t(y,A) } \marginnote{Dreiecksungleichung} \\
			&= \sum\limits_{y}\pi d(\delta_x Q^t, \delta_y Q^t) \leq d(t) \marginnote{$\max\limits{x,y}$} \\
			&\Rightarrow \text{ Max über $x$ ergibt } d(t) \leq \overline{d}(t)
	\end{align}
\end{beweis}

\begin{lemma}
	$\overline{d}$ ist submultiplikativ, d.h. 
	\begin{align}
		\overline{d}(s + t) \leq \overline{d}(s) \cdot \overline{d}(t)
	\end{align}
\end{lemma}
\begin{beweis}
	Wir benutzen (ohne Beweis): Für $\mu,\nu \in M^1(E)$ gilt
	\begin{align}
		d(\mu,\nu) &= \inf\set{\prop{X \neq Y} \given (X,Y)\text{ ist eine } \mu,\nu \text{-Kopplung}} \\
				&=  \min\set{\prop{X \neq Y} \given (X,Y)\text{ ist eine } \mu,\nu \text{-Kopplung}}
	\end{align}
	$(X,Y)$ ist eine $\mu,\nu$-Kopplung, wenn $\p^X = \mu, \p^Y = \nu$. So etwas existiert immer, wenn man z.B. $\p^{X,Y} = \mu \otimes \nu$ wählt. Die Kopplung in der das Minimum angenommen wird, heißt optimale Kopplung. Seien nun $x,y\in E$ und für alle $t$ sei $(X_t,Y_t)$ die optimale Kopplung von $\delta_x Q^t$ und $\delta_y Q^t$. Dann gilt für alle $x,w \in E$
	\begin{align}
		Q^{t+s}(x,w) = \sum\limits_{z} Q^s(x,z) Q^t(z,w) =\sum\limits_{z} \propE{X_s = z}[X] Q^t(z,w) = \EW{Q^t(X_s,w)}
	\end{align}
	und
	\begin{align}
		Q^{t+s}(y,w) = \EW{Q^t(Y_s,w)}
	\end{align}
	Also:
	\begin{align}
		Q^{s+t}(x,w) - Q^{t+s}(y,w) &= \EWE{Q^t(X_s,w) - Q^t(Y_s,w)} \\
		&\Rightarrow d(\delta_xQ^{s+t}, \delta_y Q^{s+t}) = \frac{1}{2} \sum\limits_{w} \abs{\EWE{Q^t(X_s,w) - Q^t(Y_s,w)}} \marginnote{2. Def. von $d_{TV}$} \\
		&\leq \EWE{\frac{1}{2} \sum\limits_{w} \abs{Q^t(X_s,w) - Q^t(Y_s,w) }} \marginnote{Dreiecksungleichung} \\
		&= \EWE{d(Q^t(X_s,\cdot), Q^t(Y_s,\cdot)} \\
		&\leq \EWE{\overline{d}(t) \mathds{1}_{\set{X_s \neq Y_s}}},
	\end{align}
	weil $d \leq \overline{d}$ und die Ausdrücke gleich sind, wenn $X_s = Y_s$. Weiter:
	\begin{align}
		\overline{d}(t) \prop{X_s \neq Y_s} = \overline{d}(t) d(s) \leq \overline{d}(t) \overline{d}(s)
	\end{align}
	Also:  
	\begin{align}
		c \in \NN: d(ct) \leq \overline{d}(ct) \overline{d}(t)^c && \Longrightarrow && d(ct_{mix}(\epsilon)) \leq \overline{d}(t_{mix}(\epsilon))^c \leq \enb{2d(t_{mix} (\epsilon) )}^c \leq \enb{2\epsilon}^c
	\end{align}
	Also $d(ct_{mix}) \leq 2^{-c}$
	\begin{align}
		\Rightarrow t_{mix} (\epsilon) \leq \lceil\log_2 \frac{1}{\epsilon}\rceil t_{mix} 
	\end{align}
\end{beweis}

\marginnote{Vorlesungsbeginn 13.06.2016}
Zunächst zeigen wir untere Schranken für $t_{mix}(\epsilon)$. Die erste Idee basiert darauf, dass, wenn $\pi$ die Gleichverteilung ist auf $\Omega$ ist, man zumindest potenziell einen wesentlichen Anteil der Punkte gesehen haben können muss, um nahe an $\pi$ zu sein. \\
\underline{Umsetzung:} Es sei $(X_n)_n$ ergodisch auf $\Omega, \abs{\Omega} < \infty, \pi(\omega) = \frac{1}{\abs{\Omega}}, \forall \omega \in \Omega$ und definiere $d_{out} := \abs{\set{y \given Q(x,y) > 0}}$ \todo{check} als die Anzahl der Punkte, die man in einem Schritt von $x$ aus erreichen kann. 
\begin{align}
	\Delta = \max\limits_x d_{out}(x)
\end{align}
Dann gilt
\begin{satz}
	Es ist 
	\begin{align}
		t_{mix}(\epsilon) \geq \frac{\log(\abs{\Omega} ())}{\log(\Delta)} 
	\end{align}
\end{satz}\todo{check satz}
\begin{beweis}
	Es sei $\Omega^t_x = \set{y\given Q^t(x,y) > 0}$. Es ist $\abs{\Omega^t_x} \leq \Delta^t$. Ist nun $\Delta^t < (1-\epsilon)\abs{\Omega}$ so folgt
	\begin{align}
		&||Q^t(x,\cdot) - \pi||_{TV} = d_TV(\delta_x Q^t, \pi) \\
		\geq &\abs{Q^t(x,\Omega^t_x) - \pi(\Omega^t_x)} = 1 - \frac{\abs{\Omega^x_t}}{\abs{\Omega}} \geq 1 - \frac{\Delta^t}{\abs{\Omega}} \\
		> & 1-(1-\epsilon) =  \epsilon
	\end{align}
	Das heißt solange (*) \todo{ref} wahr ist, ist der totale Variationsabstand größer als $\epsilon$.
	\todo{check formel}
	\begin{align}
		&\Rightarrow t_{mix}(\epsilon) \text{ muss? } \Delta^{t_{mix}(c)} \geq (1-\epsilon)\abs{\Omega} \\
		&\Rightarrow t_{mix}(\epsilon) \geq\frac{\log\big((1-\epsilon) \abs{\Omega}\big)}{\log\Delta}
	\end{align}
 \end{beweis}

Wichtiger Unterschied: Man muss $t_{mix}(\epsilon)$ mindestens $(1-\epsilon)\abs{\Omega}$ gesehen haben können, man muss sie de facto nicht gesehen haben (Diese Zeit wäre viel viel größer). 
\begin{beispiel}[Irrfahrt auf einem $d$-regulären Graphen]
	Es sei $G = (V,E)$ $d$-regulär, das heißt $\deg(v) = d$. Wählt man darauf die einfache Irrfahrt $\prop{X_n = y \given X_{n-1} = x} = 
		\begin{cases}
			\frac{1}{d,} & \forall y \sim x \\
			0, \text{ sonst} 
		\end{cases}$
	Dann ist das stationäre Maß von $(X_n)$ die Gleichverteilung. $\pi(v)= \frac{1}{\abs{v}}$. Weiter ist $\Delta = d$ und das ergibt $t_{mix} \geq \frac{\log( \abs{V}(1-3))}{\log d}$ \\
	Spezialfall: $V = \set{0,1}^d$ (der Hyperwürfel)
	\begin{align}
		E = \set{\set{x,y} \given d_H(x,y) = \#\set{i \given x_i \neq y} = 1} \marginnote{$d_H$: Hammy}
	\end{align}
	Die Mischzeit für SRW wird hier beschränkt durch
	\begin{align}
		t_{mix}(\epsilon) \geq \frac{\log(\abs{\Omega} (1-\epsilon))}{\log d} = \frac{log(2^d (1-\epsilon) )}{\log d} = \frac{d \log 2}{\log d} + \frac{log(1-\epsilon)}{\log d}
	\end{align}
\end{beispiel}

Der folgende Ansatz verfolgt eine ähnliche Idee, gilt aber auch für allgemeine $\pi$. Beschränke nun $\overline{d}(t)$, das via $\overline{d}(t) \leq 2 d(t)$ auch eine Schranke für $d(t)-d$ somit für $t_mix$. Gegeben sei eine ergodische Markovkette auf $\Omega$. Fortan wird in diesem Kapitel $\Omega$ endlich sein, wenn nicht explizit anders deklariert. Wir machen$\Omega$ zu einem Graphen vermöge $V = \Omega$ und 
\begin{align}
	E = \set{ \set{x,y} \given Q(x,y), Q(y,x) > 0 }
\end{align}
Dieser Graph $G = (V,E)$ hat einen natürlichen Abstand $d_G$. 
\begin{align}
	diam G := \max\limits_{x,y \in V} d_G(x,y) \marginnote{\enquote{Ah! Groß \enquote{D}. Zum Glück hat Gott auch die Großbuchstaben erfunden}, Löwe 13.06.2016}
\end{align}
Wir sagen die Markovkette $(X)$ oder die Übergangsmatrix $Q$ hat den Durchmesser $D$, falls $diam G = D$. Falls nun $diam Q = D$, dann gibt es $x_ß,y_0 \in \Omega$ mit $d_G(x_0,y_0) =  D.$ Somit können sich zwei Markovketten mit Übergangsmatrix $Q$, von denen eine in $x_0$ und eine in $y_0$ startet zur Zeit $\lfloor\frac{D-1}{2}\rfloor$ noch nicht getroffen haben. 
\begin{align}
	\Rightarrow d_{TV} \Big( \delta_x Q^{\lfloor\frac{D-1}{2}\rfloor}, \delta_y  Q^{\lfloor\frac{D-1}{2}\rfloor}  \Big) = 1 \\
	\Rightarrow \overline{d}(\lfloor\frac{D-1}{2}\rfloor) > 1.
\end{align}
Da $\overline{d}(t) > 2d(t)$ heißt dann für $\epsilon< \frac{1}{2}$  \todo{missing zeile}
... also gezeigt.

\begin{satz}
	Gilt $diam Q = D$, so gilt für alle $\epsilon > \frac{1}{2}$
	\begin{align}
		t_{mix}(\epsilon) \geq \lfloor\frac{D-1}{2}\rfloor
	\end{align}
\end{satz}

Bislang haben wir wenig von der \enquote{Gemoetrie} von $(X_n)n$ benutzt. Die Idee \enquote{Um nahe an $\pi$ zu sein, muss man die Punkte, die $\pi$ mit hoher Wahrscheinlichkeit ausstattet gesehen haben können} wollen wir nun verknüpfen mit \enquote{das ist aber schwierig, wenn der Übergangsgraph Flaschenhälse hat}. \todo{Bild Flaschenhals}Gibt es relativ kleine Teilmengen, die man nur schlecht verlassen kann? Wir suchen ein Maß für \enquote{Flaschenhalsigkeit}. Dafür sei $Q$ ergodisch und reversibel auf $\Omega$. Wie schon in Kapitel 1-3 \todo{refs?} sei $\pi Q = \pi$, definiere $c(x,y) = \pi(x) Q(x,y) = \pi(y) Q(y,x).$ Es sei für $A,B \subseteq \Omega$
\begin{align}
	C(A,B) = \sum\limits_{x\in A} \sum\limits_{y \in B} c(x,y)
\end{align}
$C(A,B)$ ist die Wahrscheinlichkeit von $A$ nach $B$ zu laufen, wenn man mit $\pi$ startet. 
\begin{definition}
	Das \enquote{Flaschenhalsmaß} einer Menge $S \subseteq \Omega$ ist
	\begin{align}
		\Phi(S) = \frac{C(S,S^C)}{\pi(S)}
	\end{align}
	Ein \enquote{Flaschenhalsmaß} für die Markovkette $X_n$, die durch $Q$ beschrieben wird ist 
	\begin{align}
		\Phi_*(Q) = \min\limits_{S:\pi(S)\leq 1/2} \Phi(S)
	\end{align}
\end{definition}\todo{check phi* def}
\begin{beispiel}
	Sei (V,E) ein zusammenhängender Graph. Sei $X_n$ Simple Random Walk auf $G$, d.h. 
	\begin{align}
		Q(x,y) = \begin{cases}
				\frac{1}{\deg x}, & \text{wenn } x \sim y \\
				0,					& \text{sonst }
 		\end{cases}
	\end{align}
	Das invariante Wahrscheinlichkeitsmaß hierfür ist $\pi(x) = \frac{\deg x}{2\abs{E}}$. Dann ist 
	\begin{align}
		c(x,y) = \begin{cases}
					\frac{1}{\cancel{\deg x}} \cdot \frac{\cancel{\deg x}}{2 \abs{E}} = \frac{1}{2 \abs{E}}, &\text{wenn } x\sim y \\
					0,		&\text{sonst}
				\end{cases}
	\end{align}  
	das heißt
	\begin{align}
		C(S,S^c= = \frac{\# \set{(x,y) \given x \in S, y \in S^c} }{2 \abs{S}} = \frac{\abs{\delta S}}{2\abs{E}}
	\end{align}
	Somit 
	\begin{align}
		\Phi(S) = \frac{\abs{\delta S}}{\sum\limits_{x \in V}^{\deg x}}
	\end{align}
	Wenn wir also die Mischzeit in Bezug zu $\Phi_*$ setzen können, dann sind Mengen kleiner Oberflächen \enquote{schlimm} für die Markovkette.
\end{beispiel}
\begin{beweis}
		\todo{enumerate?}
	\begin{enumerate}
		\item Anstelle normaler Irrfahrten betrachten man oft \enquote{faule Irrfahrten}. Diese folgt mit Wahrscheinlichkeit $1-\epsilon$ der Übergangsmatrix $Q$ und mit Wahrscheinlichkeit $\epsilon$ der Identität, d.h. die Übergangsmatrix ist $(1-\epsilon) Q + \epsilon I)$. Oft ist $\epsilon = \frac{1}{2}$. Zwei Gründe dafür: Die Kette ist automatisch aperiodisch. (ii) Beim letzten Mal motiviert: $t_{mix}$ hängt im allgemeinen von $1-\lambda_2$ und $\lambda_{\abs{\Omega} -1} +1$ ab. Für die Faule Irrfahrt mit $\epsilon = \frac{1}{2}$ ist der zweite Ausdruck 0. Für die Frage, ob die Irrfahrt Polynomiell oder Exponentiell mischt, ist das Warten irrelevant, die Konvergenz verlangsamt sich lediglich um den Faktor 2. 
		
		Für die faule Irrfahrt mit $\epsilon = \frac{1}{2}$ gilt
		\begin{align}
			\Phi(S) = \frac{2 \abs{\delta S}}{\sum\limits_{x \in S} \deg x}
		\end{align}
		\item Ist $(V,E)$ $d$-regulär, dann ist
		\begin{align}
			\Phi(S) = \frac{\abs{\delta S}}{d \abs{S}}
		\end{align}
	\end{enumerate}
	Ziel/Ausblick: Was sagt $\Phi_*$ über $t_{mix}$ aus? Wir zeigen: $t_{mix} \geq \frac{1}{4\Phi_*}$
\end{beweis} 
\marginnote{Vorlesungsbeginn 16.06.2016}
\begin{satz}
	\begin{align}
		t_{mix} \geq \frac{1}{4\Phi_*}
	\end{align}
\end{satz}
\begin{beweis}
	$S \subseteq V.$ Sei $\pi_S = \pi |S$, d.h. $\pi_S(A) = \pi(A\cap S)$ und $\mu_S(A) = \frac{\pi_S(A)}{\pi(S)}$ die bedingte Wahrscheinlichkeit gegeben $S$. \\
	1. Beobachtung: Für $\mu,\nu \in M^1(V)$ ist 
	\begin{align}
		|| \mu - \nu ||_{TV} = \sum\limits_{v: \mu(v) \geq \nu(v)}\mu(v) - \nu(v)
	\end{align}
	2. Beobachtung: 
	\begin{align}
		\pi_S || \mu_SQ - \mu_S || = || \pi_S Q - \pi_S|| = \sum\limits_{w: \pi_S Q (w) \geq \pi_S (w)}\pi_SQ(w) - \pi_S(w)
	\end{align}
	Einerseits gehören alle $w\in S^c$ zu der Menge $\set{w \given \pi_SQ(w) \geq \pi_S(w) }$, denn für diese ist $\pi_S(w) = 0$. Andererseits:
	\begin{align}
		\pi_S(w) &= \sum\limits_{v \in V}\pi_S(v)Q(v,w) = \sum\limits_{v \in S}\pi_S(v)Q(v,w) \\
				&= \sum\limits_{v \in S} \pi(v) Q(v,w) \leq \sum\limits_{v \in V}\pi(v) Q(v,w) = \pi(w)
	\end{align}
	Also wenn $w \in S \Rightarrow \pi_SQ(w) \leq \pi_S(w)$. Also
	\begin{align}
		|| \pi_SQ - \pi_S || &= \sum\limits_{w \in S^c} \pi_SQ(w) - \pi_S(w) = \sum\limits_{v \in S}\sum\limits_{w \in S^c} \pi(c)Q(v,w)-0 \\
							&= \mathcal{C}(S,S^c) 
	\end{align}
	Also (Teilen durch $\pi(S)$)
	\begin{align}
		|| \mu_SQ - \mu_S || = \Phi(S)
	\end{align}
	Von hier kommen wir mir der folgenden Übung weiter
	\begin{align}
		\forall n \geq 1: || \mu_SQ^{n+1} - \mu_S Q^n ||_{TV} \leq || \mu_SQ - \mu_S || = \Phi(S) \marginnote{Übung}
	\end{align}
	Mit der Dreiecksungleichung folgt nun
	\begin{align}
		\forall m \in n: || \mu_SQ^m-\mu_S ||_{TV} &= || \sum\limits_{n=1}^{m}\mu_SQ^n - \mu_S Q^{n-1} || \leq \sum\limits_{n = 1}^{m} || \mu_S Q^n - \mu_S Q^{n-1} || \leq m \cdot \Phi(S)
	\end{align}\todo{unschön}
	Sei nun $S$ mit $\pi(S)\leq \frac{1}{2}$. Da $\mu_S(S^c) = 0$
	\begin{align}
		d_{TV}(\mu_S,\pi) = \max\limits_A \abs{\pi(A - \mu_S(A))} \geq \pi(S^c)- \mu_S(S^c) = \pi(S^c) \geq \frac{1}{2}
 	\end{align}
 	und damit
 	\begin{align}
 		\frac{1}{2} \leq || \mu_S - \pi ||_{TV} \leq || \mu_S \mu_SQ^m || + || \mu_SQ^m-\pi ||
  	\end{align}
  	Wähle nun $m = t_{mix} \leq t_{mix}\Phi(S) + \frac{1}{4}$. Dann
  	\begin{align}
  		t_{mix} \Phi(S) \geq \frac{1}{4} \Rightarrow t_mix \geq \frac{1}{4\Phi(S)}
  	\end{align}
  	Minimieren über die $S$ mit $\pi(S) \leq \frac{1}{2}$ ergibt 
  	\begin{align}
  		t_{mix} \geq \frac{1}{4\Phi_*}
  	\end{align}
  	\todo{Beweis nochmal durchgehenb}
\end{beweis}

\begin{beispiel}[Verklebte Tori]
	Ein diskreter d-dimensionaler Torus ist ein Graph $G = (V,E)$ mit $V = \ZZ_n \times \dots \times \ZZ_n$, d-mal, und $\ZZ_n = \set{0,\dots,n-1}$ und für $x = (x_1,\dots,x_d), y = (y_1,\dots,y_d)$ sei 
	\begin{align}
		e = \set{x,y} \in E \Leftrightarrow \exists j \in \set{1,\dots,d} :\forall i \neq j: x_i = y_i  \text{ und } x_j = y_j +- 1 \mod{n}.
	\end{align} 
	Seien nun $T_1$ und $T_2$ zwei d-dimensionale n-Tori. Diese verkleben wir in genau einem Punkt $v*$. Sei mim $V_1 = V(T_1)$, $V_2 = V(T_2) \Rightarrow V_1 \cap V_2 = \set{v*}$. Wähle für die einfache Irrfahrt auf diesen Gebilden $S = V_1\backslash \set{v*}$. Da jeder Knoten aus $T_1$ oder $T_2$ mit genau $2d$ Knoten aus diesem Torus verbunden ist, ist $\abs{\delta S} = 2d$. Nun ist 
	\begin{align}
		\sum\limits_{x \in S} \deg{x} = 2d(n^d-1). 
	\end{align}
	Damit ist $\Phi(S) = \frac{1}{n^d - 1}$. Wählt man $d = 2$ ist $\Phi(S) \approx \frac{1}{n^2}$. Damit erhält man in diesem Fall $t_{mix} \geq \frac{\enb{n^2-1}}{4}$. Die wahre Mischzeit ist $\mathcal{O}(n^2 \log n)$
\end{beispiel}\todo{überarbeitten Anfnag bsp}

\begin{beispiel}[Graphenfärben]
%	Sei $G = (V,E)$ ein Graph- Eine Färbung von $G$ mit $q$ Farben $\set{1,\dots,q}$ ist eine Abbildung $f: V \to \set{1,\dots,q}$, mit $f(v) \neq f(w)$, falls $\set {v,w} \in E$. Es gibt zwei offensichtliche Fragen: Wie groß muss $q$ minimal sein? Und wie viele Färbungen gibt es? Die zweite Frage versucht man approximativ mithilfe eines MCMC-Verfahrens zu lösen.\marginnote{MCMC = Monte Carlo Markov Chain} Dazu sucht man eine Markovkette auf $\Omega = \set{f:V \to \set{1,\dots,q} \given f \text{ zulässige Färbung}}$, die als stationäres Maß die Gleichverteilung hat. Nun bastelt man Graph $G = G_\abs{E} \supseteq G_\abs{E}-1 \supset \dots \supset G_0 = \enb{V,E_0}, E_0 = \emptyset$, indem man sukzessive Kanten weglässt. Sei $\Omega_i$ die Menge der Färbungen von $G_i \Rightarrow G_0 = q^{\abs{V}}$. Kann man die Gleichverteilung auf allen $\Omega_i$ gut simulieren, so kann man auch $\frac{\abs{\Omega_i}}{\abs{\Omega_{i-1}}}$ gut schätzen. Also kann man auch $\abs{\Omega}$ gut schätzen. Wie sampelt man die Gleichverteilung auf $\Omega$?
%	
	\enquote{Glauberdynamik} - Beginne mit irgendeiner zulässigen Färbung.
	\begin{itemize}
		\item wähle zufällig $v \in V$
		\item wähle zufällige Farbe
		\item ist diese nicht zufällig, behält $v$ seine Farbe, ansonsten streiche $v$ in der neuen Farbe 
	\end{itemize}\todo{check}
	\underline{Mitteilung:} Wenn $q > \Delta = \max\limits_{v \in V} \deg{v}$, dann ist die Kette aperiodisch und irreduzibel, und $\pi = \frac{1}{\abs{\Omega}}$ ist das stationäre Maß \\
	\underline{Mitteilung:} Ost $q > 2\Delta$, dann mischt die Kette in Zeit $\abs{V} \log \abs{V}$. Es kann aber auch schief gehen, wenn $\Delta$ groß wird. Sei dazu $G = (V,E)$ der $k$-Stern. d.h. $G$ ist ein Baum der Tiefe 1 mit Wurzel $v*$ und $n-1$ Blättern. \todo{Stern bild}
	Sei $\Omega \supset S = \set{f \in \Omega \given f(v*) =  1}$. Sei $(x,y) \in S \times S^c, c(x,y) > 0 \Leftrightarrow x(v*) = 1, y(v*) \neq 1 \land x(v) =  y(v) \forall v \neq v* land x(v) \notin \set{1,y(v*)} \forall v \neq v*$ \todo{Layout überarbeiten}. Also gilt für 
	\begin{align}
		A = \set{(x,y) \in S\times S^c, c(x,y)> 0} \Rightarrow \abs{A} = (q-1)(q-2)^{n-1} \marginnote{Es gibt $(q-1)$ Färbungen von $v*$ unter $y$}
	\end{align}
	Für alle $(x,y) \in A$ ist außerdem $c(x,y) = \pi(x)Q(x,y) \leq \frac{1}{\abs{\Omega}} \frac{1}{n}$. Daraus folgt
	\begin{align}
		C (S,S^c) \leq \frac{1}{\abs{\Omega}} \frac{1}{n} \enb{q-1} \enb{q-2}^{n-1}.
	\end{align}
	Umgekehrt gilt $x \in S \Leftrightarrow x(v*) = 1 \land x(v) \neq 1 \forall v \neq v^*$. Das heißt $\abs{S} = \enb{q-1}^{n-1}$
	\begin{align}
		\frac{C(S,S^c)}{\pi(S)} \leq \frac{1}{\abs{\Omega} n}(q-1)(q-2)^{n-1} \frac{\abs{\Omega}}{(q-1)^{n-1}} = \frac{\enb{q-1}^2}{n(q-2)} \enb{1  - \frac{1}{q-1}}^n \leq \frac{(1-1)^2}{n(q-2)} - \frac{n}{q-1} \\
		\Rightarrow t_{mix} \geq \frac{n(q-2)}{4(q-1)^2} \frac{n}{q-1}
	\end{align}
	\todo{Brüche}
	Wählt man nun $n$ und $q$ so, dass $\frac{n}{q \log q} \to \infty$ dann wächst $t_{mix}$ super-polynomiell
\end{beispiel}
 
\marginnote{Vorlesungsbeginn 20.06.2016}
\todo[inline]{missing part}
\begin{beispiel}[Der Binärbaum]
	Sei $T=(V,E)$ der Binärbaum der Tiefe $k$, d.h. ein Baum mit Wurzel $v*$, wobei $\deg(v*) = 2, \deg(v) = 3$ für alle $v \neq v*$, die keine Blätter sind und $\deg(v) = 1$ für alle Blätter. Sei $n = \abs{V} = 2^{k+1} -1$. Dann ist $\abs{E} = n-1, d(v,v*) = k$ für alle Blätter.
	
\end{beispiel}
\begin{uebung}
	Ist $G = (V,E)$ ein einfacher Graph, dann ist $G$ ein Baum genau dann, wenn $\abs{V} = \abs{E} +1$. 
\end{uebung}
 
 Auf $T$ lassen wir die einfache Irrfahrt laufen. \\
 \underline{Behauptung:} $t_{mix} \geq \frac{k-2}{4}$. \\
 Um das zu zeigen, seien $v_l$ und $v_r$ die direkten Nachbarn von $v*$ und wir nennen $w$ einen Abkömmling von $v$ (oder $v$ einen Vorgänger von $w$), wenn der kürzeste Weg von $v*$ nach $w$ durch $v$ geht. Wir wählen als $S$ die Menge der Abkömmlinge von $v_r$ inklusive $v_r$ selbst. Um $\Phi(S)$ zu berechnen, müssen wir zunächst $\pi(x), x \in V$ ausrechnen. 
 \begin{align}
 	\pi(x) = \begin{cases}
			 	\frac{2}{2n-2}, & x = v* \\
			 	\frac{1}{2n-2}, & x \text{ ist ein Blatt}\\
			 	\frac{3}{2n -2}, & x \text{ ist ein innerer Knoten } \neq v*	
		 	\end{cases}
 \end{align}
 Damit ist
\begin{align}
	\pi(S) = \sum\limits_{v: v \leftarrow v*}\pi(v) = \sum\limits_{v \leftarrow v*, v \text{ innerer Knoten}} + \sum\limits_{v \leftarrow v*, v \text{ Blatt}} \overset{*}{=} \frac{1}{2} - \frac{1}{n-2} = \frac{n-2}{2n-2} \marginnote{* aus Symmetriegründen}
\end{align} \todo{Inhalte summe vergessen}
 
 Man berechnet 
 \begin{align}
 	C =(S,S^c) &= \sum\limits_{x \in S, y\in S^c} \pi(x) Q(x,y) = \pi(v*) \frac{1}{3} \\
	 	&= \frac{3}{2n-2}\frac{1}{3} = \frac{1}{2n-2}
 \end{align}
 Somit:
 \begin{align}
 	\Phi(S) = \frac{1}{2n-2} \frac{2n-2}{n-2} = \frac{1}{n-2}.
 \end{align}
 Damit $t_{mix} \geq \frac{n-2}{4}$. Diese Abschätzung ist ziemlich gut, denn ,man kann auhc zeigen dass $t_{mix} \leq 4n$ gilt, d.h. untere und obere Schranke unterscheiden sich lediglich um eine Konstante. \\
 Curie-Weiss: $\beta =1$ kritische Temperatur. $m_N = \frac{1}{N} \sum\limits_{i=1}^{N}\delta_i : m_N \sim 0$ \todo{check} für $\beta \leq 1$. $m_N$ konzentriert sich. \todo{missing part}
 
 \enquote{Man kann zeigen, dass $t_{mix} \leq \dots$ gilt} haben wir in allen Beispielen 4.2-4.4 \todo{REF} gesagt. Aber wie zeigt man so etwas? Wir wollen eine Technik kennenlernen, die solche Abschätzungen aus der Verteilung der Eigenwerte von $Q$ generiert. Die Situation ist wieder die, dass $Q$ eine bezüglich $\pi$ reversible Markovkette auf einem endlichen Zustandsraum $\Omega$ mit $\abs{\Omega} = m$ ist. Wir werden sehen, dass $Q$ nur reelle Eigenwerte hat. Diese ordnen wir an. $1 = \beta_0 > \beta_1 \dots \geq \beta_{m-1} \overset{*}{>} -1$\marginnote{* die Kette sei aperiodisch}. Man kann einen Operator zu $Q$ assoziieren, der vermöge
 \begin{align}
 	\benb{Q\Phi}(x) = \sum\limits_{z \in \Omega} Q(x,z)\Phi(z)
 \end{align}
 auf $L^2(\pi)$ (also alle Funktionen auf $\Omega$) wirkt. Intuitiv ist klar, dass $t_{mix}$ etwas mit $1 - \max\set{\abs{\beta_1},\abs{\beta_{n-z}}}$ zu tun haben muss. 
 	\begin{align}
 		Q^n = S-{-1} D^n S = S^{-1} MATRIXMISSiNG S
  	\end{align}\todo{MATIRX}
  	D.h. die Konvergenz findet in $D^n$ statt. Diese Matrix konvergiert gegen \todo{MATRIX} (1,0,0,0) und die Konvergenzgeschwindigkeit richtet sich nach dem betragsmäßig zweitgrößten Eigenwert.
  	
\begin{satz}
	In der obigen Situation sei $\beta_* = \max\set{\abs{\beta_i},i\neq 0}, \gamma* = 1- \beta_*$ die absolute Spektrallücke, $\gamma = 1 - \beta_1$ die Spektrallücke und $t_{rel} = \frac{1}{\gamma^*}$ die Relaxationszeit. Dann gilt für alle $\epsilon > 0$ 
	\begin{align}
		(t_{rel} - 1) \log \frac{1}{2 \epsilon} \leq t_{mix} (\epsilon) \leq \log\enb{\frac{1}{2\epsilon \sqrt{\pi_{min}}}} t_{rel}
	\end{align}
	wobei $\pi_{min} = \min\limits_{x} \pi(x) > 0$.
\end{satz}
\begin{beweis}
	\begin{enumerate}
		\item $t_{rel}$ gibt es bis auf Faktoren Auskunft über $t_{mix}$.
		\item Durch Übergang zur faulen Version der Markovkette, d.h. $\hat{Q} = \frac{1}{2}(Q + \id)$ genügt es \enquote{immer} $\gamma$ statt $\gamma*$ zu betrachten.
	\end{enumerate}
	Wir beginnen mit einem Lemma
	\begin{lemma}
		Sei $Q$ reversibel bezüglich $\pi$. Dann gilt:
		\begin{enumerate}[(i)]
			\item $Q$ hat nur reelle Eigenwerte $(\beta_0,\dots,\beta_m)$
			\item $(R^m,<\cdot,\cdot>)$ hat eine Orthonormalbasis aus Eigenfunktionen zu $(\beta_i)$
			\item $\forall t \in \NN$ gilt $\frac{Q^t(x,y)}{\pi(y)}  = \sum\limits_{j = 0}^{m-1} f_j(x) f_j(y)\beta_j^t$
			\item $f_0 \equiv 1$ ist zulässig also \begin{align}
				\frac{Q^t(x,y)}{\pi(y)} = 1+ \sum\limits_{j = 1}^{m-1} f_j(x) fj(y) \beta^t_j
					\end{align}
		\end{enumerate}
	\end{lemma}
	\begin{beweis}
		Sei $A(x,y) = \sqrt{\frac{\pi(x)}{\pi(y)}} Q(x,y).$ $A$ ist symmetrisch, denn $Q$ ist reversibel bezüglich $\pi$. Also hat $A$ lauter reelle Eigenwerte, nennen wir sie auch $\beta_0, \dots \beta_m-1$. Außerdem gibt es eine ONB des $\RR^m$ bezüglich $<\cdot,\cdot>$ (euklidisch) aus Eigenfunktionen $(\phi_j)_{j=0}^{m-1}$ von $A$. Man rechnet nach, dass $(\sqrt{\pi(x)})$ ein Eigenvektor zum größten Eigenwert $\beta_0$ ist. Weiter ist mit $D_\pi = diag(\pi(x))$ \todo{diag} $A = D^{1/2}_{\pi}  Q D^{1/2}_{\pi}$. Weiter sei $f_j =  D^{-1/2} \phi_j$. Dann sind $f_j$ Eigenfunktionen zu Eigenwert $\beta_j$ und der Matrix $Q$. 
		\begin{align}
			Qf_j = Q D^{-1/2}_{\pi} \phi_j = D^{-1/2}_\pi (D^{1/2}_\pi Q D^{-1/2}_{\pi}) \phi_j = D^{-1/2}_{\pi} A \phi_j = D^{-1/2}_{\pi} \beta_j \phi_j = \beta_j f_j
		\end{align}
		$f_j$ sind orthogonal (orthonormal) bezüglich $<\cdot,\cdot>_{\pi}$.
		\begin{align}
			\delta_{ij} = <\phi_i,\phi_j> = <D^{1/2}_{\pi} f_i, D_{\pi}^{1/2} f_j > = \sum\limits_{x} f_i(x) f_j(x) \pi(x) = <f_i,f_j>_{\pi}
		\end{align}
		Sei $\delta_y(x)ß = \begin{cases}
			1, &y = x \\
			0, &\text{sonst}
		\end{cases}$ Dann hat die Zerlegung 
		\begin{align}
			\delta_y = \sum\limits_{j = 0}^{m-1} <\delta_y,f_j>_{\pi} f_j = \sum\limits_{j = 0}^{m-1} f_j(y) \pi(y) f_j
		\end{align}
		Da $Q^t f_j = \beta^t_j f_j$ und $Q^t(x,y) = \benb{Q^t\delta_y} (x)$ folgt $Q^t(x,y) = \sum\limits_{j}f_j(y) \pi(y) \beta_j^t f(x) \Rightarrow $ Division durch $\pi(y)$ gibt das Ergebnis.
	\end{beweis}
	\underline{Beweis 4.5} \todo{REF!}
	
	Wir zeigen nur die obere Schranke, weil uns diese hilft $t_{mix}$ von oben zu beschränken. Nach Definition von TV:
	\todo[inline]{auskommentiert}
%	\begin{align}
%		|| Q_t(x, \cdot) - \pi(\cdot) ||_{TV} &= \frac{1}{2} \sum\limits_{y} \abs{Q^t(x,y) - \pi(y)} = \frac{1}{2} \sum\limits_{y} \pi(y) \abs{\frac{Q^t(x,y)}{\pi(y)} -1} \\
%						&= \frac{1}{2} || \frac{Q^t(x,\cdot)}{\pi(\cdot)} -1 ||_1 \marginnote{$||\cdot||_1 $1-Norm in $ L^1(\pi)$} 
%						
%	\end{align}
	d.h. $4 ||Q^t(x,\cdot) - \pi(\cdot )||^2_{TV} \leq ||\frac{Q^t(x,\cdot)}{\pi(\cdot)} -1||^2_2$. \marginnote{Mit Jensenscher Ungleichung und $||\cdot||_2$ die Norm in $L^2(\pi)$} Nach dem Lemma gilt:
	\begin{align}
		\abs{\frac{Q^t(x,y)}{\pi(y)} -1} &\overset{\Delta-\text{Ungl.}}{\leq} \sum\limits_{j =  1}^{m-1} \abs{f_j(x) f_j(y)} \beta^t_j \leq \beta^t_* \sum\limits_{j=1}^{m-1} \abs{f_j(x) f_j(y)} \\
			&\overset{C.S.}{\leq} \beta^t_* \enb{\sum\limits_{j = 0}^{m-1} f_j^2(x)}^{1/2} \enb{\sum\limits_{j = 0}^{m-1} f_j^2(y)}^{1/2}
	\end{align}
	Da \begin{align}
		\pi(x) &= <\delta_x(\cdot),\delta_x(\cdot)> = <\sum\limits_{j = 0}^{m-1}f_j(x \pi(x) f_j(\cdot), \sum\limits_{j = 0}^{m-1} f_j(x) \pi(x) f_j(\cdot)>_\pi \\
			&=\sum\limits_{j = 0}^{m-1} f_j^2(x)\pi^2(x) = \sum\limits_{j=0}^{m-1} f_j^2(x) = \frac{1}{\pi(x)}
	\end{align}
	Also
	\begin{align}
		4 || Q^t (x,\cdot) - \pi(\cdot) ||^2_{TV} \leq \beta^{2t}_* \sum\limits_{y}\frac{1}{\pi(x)} \frac{1}{\pi(y)} \pi(y) = \frac{\beta^{2t}_*}{\pi(x)} \leq \beta^{2t}_* \pi_{min} \\
		& (1-\gamma*)^{2t} \frac{1}{\pi_{min}} \leq e^{-2jt} \frac{1}{\pi_{min}} \\
		\Rightarrow d(t) \leq \epsilon \text{ wenn } t \geq \frac{1}{\gamma*} \log \frac{1}{2\epsilon \sqrt{\pi_{min}}} = \log \enb{\frac{1}{2\epsilon\pi_{min}}} t_{rel}
	\end{align}
\end{beweis}
\todo{check begin/letztes Ende}

\marginnote{Vorlesungsbeginn 23.06.2016}
\underline{Moral:} Es genügt $1-\beta_1$ abzuschätzen, um $t_{mix}$ zu kontrollieren. Eine Möglichkeit geht über den mit $Q$ assoziierten Laplace-Operator: $L = \id -Q$. Die Spektrallücke ist dann der erste von 0 verschiedene Eigenwert.

\begin{definition}
	Sei $Q$ reversibel bezüglich $\pi$, $c(x,y) = \pi(x) Q(x,y)$ die zu $(Q,\pi)$ assoziierte Dirichletform ist definiert durch
	\begin{align}
		\mathcal{E}(f,g) = \frac{1}{2}\sum\limits_{x,y}c(x,y)\enb{f(y) - f(x)}\enb{g(y)- g(x)}.
	\end{align}
	Also insbesondere
	\begin{align}
		\mathcal{E}(f,f) = \frac{x,y} c(x,y) \enb{f(y)-f(x)}^2
	\end{align}
\end{definition}

Man kann mittels $\mathcal{E}$ eine Abschätzung für $1-\beta_1$	erreichen.
\begin{satz}
	Sei $\lambda_1 = 1-\beta_1$, dann ist  
	\begin{align}
		\lambda = \min\limits_{f:S\to \RR, E_\pi(f) = 0, \Var{f}_{\pi} = 1} \mathcal{E}(f,f) = \min\limits_{f:S\to \RR, E_\pi (f) = 0} ||f||^2_{2,\pi} = \min\limits_{f:S \to \RR, f \neq const.} \frac{\mathcal{f,f}}{\Var{f}_{\pi}}
	\end{align}
\end{satz}
\todo{formel pi Varianz + mins}
\begin{bemerkung}
	Das heißt in der Physik auch manchmal Raleigh-Ritz-Prinzip. Man sucht in gewissem Sinne möglichst glatte Funktionen, die aber nicht konstant sind.
\end{bemerkung}
\begin{beweis}
	Das letzte Gleichheitszeichen erhält man, indem man $f$ durch $f- \EW{f}[\pi]$ \todo{ EW für oben unten ertweitern} ersetzt, das vorletzte Gleichheitszeichen erhählt man, indem man $f$ durch $\frac{f}{||f||_{2,\pi}}$ ersetzt.
	Für die erste Gleichheit, sei $f: \EW{f}_[pi](f), ||f||^{2,\pi} = 1$. Dann gilt
	\begin{align}
		0 = \EW{f}_{\pi}= \sum\limits_{x}f(x)\pi(x) = \sum\limits_{x} f(x) \mathds{x} \pi(x) ) <f,\mathds{1}>_{pi} \marginnote{$\mathds{1}$-konstante 1 Fkt}
	\end{align}
	Da $\mathds{1}$ Eigenfunktion zu $\beta_0$ ist, ist $f orthogonal$ Eigenraum zu $\beta_0$. Also, mit $f_j$ Eigenfunktion von $\beta_j$
	\begin{align}
		f = \sum\limits_{j = 0}^{m-1}<f,f_j>_{\pi}f_j 0 \sum\limits_{j = 1}^{m-1}<f,f_j>_{\pi} f_j 
	\end{align}
	Andererseits
	\begin{align}
		\mathcal{E}(f,f) &= \frac{1}{2} \sum\limits_{x,y}c(x,y) \enb{f(x) - f(y)}^2 = \frac{1}{2} \sum\limits_{x,y}f^2(x) \pi(x) Q(x,y) + \frac{1}{2} \sum\limits_{x,y} f^2(y) \pi(x) Q(x,y) - \sum\limits_{x,y} f(x)f(y)\pi(x) Q(x,y) \\
		&= \frac{1}{2} \sum\limits_{x}f^2(x) \pi(x) \sum\limits_{y} Q (x,y) + \frac{1}{2} \sum\limits_{y} f^2(y) \pi(y) - \sum\limits_{x,y}f(x) f(y) \pi(x) Q(x,y) \\
		&= <(I-Q)f,f >_{\pi} = <Lf,f>_{\pi} = <L\sum\limits_{j=1}^{m-1}<f_i,f_j>_{pi},f_j, \sum\limits_{j=1}^{m-1}<f,f_j>_{pi}f_j>_{\pi} \\
		&= <\sum\limits_{j=1}^{m-1}(1-\beta_j)<f,f_j>_{\pi}f_j,\sum\limits_{j=1}^{m-1}<f,f_j>_{\pi}f_j>_{\pi} \marginnote{$f_j$ sind Eigenfkt von $L$} \\
		&= \sum\limits_{j = 1}^{m-1}(1-\beta_j)<f,f_j>^2_{\pi}\geq \enb{1-\beta_1} \sum\limits_{j = 1}^{m-1}<f,f_j>^2_{\pi}  =\enb{1-\beta_1} \sum\limits_{j = 0}^{m-1}<f,f_j>^2_n \marginnote{$f_j$ ONB}
		&= \enb{1- \beta_1} ||f||^2_{2,\pi} = (1-\beta_)
	\end{align}
\end{beweis}

Um Satz 4.6 \todo{ref} anzuwenden, sei nun ein Random Walk auf einem zusammenhängenden Netzwerk $G = (V,E,c)$ mit Übergangsmatrix $Q$ und stationären Maß $\pi$ gegeben. F+r je zwei Punkte $x \neq y \in V$ sei $\gamma_{xy}$ ein Pfad von $x$ nach $y$ und 
\begin{align}
	\Gamma = \set{\gamma_{x,y} \given x,y \in V}
\end{align}
Weiter sei
\begin{align}
	\abs{\gamma_{xy}}_Q = \sum\limits_{e \in \gamma_{xy}}\frac{1}{c(e)}
\end{align}
Weiter sei 
\begin{align}
	\kappa = \kappa(\Gamma) = \max\limits_{e} \sum\limits_{\gamma_{xy}}\abs{\gamma_{xy}}_Q.
\end{align}
Dann gilt 
\begin{satz}[Poincaré Ungleichung]
	In der obigen Situation gilt $\beta_1 \leq 1 - \frac{1}{\kappa}$ oder $\lambda_1 \geq \frac{1}{\kappa}$
\end{satz}
\begin{beweis}
	Für eine Funktion (Zufallsvariable) $f: V \to \RR$ gilt:
	\begin{align}
		\Var{f}_{\pi} &= \EW{f^2}_{\pi} - \enb{\EW{f}_{\pi}}^2 = \sum\limits_{x}f^2(x) \pi(x) - \enb{\sum\limits_{x}f(x) \pi(x) }^2 \\
				&=\frac{1}{2}\sum\limits_{x} f^2(x) \pi(x) + \frac{1}{2} \sum\limits_{y}f^2(y) \pi(y) - \enb{\sum\limits_{x}f(x) \pi(x) }\enb{\sum\limits_{y}f(y)\pi(y) } \\
				&= \frac{1}{2} \sum\limits_{x,y} \enb{f(x) - f(y)}^2 \pi(x) \pi(y) \\
				&= \frac{1}{2} \sum\limits_{x,y} \enb{\sum\limits_{e \in \gamma_{xy}} \sqrt{\frac{c(e)}{c(e)}}  f(e) }^2 \pi(x)\pi(y) \marginnote{Teleskopsumme. $f(e) = f(a) - f(b),  c(e) = c(a,b)$ für $e = (a,b)$} \\
				&\leq \frac{1}{2} \sum\limits_{x,y} \sum\limits_{e' \in \gamma_{xy}} \frac{1}{c(e')} \sum\limits_{e \in \gamma_{xy}} f^2(e) \pi(x) \pi(y) \\
				&= \frac{1}{2}\sum\limits_{e}c(e) f^2(e) \underbrace{\sum\limits_{\gamma_{xy \ni e}} \abs{\gamma_{xy}}_Q \pi(x) \pi(y)}_{\kappa} \\
				&\leq \frac{1}{2} \kappa \sum\limits_{e} c(e) f^2(e) \\
				&= \frac{1}{2} \kappa \sum\limits_{x,y} c(x,y) \enb{f(x) - f(y) }^2 = \kappa \mathcal{E}(f,f)
	\end{align}
	Mit Satz 4.6 \todo{REF} folgt die Behauptung.
\end{beweis}

\underline{Ähnlich:} 
\begin{satz}
	Sei $\abs{\gamma_{xy}}$ in obiger Situation die Länge von $\gamma_{xy}$ und $k = \max_e \set{\frac{1}{c(e)} \sum\limits_{\gamma_{xy} \ni e} \abs{\gamma_{xy}} \pi(x) \pi(y) }$. Dann
	\begin{align}
		\beta_1 \leq 1- \frac{1}{k}
	\end{align}
\end{satz}
\begin{beweis}
	Sei $f: V \to \RR$.
	\begin{align}
		\Var{f}_{\pi} &= \frac{1}{2} \sum\limits_{x,y} \enb{\sum\limits_{e \in \gamma_{xy}} f(e) }^2\pi(x) \pi(y) \\
			&\leq \sum\limits_{e} f^2(e) \sum\limits{\gamma_{xy} \ni e} \abs{\gamma_{xy}}\pi(x)\pi(y) \marginnote{C.S.} \\
			&" \frac{1}{2} \sum\limits_{e} f^2(e) c(e) \sum\limits_{\gamma_{xy}} \frac{1}{c(e)} \abs{\gamma_{xy}} \pi(x) \pi(y)
	\end{align}
\end{beweis}
\begin{korollar}
	Für alle $\epsilon > 0$ gilt
	\begin{align}
		t_{mix} \leq \log\enb{\frac{1}{2 \epsilon \pi_{min}}} \kappa(\Gamma) \\
		t_{mix} \leq \log \enb{\frac{1}{2\epsilon\pi_{min}}} k
	\end{align}
\end{korollar}
\begin{beweis}
	folgt unmittelbar aus Satz 4.5, 4.7 und 4.8 \todo{REF} \todo{d*}
\end{beweis}
\begin{korollar}
	Für die einfache Irrfahrt auf $G$ gilt $\beta_1 \leq 1 - \frac{2\abs{E}}{d_*^2\gamma_* b}$. Dabei $d^2_* = \max\limits_x \deg(x), \gamma_* = \max\set{\abs{\gamma_{xy}} \given x,y \in V }$ und $b = \max\limits_{e} \abs{\set{\gamma \in \Gamma \given e \in  \gamma}}$.
\end{korollar}\todo{check korollar formel}
\begin{beweis}
	Hier ist $Q(x,y) = \frac{1}{\deg (x)}$, falls $x \sim y$, sonst 0. Sei $\pi(x) = \frac{deg (x)}{2\abs{E}}$ und daher $c(e) = \frac{1}{2\abs{E}}$, für alle $e \in E$.
	\begin{align}
		\Rightarrow k(\Gamma) = \max\limits_x \sum\limits_{\gamma_{xy}} \abs{\gamma_{xy}}_Q \pi(x)\pi()
	\end{align}
	\todo{missing end}
\end{beweis}
%
\marginnote{Vorlesungsbeginn 27.06.2016}
%\todo[inline]{weg lassen wiederhilung?}
%\underline{Poincaré:} Es genügt $1- \beta$ abzuschätzen; das hieß gelegentlich auch $\lambda_1$.
%\begin{align}
%	1 - \beta \geq \frac{1}{\kappa} \\
%	\kappa = \max\limits_e \sum\limits_{\gamma_{xy} \ni e} \abs{\gamma_{xy}}_Q \pi(x) \pi(y)
%\end{align}
%wobei
%\begin{align}
%	\Gamma 
%\end{align}
%\todo{missing part erklärung/wiederholung}
%Insbesondere für SRW auf einem zusammenhängenden Graphen
%\begin{align}
%	1- \beta_1 \geq \frac{2 \abs{E}}{d^2_*} && d_* = \max\limits_x \deg(x), \gamma_* := \max\limits
%\end{align}
%\todo[inline]{ende der wiederholung}
%
\begin{beispiel}
	Irrfahrt auf dem $p$-Kreis $\ZZ_p$. Sei $p$ ungerade und wir betrachten die gewöhnliche Irrfahrt auf $\ZZ_p$. Es ist $\pi(x) = \frac{1}{p} \forall x \in \ZZ_p$ das invariante Maß. \todo{missing figure} Außerdem gilt $d_* = 2$. Wir wählen (typisch für Simple Random Walk) $\gamma_{xy}$ als den kürzesten Pfad von $x$ nach $y$. Dann ist $\gamma_* = \frac{p-1}{2}$. Nun berechnen wir $b$. Aus Symmetriegründen wird jede Kante von gleich vielen Pfaden benutzt. Nehmen wir die Kante $(0,1)$. \todo{missing picture} $\gamma_{xy}$, die diese Kante benutzen haben $x$ \enquote{links} von $0$ und $y$ \enquote{rechts} von 1. Sei $d(x,0) = i$. Dann laufen alle Pfade von $\gamma_{xy}$ über $(0,1)$ mit $j \neq i \leq \frac{p-1}{2}$ mit $j = d(0,y)$ über $(0,1)$. Das heißt für ein festes $i$ gibt es $\frac{p-1}{2} - i$ solche Pfade ($i = 0, \dots, \frac{p-3}{2}$). Also ist 
	\begin{align}
		b = \sum\limits_{i = 0}^{\frac{p-3}{2}} \frac{p-1}{2} -i = \enb{\frac{p-1}{2}}^2 - \enb{\frac{p-3}{2} - \frac{p-1}{2} \frac{1}{2}} = \frac{p^2-1}{8}
	\end{align}
	Also ist nach unserer Abschätzung:
	\begin{align}
		\beta_1 \leq 1-\frac{2p}{4 \frac{p-1}{2} \frac{p^2-1}{2} } = 1 - \frac{8p}{\enb{p-1}^2 (p+1) }
	\end{align}
	Wie gut ist das? Mit Darstellungstheorem kann man in diesem Fall die $(\beta_j)$ persönlich ausrechnen:
	\begin{align}
		\beta_j = \cos(\frac{2\pi}{p}j), j = 0, \dots, p-1
	\end{align}
	d.h.
	\begin{align}
		\beta_1 = \cos(\frac{2\pi}{p}) \approx 1 - \frac{2 \pi^2}{p^2} + \mathcal{O}\enb{\frac{1}{p^3}}
	\end{align}
	für große $p$. Unsere Abschätzung $\beta_1 = 1 - \frac{8}{p^2}\dots$ das ist brillant. Wir haben die richtige Ordnung und fast die richtige Konstante.
\end{beispiel}
\todo[inline]{mathbb undefined control sequence in title...}
\begin{beispiel}[Irrfahrt auf dem Torus $Z_2^d$]
	\marginnote{$\ZZ^d_2 = \set{0,1}^d$} Wir berechnen: $\abs{E} = d 2^d, d_* = d$ und wir wählen $\gamma_{xy}$ wie folgt: Wenn $x \neq y$, z.B. $d_H(x,y) = k$, dann gibt es $i_1 < \dots < i_k$, sodass $x_{i_j} \neq y_{i_j}, \forall j = 1, \dots, k$ und in allen anderen Koordinaten sind $x$ und $y$ gleich. $\gamma_{xy}$ besteht nun darin, dass man sukzessiv $x_{i_1} \to y_{i_1}, x(i_2) \to y_{i_2}, \dots$ usw. verändert.b Dann ist $\gamma_* = d$.

	Nun zu $b$. Sei $e = (w,z) \in E$. Da bedeutet, dass $w_i = z_i$ für alle $i$ bis auf eines, beispielsweise das $j$'te. Die Kante $e$ verbindet alle $x$ und $y$, sodass $x$ mit $w$ in den Koordinaten $j,j+1,\dots,d$ übereinstimmt und $y$ mit $z$ in der Koordinaten $1, \dots, j$. Beispiel:
	\begin{gather}
		w = (0,0,0,0,0), z = (0,0,1,0,0), \text{ ist } x = (1,0,0,1,0), y = (0,0,1,0,1) \\
		x\to y: (0,0,0,1,0)
	\end{gather}
	Also gibt es für die Kante $b$ $2^{j-1} 2^{d-j} = 2^{d-1}$ Möglichkeiten. \underline{Also:}
	\begin{align}
		\beta_1 \leq 1 . \frac{d2^{d+1}}{d^2 \cdot d \cdot d^{d-1}} = 1 - \frac{4}{d^2}
	\end{align}
	Der wahre Wert ist $\beta_1 = 1 - \frac{2}{d}$. \todo{bemerkung ohne begin/end?}
	\begin{bemerkung}
		Mann kann sich fragen, ob das an unserer Pfadwahl liegt.
		\begin{enumerate}
			\item Nein, weil alle Kanten gleich oft benutzt werden. 
			\item Nein, denn es gibt zu jedem Punkt $v \in \ZZ^d_2$ Punkte im Abstand $i$. Um $v$ mit solch einem Punkt zu verbinden, muss man also mindestens einen Pfad der Länge $i$ wählen. Das ergibt insgesamt $2^d \sum_{i = 1}^{d} i \cdot (d nCr i) = 2^d \cdot d \cdot 2^{d-1}$ Kanten. Da $\abs{E} = d 2^d$ muss man mindestens eine Kante $2^{d-1}$-mal benutzen.
		\end{enumerate}
	\end{bemerkung}
\end{beispiel}

\begin{beispiel}[Binärbaum]
	Sei $T = (V,E)$ der Binärbaum mit Wurzel $v*$ der Tiefe $d$. Betrachte wieder einen Simple Random Walk. Für $x \neq y$ wähle $\gamma_{xy}$ als den direkten Pfad von $x$ nach $y$. Dann:
	\begin{align}
		\abs{E} = \abs{V} -1 = 2^{d+1} - 2, d* = 3, \gamma_* = 2d
	\end{align}
	Offenbar werden $(v*,v_l)$ und $(v*,v_r)$ am meisten benutzt \marginnote{$v_r,v_l$ die Kinder links bzw. rechts von $v*$} und zwar $2^d \cdot \enb{2^d-1}$ mal (Alle Knoten (links + Wurzel) mal alle Knoten (rechts + Wurzel))
	\todo[inline]{missing figure tree}
	\begin{align}
		b = 2^d(2^d-1) \Rightarrow \beta_1 \leq 1- \frac{2(2^{d+1} -2)}{q \cdot 2d \cdot 2^d(2^d-1)}) = \frac{2}{q d \cdot 2^d} = \frac{1}{q d 2^{d-1}}
	\end{align}
	Tatsächlich $\beta_1 = 1- \frac{1+ \mathcal{o}(1)}{2^{d+1}}$. Also ist unsere Abschätzung um eine Ordnung $d$ verkehrt. Das ist trotzdem nicht so schlecht, wenn man bedenkt, dass die Spektrallücke exponentiell klein ist.	
\end{beispiel}
	\todo[inline]{missing figure stern}
\begin{beispiel}[Der Stern]

	Sei $G = (V,E)$ der Stern mit $n$ Spitzen und Zentrum $v*$. Betrachte wieder SRW. Man kann das Spektrum der einfachen Irrfahrt ausrechnen:
	\begin{align}
		\beta_0  = 1 && \beta_1 = \dots = \beta_n = 0 &&  \beta_{n+1} = -1
	\end{align}
	Für die faule Irrfahrt ist $\beta_0, \beta_1 = \dots = \beta_n = \frac{1}{2}, \beta_{max} = 0$. Die Irrfahrt mischt sehr schnell: $t_{rel} = 2$ Es ist
	\begin{align}
		\abs{E} = n && d_* = n && \gamma_* = 2 && b = n 
	\end{align}
	Daraus folgt
	\begin{align}
		1 - \beta_1 \geq \frac{2n}{2n^3}= \frac{1}{n^2}
	\end{align}
	Das ist für großes $n$ eine Katastrophe. Übung: Poincaré ist unschuldig.
\end{beispiel}

\marginnote{Vorlesungsbeginn 30.06.2016}

\begin{satz}[Cheeger-Ungleichung]
	Sei $Q$ die Übergangsmatrix einer ergodischen Markovkette auf $V$ (endlich) und $\pi$ sei das reversible Wahrscheinlichkeitsmaß zu $Q$. Sei wieder 
	\begin{align}
		h = \min\limits_{S:\pi(S)<1/2} \set{\frac{C(S,S^c)}{\pi(S)}} && \text{mit } C(S,S^C) = \sum\limits_{x \in S, y \notin S} c(x,y) = \sum\limits_{x \in S, y \notin S} \pi(x) Q(x,y)
	\end{align}
	Dann gilt:
	\begin{align}
		1-2h \leq \beta_1 \leq 1-\frac{h^2}{2} && \beta_1 \text{ist 2. größter Eigenwert}
	\end{align}
\end{satz}
\begin{beweis}
	Setze $\lambda_1 := 1 - \beta_1$ und zeige $\frac{h^2}{2} \leq \lambda_1 \leq 2h$. Sei $S$ mit $0 < \pi(S) \leq \frac{1}{2}$. Setze
	\begin{align}
		\Psi_S(x) = \begin{cases}
						\pi(S^C, & x\in S) \\
						\pi(S), & x \notin S
					\end{cases}
	\end{align}
	Es gilt $\EW{\Psi_S} = 0$. \underline{Also:}
	\begin{align}
		\lambda_1 \leq \frac{\mathcal{E}(\Psi_S,\Psi_S)}{\Var{\Psi_S}} &= \frac{\frac{1}{2}\sum\limits_{x,y}\enb{\Psi_S(x) - \Psi_S(y)}^2 c(x,y)}{\frac{1}{2} \sum\limits_{x,y} \enb{\Psi_S(x - \Psi_S(y))}} \\
							&= \frac{\sum\limits_{x\in S} \sum\limits_{y \in S} 1 c(x,y) }{\sum\limits_{x \in S}\sum\limits_{y \in S} 1 \pi(x) \pi(y) } = \frac{C(S,S^c)}{\pi(S) \pi(S^c)} \leq \frac{2C(S,S^c)}{\pi(S)}
	\end{align}

	\underline{Infimum bilden:} $\lambda_1 \leq 2h$ \\
	U.S: \underline{1. Beobachtung:} $\Psi \in L^2(\pi), \Psi^+ = \max(\Psi,0),	S(\Psi) = \set{ x : \Psi (x) > 0 }$. Dann gilt $\forall S(\Psi) \neq 0$ und alle $\lambda \geq 0$
	\begin{align}
		\lambda ||\Psi^+||^2_{L^2(\pi)} \geq \mathcal{E} (\Psi^+,\Psi^+)
	\end{align}
	falls $L\Psi := (\id - Q) \Psi \leq \lambda\Psi$ auf $S(\Psi)$. Denn
	\begin{align}
		\lambda ||\Psi^+||_{L^2(\pi)} = < \lambda\Psi^+, \Psi^+>_{\pi} \overset{*}{\geq} <L_{\Psi^+}, \Psi^+ > = \mathcal{E} (\Psi^+,\Psi^+)
	\end{align}

	\underline{2. Beobachtung:} $\forall \Psi \in L^2(\pi)$ mit $S(\Psi) \neq \emptyset$ gilt
	\begin{align}
		\mathcal{E} (\Psi^+,\Psi^+) \geq \frac{h^2(\Psi) ||\Psi||^2_{L^2(\pi)}}{2} \\
		\text{wo } h(\Psi) = \inf \set{ \frac{C(S,S^)}{\pi(S)} - t, \emptyset \neq S \subseteq S(\Psi)  }
	\end{align}
	denn einerseits: O.B.d.A. $\Psi \geq 0$ überall, dann gilt
	
	\begin{align}
		\sum\limits_{x,y} \abs{\Psi^2(x) - \Psi^2(y)} c(x,y) &= \sum\limits_{x,y} \abs{\Psi(x) - \Psi(y)} \abs{\Psi(x) - \Psi(y)} \sqrt{c(x,y)} \sqrt{c(x,y)} \\
		&\leq \sqrt{2} \mathcal{E}^{1/2} (\Psi,\Psi) \enb{\sum\limits_{x,y}\enb{\Psi(x) + \Psi(y)}^2 c(x,y) }^{1/2} \marginnote{Siehe Nebenrechnung}\\
		&\leq 2 \mathcal{E}^{1/2}(\Psi,\Psi) \enb{\sum\limits_{x,y} \enb{\Psi^2(x) + \Psi^2(y) } c(x,y) }^{1/2} \\
		&\leq 2^{3/2} \mathcal{E} (\Psi,\Psi) || \Psi ||_{L^2(\pi)}
	\end{align}
	
	Nebenrechnung: \todo{norm psi fehlt?}
	\begin{align}
		\Psi^2(x) + \Psi^2(y) c(x,y) = \sum\limits_{x,y} \Psi^2(x) \pi(x) Q(x,y) + \sum\limits_{x,y} \Psi^2(y) \pi(x) Q(x,y) = \sum\limits_{y}\Psi^2(y)\pi(y)
	\end{align}
	
	\underline{Andererseits:} 
	\begin{align}
		\sum\limits_{x,y} \abs{\Psi^2(x) - \Psi^2(y)}c(x,y) &= 2 \sum\limits_{x,y, \Psi(y) > \Psi(x)} \enb{\Psi^2(y) - \Psi^2(x)} c(x,y) = 4 \sum\limits_{x,y, \Psi(y) > \Psi(x)} \int_{\Psi(x)}^{\Psi(y)} t dt c(x,y) \\
		&= \marginnote{Fubini} 4 \int_{0}^{\infty} t\enb{\sum\limits_{\Psi(x) \leq t \leq \Psi(y)} c(x,y) } dt \marginnote{Sei $S := \set{y \given \Psi(y) > t} \subseteq S(\Psi)$} \\
		&= 4 \int_{0}^{\infty} t C(S,S^c) dt = 4 \int_{0}^{\infty} t \frac{C(S,S^c)}{\pi(S)} \pi(S) dt \\
		&\geq 4h(\Psi) \int_{0}^{\infty} t \pi(y : \Psi(y) > t) dt \\
		&= 4 \frac{h(\Psi)}{2} || \Psi ||_{L^2(\pi)}
	\end{align}		
	
	\todo{missing nebenrechnung. check gleichung oben}
	Zusammen: 
	\begin{align}
		2^{3/2} \mathcal{E}^{1/2}(\Psi,\Psi) || \Psi ||_{L^2(\pi)} \geq \sum\limits_{x,y} \abs{\Psi^2(x) - \Psi^2(y)} c(x,y) \geq 2 h(\Psi) || \Psi||^2_{L^2(\pi)}
	\end{align}
	Zusammen mit (3.3) \todo{REF}
	\begin{gather}
		\lambda || \Psi_+ ||^2_{L^2(\pi)} \geq \mathcal{E}(\Psi^+,\Psi^+) \frac{h^2(\Psi)}{2} ||\Psi_+||^2_{L^2(\pi)} \\
		\Rightarrow \lambda \geq \frac{h^2(\Psi)}{2}
	\end{gather}	
	wobei $h = \inf\limits_{S: \pi(S) \leq 1/2} \frac{C(S,S^c)}{\pi(S)}$. \\
	Das betrechten wir für $\lambda = \lambda_1$. Sei $\Psi$ eine Eigenfunktion von $L$ zu $\lambda_1$- Damm stimmt $(*)$ \todo{REF}, d.h. 
	\begin{align}
		\lambda_1 \geq \frac{h^2(\Psi)}{2} \geq \frac{h^2}{2} && h^2(\Psi) = \inf\limits_{S \subseteq S(\Psi)} \frac{C(S,S^c)}{\pi(S)}
	\end{align}
	solange $\pi\Big(S(\Psi)\Big) \leq \frac{1}{2}$ geht alles gut, denn dann ist das Infimum in $h(\Psi)$ über weniger Mengen $S$ als das in $h$. Nun ist $\EW{\Psi}_{\pi} = <\Psi, \mathds{1}>_{\pi} = 0$, denn $\Psi \perp \mathds{1}$, d.h. wenn $S(\Psi)$ so sein sollte, dass $\pi(S(\Psi)) > \frac{1}{2}$, dann gehen wir zu $\Psi$ über und erhalten $\pi(S(-\Psi)) \leq \frac{1}{2}$. Somit sind wir fertig.
\end{beweis}
\todo[inline]{undefined Control sequence titel $Z_p$ und $Z^d_2$}
\begin{beispiel}%[$\ZZ_p$]
	Sei $p$ ungerade und $\ZZ_p$ der $p$-Kreis. $\pi(x) = \frac{1}{p}, \forall x$, $h$ wird angenommen, wenn man für $S$ ein Intervall der Länge $\frac{p-1}{2}$ wählt. Dann gilt $h = \frac{2}{p-1}$. Also folgt mit Cheeger \todo{REF}
	\begin{align}
	1 - \frac{\Psi}{p-1} \leq p_1 \leq 1 - \frac{2}{\enb{p-1}^2}
	\end{align}
	\underline{Erinnerung:} $\beta_1 = \cos(\frac{2\pi}{p}) = 1 - \frac{2\pi^2}{p^2} + \mathcal{O}\enb{\frac{1}{p^4}}$. Hier ist für $p$ groß die untere Schranke schlecht und die obere Schranke bis auf einen Faktor $\pi^2$ gut.
\end{beispiel}

\begin{beispiel}%[$\ZZ^d_2$]
	Sei $V = \set{0,1}^d, \pi(x) = \frac{1}{2^d}, \forall x$.$h$ wird angenommen für die $S$, welche genau eine Koordinate festhalten, z.B.
	\begin{align}
		S = \set{x \given x_1 = 0} && \pi(S) = \frac{1}{2}
	\end{align}
	\begin{align}
		h = \frac{C(S,S^c)}{\pi(S)} = 2 \sum\limits_{x \in S, y \notin S} c(x,y) = \frac{2 \abs{S} }{2 \abs{E}} = \frac{2 \cdot 2^{d-1}}{2 (d/2) 2^d} = \frac{1}{d} \marginnote{SRW hat $c(x,y) = \frac{1}{2\abs{E}}$}
	\end{align}
	Es gilt $p_1 =  1 - \frac{2}{d}$ und wir erhalten mit Cheeger:
	\begin{align}
		1 - \frac{2}{d} \leq p_1 \leq 1 - \frac{1}{2d^2}
	\end{align}
	Die untere Schranke ist perfekt, während die obere Schranke um die Ordnung $d$ falsch ist (wie bei Pointcaré).	
\end{beispiel}

\begin{beispiel}[Binärbäume]
	
	Sei $T$ der Binärbaum mit Wurzel $v*$ und der Tiefe $d$. \todo{missing Tree?} Wähle als $S$ die linken oder rechten Nachfolger von $v*$ ohne die Wurzel. Dann gilt $\pi(S) = \frac{2^d-1}{2^{d+1}-1}$ Man rechnet aus
	\begin{align}
		h = \frac{1}{4 \cdot 2^{d-1} - 1} = \frac{1}{2\abs{E}} = \frac{1}{(2d^{d+2}) -4 } \marginnote{\enquote{Das ist noch eins der Dinge, die ich ganz gut kann: 2 mal 2}, Löwe}
	\end{align}
	\underline{Cheeger:}
	\begin{align}
		1 - \frac{1}{2^{d+1} -2} \leq \beta^1 \leq 1- \frac{1}{2} \enb{\frac{1}{2^{d+2} -4 }}^2
	\end{align}
	Der wahre Wert für $d$ groß ist $1 - \frac{1 + o(1)}{2^{d-2}}$. Pointcaré: $1- \frac{const}{d2^d}$
\end{beispiel}

\begin{beispiel}[Stern]
	Sei $Q$ die Übergangsmatrix für einen Random Walk auf dem $n$-Stern ($n$ Spitzen) mit holding $\Theta$. Man rechnet aus $h = 1 - \Theta$, $2 \Theta - 1 \leq \beta_1 \leq 1 - \frac{(1- \Theta)^2}{2}$. Z.B. für $\Theta = \frac{1}{2}$  $\beta_1 \leq \frac{7}{8}$. (Der wahre Wert $\beta_1 = \frac{1}{2}$)
\end{beispiel}








 