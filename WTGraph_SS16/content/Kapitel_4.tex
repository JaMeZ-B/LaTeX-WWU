\chapter{Cutsets, Pfade und Mischzeiten}
\section{Markovketten}
Wir betrachten für dieses Kapitel ergodische Markovketten auf einem endlichen Zustandsraum. Erinnerung: Eine Markovkette mit Übergangsmatrix $Q$ heßt ergodisch, wenn \underline{entweder}:
\begin{enumerate}
	\item $Q$ ist irreduzibel, d.h. $\forall x,y \ \exists n: Q^n(x,y)>0$ und
	\item $Q$ ist apersiondisch, d.h. $ggT\set{n \given Q^n(x,x) > 0} = 1$
\end{enumerate}
\underline{oder} 
\begin{gather}
	\exists N: Q^N \gg 0,\text{ d.h. } Q^N(x,y)>0, \forall x,y
\end{gather}
Wichtigster Satz über endliche Markovketten:
\begin{satz}[Ergodensatz für Markovketten]
	\label{satz:7-1}
	Sei $X_0,X_1,\dots$ eine ergodische Markovkette mit Startverteilung $v: \prop{X_0 = a} = v(a), \forall a$ und Übergangsmatrix $Q$ 
	auf einem endlichen Zustandsraum $\Omega$. Dann existiert genau ein $\mu \in M^1(\Omega)$ mit 
	\begin{enumerate}
		\item $\mu Q = \mu$
		\item $\prop{X_n = a}[][v] \to \mu(a) $
	\end{enumerate}
\end{satz}
\begin{bemerkung}
	Der Satz ist deshalb wichtig, weil er auch die Grundlage für Simulationsverfahren ist. Gegeben sei ein Wahrscheinlichkeitsmaß $\mu$ auf $\Omega$, welches man simulieren soll. Der Monte-Carlo-Ansatz ist, das Intervall $[0,1]$ in Teile der Länge $\mu(w_1), \dots, \mu(w_n), \abs{\Omega} = n$. Dann wählt man eine Zufallszahl $x \in [0,1]$ und wenn $x$ im $k$'-ten Intervall liegt, so sagt man, dass man $w_k$ als Ergebnis der Simulation erhält. Dieses Verfahren hat zwei mögliche Probleme. 
	\begin{enumerate}[a)]
		\item $\Omega$ kann endlich, aber groß sein, z.B. Curie-Weiss-Modell: $\Omega = \set{+- 1}^N$ mit $N \approx 10^{23}$. Somit ist $\abs{\Omega} \approx {2^{10}}^{23}$
		\item $\Omega$ kann endlich sein, aber $\abs{\Omega}$ ist unbekannt. Zum Beispiel im Knipsack-Problem: Gewichte $0 \leq a_1, \dots, a_N$ sollen in einen Rucksack der Kapazität $b$ gepackt werden. Wie viele Packungen gibt es?
	\end{enumerate}
\end{bemerkung}

\marginnote{Vorlesungsbeginn 09.06.2016}
	\underline{Beweisskizze zu \autoref{satz:7-1}} 
	\begin{enumerate}
		\item Die Existenz von $\pi$ glauben wir
		\item Mit etwas Zahlentheorie zeigt man: Wenn $Q$ ergodisch ist, dann existiert ein $N$, sodass $Q^N \gg 0$, d.h. $Q^N(x,y) > 0, \forall x,y \in E$
		\item Kopplungsargument: Wenn man mit der Startverteilung $\pi$ beginnt, so ist stets die Verteilung der Markovkette wieder $\pi$ (zu allen Zeiten). Man startet mit zwei Markovketten mit Übergangsmatrix $Q$, die eine $X_n$ mit Startverteilung $\nu$, die andere $X'_n$ mit Startverteilung $\pi$. Falls $X_m \neq X'_m, \forall m \leq n$, so laufen die Ketten unabhänig, vom Treffzeitpunkt an nehmen sie den gleichen Weg. Nachdem sich die Ketten treffen ist also auch $X_n$ nach $\pi$-verteilt. Zu zeigen ist also: Die Ketten treffen sich irgendwann. Da nun $Q^N \gg 0, \exists \alpha := \min\limits_{x,y} Q^N(x,y) > 0$. Dann ist die $\propE{X_N = X'_N} \geq \alpha^2$. Also $\propE{X_N \neq X'_N} \leq 1- \alpha^2$, und somit $\propE{X_n \neq X'_n, \forall n = 1, \dots, mN} \leq \enb{1- \alpha^2}^m \to 0.$
	\end{enumerate}

\begin{korollar}[Bemerkung \& Korollar]
	Dieser Satz lässt sich auch in anderen Metriken formulieren, typischerweise nimmt der Abstand der Totalvariation $d_{TV}$. Seien $\mu,\nu \in M^1(E)$, dann
	\begin{align}
		d_{TV} (\mu,\nu) &:= \sup\limits_{A \subseteq E}\abs{\nu(A) - \mu(A)} \\
		&= \frac{1}{\epsilon} \sum\limits_{x \in E}(\mu(\set{x}) - \nu(\set{x})  \marginnote{Da $E$ diskret}
	\end{align}
	Dann ließt sich der Ergodensatz für Markovketten als: Sei $(X_n)_n$ eine ergodische Markovkette auf $E$ mit Übergangsmatrix $Q$, dann gilt für alle $\nu \in M^1(E)$
	\begin{align}
		d_{TV}(\nu Q^n,\pi) = d(\nu Q^n,\pi) \to 0
	\end{align}	
\end{korollar}
Für alle Anwendungen ist allerdings die Frage der Konvergenzgeschwindigkeit essentiell. Die Konvergenz gegen das Gleichgewichtsmaß $\pi$ sollte wesentlich schneller sein als ein Monte-Carlo-Sample von $\pi$.
\begin{align}
	\nu Q^n = \nu S^{-1} D^n S = \nu S^{-1} \begin{psmallmatrix}
	1 & & & \\
	  & \lambda_2 &0 & \\
	  & 0 & \ddots & \\
	  & & &\lambda_{\abs{E}-1}
	\end{psmallmatrix}^n S
\end{align}
\begin{definition}
	Es sei $\epsilon > 0$, dann ist 
	\begin{gather}
		t_{mix}(\epsilon) = \min\set{t \given d_{TV}(\nu Q^t, \pi) \leq \epsilon} \\
		t_{mix} := t_{mix}(\frac{1}{4})
	\end{gather} 
	die $\epsilon$-Mischzeit bzw. die Mischzeit der Markovkette. Weiter sei 
	\begin{align}
		d(t) &= \max\limits_x \set{d\enb{\delta_x Q^t, \pi}} \\
		\overline{d}(t) &= \max_{x,y} \set{d\enb{\delta_x Q^t, \delta_y Q^t}}
	\end{align}
\end{definition}
\begin{bemerkung}
	Die Wahl von $\epsilon = \frac{1}{4}$ ist willkürlich, jedes $\epsilon < \frac{1}{2}$ liefert sinnvolle Ergebnisse, wie wir gleich sehen werden. 
\end{bemerkung}

\begin{lemma}
	Es gilt stets
	\begin{align}
		d(t) \leq \overline{d}(t) \leq 2d(t).
	\end{align}
\end{lemma}
\begin{beweis}
	Es gilt: 
	\begin{align}
		\overline{d}(t) &= \max\limits_{x,y} d(\delta_x Q^t, \delta_y Q^t) \\
			&\leq \max d\enb{\delta_x Q^t, \pi} + \max\limits_y \enb{\delta_y Q^t,\pi} \marginnote{Dreiecksungleichung} \\
			&= 2d(t)
	\end{align}
	Für die andere Richtung bemerke, dass mit $Q(y,A) = \sum\limits_{x \in A} Q(y,x), A \subseteq E$ gilt:
	\begin{align}
		\pi(A) = \pi(y) Q^{(t)}(y,A)
	\end{align}
	Also
	\begin{align}
		d(\delta_x Q^t,\pi) &= \max\limits_{A \subseteq \Omega} \abs{\delta_x Q^t (A) = \pi (A)} \\
			&= \max\limits_{A \subseteq \Omega} \abs{\sum\limits_{y \in \Omega} \pi(y) \benb{Q(x,A) - Q^t(y,A)} } \\
			&\leq \sum\limits_{y} \pi(y) \max\limits_{A} \abs{Q^t\enb{x,A} - Q^t(y,A) } \marginnote{Dreiecksungleichung} \\
			&= \sum\limits_{y}\pi d(\delta_x Q^t, \delta_y Q^t) \leq d(t) \marginnote{$\max\limits{x,y}$} \\
			&\Rightarrow \text{ Max über $x$ ergibt } d(t) \leq \overline{d}(t)
	\end{align}
\end{beweis}

\begin{lemma}
	$\overline{d}$ ist submultiplikativ, d.h. 
	\begin{align}
		\overline{d}(s + t) \leq \overline{d}(s) \cdot \overline{d}(t)
	\end{align}
\end{lemma}
\begin{beweis}
	Wir benutzen (ohne Beweis): Für $\mu,\nu \in M^1(E)$ gilt
	\begin{align}
		d(\mu,\nu) &= \inf\set{\prop{X \neq Y} \given (X,Y)\text{ ist eine } \mu,\nu \text{-Kopplung}} \\
				&=  \min\set{\prop{X \neq Y} \given (X,Y)\text{ ist eine } \mu,\nu \text{-Kopplung}}
	\end{align}
	$(X,Y)$ ist eine $\mu,\nu$-Kopplung, wenn $\p^X = \mu, \p^Y = \nu$. So etwas existiert immer, wenn man z.B. $\p^{X,Y} = \mu \otimes \nu$ wählt. Die Kopplung in der das Minimum angenommen wird, heißt optimale Kopplung. Seien nun $x,y\in E$ und für alle $t$ sei $(X_t,Y_t)$ die optimale Kopplung von $\delta_x Q^t$ und $\delta_y Q^t$. Dann gilt für alle $x,w \in E$
	\begin{align}
		Q^{t+s}(x,w) = \sum\limits_{z} Q^s(x,z) Q^t(z,w) =\sum\limits_{z} \propE{X_s = z}[X] Q^t(z,w) = \EW{Q^t(X_s,w)}
	\end{align}
	und
	\begin{align}
		Q^{t+s}(y,w) = \EW{Q^t(Y_s,w)}
	\end{align}
	Also:
	\begin{align}
		Q^{s+t}(x,w) - Q^{t+s}(y,w) &= \EWE{Q^t(X_s,w) - Q^t(Y_s,w)} \\
		&\Rightarrow d(\delta_xQ^{s+t}, \delta_y Q^{s+t}) = \frac{1}{2} \sum\limits_{w} \abs{\EWE{Q^t(X_s,w) - Q^t(Y_s,w)}} \marginnote{2. Def. von $d_{TV}$} \\
		&\leq \EWE{\frac{1}{2} \sum\limits_{w} \abs{Q^t(X_s,w) - Q^t(Y_s,w) }} \marginnote{Dreiecksungleichung} \\
		&= \EWE{d(Q^t(X_s,\cdot), Q^t(Y_s,\cdot)} \\
		&\leq \EWE{\overline{d}(t) \mathds{1}_{\set{X_s \neq Y_s}}},
	\end{align}
	weil $d \leq \overline{d}$ und die Ausdrücke gleich sind, wenn $X_s = Y_s$. Weiter:
	\begin{align}
		\overline{d}(t) \prop{X_s \neq Y_s} = \overline{d}(t) d(s) \leq \overline{d}(t) \overline{d}(s)
	\end{align}
	Also:  
	\begin{align}
		c \in \NN: d(ct) \leq \overline{d}(ct) \overline{d}(t)^c && \Longrightarrow && d(ct_{mix}(\epsilon)) \leq \overline{d}(t_{mix}(\epsilon))^c \leq \enb{2d(t_{mix} (\epsilon) )}^c \leq \enb{2\epsilon}^c
	\end{align}
	Also $d(ct_{mix}) \leq 2^{-c}$
	\begin{align}
		\Rightarrow t_{mix} (\epsilon) \leq \lceil\log_2 \frac{1}{\epsilon}\rceil t_{mix} 
	\end{align}
\end{beweis}

\marginnote{Vorlesungsbeginn 13.06.2016}
Zunächst zeigen wir untere Schranken für $t_{mix}(\epsilon)$. Die erste Idee basiert darauf, dass, wenn $\pi$ die Gleichverteilung ist auf $\Omega$ ist, man zumindest potenziell einen wesentlichen Anteil der Punkte gesehen haben können muss, um nahe an $\pi$ zu sein. \\
\underline{Umsetzung:} Es sei $(X_n)_n$ ergodisch auf $\Omega, \abs{\Omega} < \infty, \pi(\omega) = \frac{1}{\abs{\Omega}}, \forall \omega \in \Omega$ und definiere $d_{out}(x) := \abs{\set{y \in \Omega \given Q(x,y) > 0}}$ als die Anzahl der Punkte, die man in einem Schritt von $x$ aus erreichen kann. Sei
\begin{align}
	\Delta := \max\limits_x d_{out}(x)
\end{align}
Dann gilt
\begin{satz}
	Es ist 
	\begin{align}
		t_{mix}(\epsilon) \geq \frac{\log(\abs{\Omega} (1 - \epsilon))}{\log(\Delta)} 
	\end{align}
\end{satz}
\begin{beweis}
	Es sei $\Omega^t_x = \set{y\given Q^t(x,y) > 0}$. Es ist $\abs{\Omega^t_x} \leq \Delta^t$. Ist nun $\Delta^t < (1-\epsilon)\abs{\Omega}$ so folgt
	\begin{align}
		&\Norm{Q^t(x,\cdot) - \pi}_{TV} = d_{TV}(\delta_x Q^t, \pi) \\
		\geq &\abs{Q^t(x,\Omega^t_x) - \pi(\Omega^t_x)} = 1 - \frac{\abs{\Omega^t_x}}{\abs{\Omega}} \geq 1 - \frac{\Delta^t}{\abs{\Omega}} \\
		\overset{(*)}{>} & 1-(1-\epsilon) =  \epsilon,
	\end{align}
	wobei $(*): \Delta^t < (1-\epsilon) \abs{\Omega}$. 
	Das heißt solange $(*)$ wahr ist, ist der totale Variationsabstand größer als $\epsilon$.
	\begin{align}
		&\Rightarrow t_{mix}(\epsilon) \text{ muss } \Delta^{t_{mix}(\epsilon)} \geq (1-\epsilon)\abs{\Omega} erfüllen \\
		&\Rightarrow t_{mix}(\epsilon) \geq\frac{\log\big((1-\epsilon) \abs{\Omega}\big)}{\log\Delta}
	\end{align}
 \end{beweis}

Wichtiger Unterschied: Man muss $t_{mix}(\epsilon)$ mindestens $(1-\epsilon)\abs{\Omega}$ gesehen haben können, man muss sie de facto nicht gesehen haben (Diese Zeit wäre viel viel größer). 
\begin{beispiel}[Irrfahrt auf einem $d$-regulären Graphen]
	Es sei $G = (V,E)$ $d$-regulär, das heißt $\deg(v) = d$. Wählt man darauf die einfache Irrfahrt 
	\begin{gather}
		\prop{X_n = y \given X_{n-1} = x} = 
		\begin{cases}
			\frac{1}{d}, & \forall y \sim x \\
			0, & \text{ sonst} 
		\end{cases}
	\end{gather}
	Dann ist das stationäre Maß von $(X_n)$ die Gleichverteilung. $\pi(v)= \frac{1}{\abs{v}}$. Weiter ist $\Delta = d$ und das ergibt $t_{mix} \geq \frac{\log( \abs{V}(1-\epsilon))}{\log d}$ \\
	Spezialfall: $V = \set{0,1}^d$ (der Hyperwürfel)
	\begin{align}
		E = \set{\set{x,y} \given d_H(x,y) = \#\set{i \given x_i \neq y} = 1} \marginnote{$d_H$: Hammy}
	\end{align}
	Die Mischzeit für SRW wird hier beschränkt durch
	\begin{align}
		t_{mix}(\epsilon) \geq \frac{\log(\abs{\Omega} (1-\epsilon))}{\log d} = \frac{log(2^d (1-\epsilon) )}{\log d} = \frac{d \log 2}{\log d} + \frac{log(1-\epsilon)}{\log d}
	\end{align}
\end{beispiel}

Der folgende Ansatz verfolgt eine ähnliche Idee, gilt aber auch für allgemeine $\pi$. Beschränke nun $\overline{d}(t)$, das via $\overline{d}(t) \leq 2 d(t)$ auch eine Schranke für $d(t)-d$ somit für $t_mix$. Gegeben sei eine ergodische Markovkette auf $\Omega$. Fortan wird in diesem Kapitel $\Omega$ endlich sein, wenn nicht explizit anders deklariert. Wir machen$\Omega$ zu einem Graphen vermöge $V = \Omega$ und 
\begin{align}
	E = \set{ \set{x,y} \given Q(x,y), Q(y,x) > 0 }
\end{align}
Dieser Graph $G = (V,E)$ hat einen natürlichen Abstand $d_G$. 
\begin{align}
	\text{diam} (G) := \max\limits_{x,y \in V} d_G(x,y) \marginnote{\enquote{Ah! Groß \enquote{D}. Zum Glück hat Gott auch die Großbuchstaben erfunden}, Löwe 13.06.2016}
\end{align}
Wir sagen die Markovkette $(X_n)$ oder die Übergangsmatrix $Q$ hat den Durchmesser $D$, falls $\text{diam} (G) = D$. Falls nun $\text{diam} (Q) = D$, dann gibt es $x_ß,y_0 \in \Omega$ mit $d_G(x_0,y_0) =  D.$ Somit können sich zwei Markovketten mit Übergangsmatrix $Q$, von denen eine in $x_0$ und eine in $y_0$ startet zur Zeit $\lfloor\frac{D-1}{2}\rfloor$ noch nicht getroffen haben. 
\begin{align}
	\Rightarrow d_{TV} \Big( \delta_x Q^{\lfloor\frac{D-1}{2}\rfloor}, \delta_y  Q^{\lfloor\frac{D-1}{2}\rfloor}  \Big) = 1 \\
	\Rightarrow \overline{d}(\lfloor\frac{D-1}{2}\rfloor) > 1.
\end{align}
Da $\overline{d}(t) > 2d(t)$ heißt das für $\epsilon< \frac{1}{2}$, dass $t_{mix} (\epsilon) > \lfloor \frac{D-1}{2} \rfloor.$ 
Also haben wir gezeigt:

\begin{satz}
	Gilt $\text{diam} (Q) = D$, so gilt für alle $\epsilon > \frac{1}{2}$
	\begin{align}
		t_{mix}(\epsilon) \geq \lfloor\frac{D-1}{2}\rfloor
	\end{align}
\end{satz}

Bislang haben wir wenig von der \enquote{Gemoetrie} von $(X_n)n$ benutzt. Die Idee \enquote{Um nahe an $\pi$ zu sein, muss man die Punkte, die $\pi$ mit hoher Wahrscheinlichkeit ausstattet gesehen haben können} wollen wir nun verknüpfen mit \enquote{das ist aber schwierig, wenn der Übergangsgraph Flaschenhälse hat}. \todo{Bild Flaschenhals}Gibt es relativ kleine Teilmengen, die man nur schlecht verlassen kann? Wir suchen ein Maß für \enquote{Flaschenhalsigkeit}. Dafür sei $Q$ ergodisch und reversibel auf $\Omega$. Sei $\pi Q = \pi$, $\pi$ W-Maß. Wie schon in Kapitel I-III definiere $c(x,y) = \pi(x) Q(x,y) = \pi(y) Q(y,x).$ Es sei für $A,B \subseteq \Omega$
\begin{align}
	C(A,B) = \sum\limits_{x\in A} \sum\limits_{y \in B} c(x,y)
\end{align}
$C(A,B)$ ist die Wahrscheinlichkeit von $A$ nach $B$ zu laufen, wenn man mit $\pi$ startet. 
\begin{definition}
	Das \enquote{Flaschenhalsmaß} einer Menge $S \subseteq \Omega$ ist
	\begin{align}
		\Phi(S) = \frac{C(S,S^C)}{\pi(S)}
	\end{align}
	Ein \enquote{Flaschenhalsmaß} für die Markovkette $X_n$, die durch $Q$ beschrieben wird ist 
	\begin{align}
		\Phi_* := \Phi_*(Q) = \min\limits_{S:\pi(S)\leq 1/2} \Phi(S)
	\end{align}
\end{definition}
\begin{beispiel}
	Sei (V,E) ein zusammenhängender Graph. Sei $X_n$ Simple Random Walk auf $G$, d.h. 
	\begin{align}
		Q(x,y) = \begin{cases}
				\frac{1}{\deg x}, & \text{wenn } x \sim y \\
				0,					& \text{sonst }
 		\end{cases}
	\end{align}
	Das invariante Wahrscheinlichkeitsmaß hierfür ist $\pi(x) = \frac{\deg x}{2\abs{E}}$. Dann ist 
	\begin{align}
		c(x,y) = \begin{cases}
					\frac{1}{\cancel{\deg x}} \cdot \frac{\cancel{\deg x}}{2 \abs{E}} = \frac{1}{2 \abs{E}}, &\text{wenn } x\sim y \\
					0,		&\text{sonst}
				\end{cases}
	\end{align}  
	das heißt
	\begin{align}
		C(S,S^c= = \frac{\# \set{(x,y) \given x \in S, y \in S^c} }{2 \abs{S}} = \frac{\abs{\delta S}}{2\abs{E}}
	\end{align}
	Somit 
	\begin{align}
		\Phi(S) = \frac{\abs{\delta S}}{\sum\limits_{x \in V}^{\deg x}}
	\end{align}
	Wenn wir also die Mischzeit in Bezug zu $\Phi_*$ setzen können, dann sind Mengen kleiner Oberflächen \enquote{schlimm} für die Markovkette.
\end{beispiel}
\begin{beweis}
	\begin{enumerate}
		\item Anstelle normaler Irrfahrten betrachten man oft \enquote{faule Irrfahrten}. Diese folgt mit Wahrscheinlichkeit $1-\epsilon$ der Übergangsmatrix $Q$ und mit Wahrscheinlichkeit $\epsilon$ der Identität, d.h. die Übergangsmatrix ist $(1-\epsilon) Q + \epsilon I)$. Oft ist $\epsilon = \frac{1}{2}$. Zwei Gründe dafür: 
		\begin{enumerate}[i]
			\item Die Kette ist automatisch aperiodisch.
			\item Beim letzten Mal motiviert: $t_{mix}$ hängt im allgemeinen von $1-\lambda_2$ und $\lambda_{\abs{\Omega} -1} +1$ ab. 
		\end{enumerate}
	Für die Faule Irrfahrt mit $\epsilon = \frac{1}{2}$ ist der zweite Ausdruck $\geq$ 0. Für die Frage, ob die Irrfahrt polynomiell oder exponentiell mischt, ist das Warten irrelevant, die Konvergenz verlangsamt sich lediglich um den Faktor 2. 
		
		Für die faule Irrfahrt mit $\epsilon = \frac{1}{2}$ gilt
		\begin{align}
			\Phi(S) = \frac{2 \abs{\delta S}}{\sum\limits_{x \in S} \deg x}
		\end{align}
		\item Ist $(V,E)$ $d$-regulär, dann ist
		\begin{align}
			\Phi(S) = \frac{\abs{\delta S}}{d \abs{S}}
		\end{align}
	\end{enumerate}
	Ziel/Ausblick: Was sagt $\Phi_*$ über $t_{mix}$ aus? Wir zeigen: $t_{mix} \geq \frac{1}{4\Phi_*}$
\end{beweis} 
\marginnote{Vorlesungsbeginn 16.06.2016}
\begin{satz}
	\begin{align}
		t_{mix} \geq \frac{1}{4\Phi_*}
	\end{align}
\end{satz}
\begin{beweis}
	$S \subseteq V.$ Sei $\pi_S = \pi |S$, d.h. $\pi_S(A) = \pi(A\cap S)$ und $\mu_S(A) = \frac{\pi_S(A)}{\pi(S)}$ die bedingte Wahrscheinlichkeit gegeben $S$. \\
	1. Beobachtung: Für $\mu,\nu \in M^1(V)$ ist 
	\begin{align}
		\Norm{ \mu - \nu }_{TV} = \sum\limits_{v: \mu(v) \geq \nu(v)}\mu(v) - \nu(v)
	\end{align}
	2. Beobachtung: 
	\begin{align}
		\pi_S \Norm{\mu_SQ - \mu_S } = \Norm{ \pi_S Q - \pi_S}= \sum\limits_{w: \pi_S Q (w) \geq \pi_S (w)}\pi_SQ(w) - \pi_S(w)
	\end{align}
	Einerseits gehören alle $w\in S^c$ zu der Menge $\set{w \given \pi_SQ(w) \geq \pi_S(w) }$, denn für diese ist $\pi_S(w) = 0$. Andererseits:
	\begin{align}
		\pi_S(w) &= \sum\limits_{v \in V}\pi_S(v)Q(v,w) = \sum\limits_{v \in S}\pi_S(v)Q(v,w) \\
				&= \sum\limits_{v \in S} \pi(v) Q(v,w) \leq \sum\limits_{v \in V}\pi(v) Q(v,w) = \pi(w)
	\end{align}
	Also wenn $w \in S \Rightarrow \pi_SQ(w) \leq \pi_S(w)$. Also
	\begin{align}
		\Norm{ \pi_SQ - \pi_S } &= \sum\limits_{w \in S^c} \pi_SQ(w) - \pi_S(w) = \sum\limits_{v \in S}\sum\limits_{w \in S^c} \pi(c)Q(v,w)-0 \\
							&= \mathcal{C}(S,S^c) 
	\end{align}
	Also (Teilen durch $\pi(S)$)
	\begin{align}
		\Norm{ \mu_SQ - \mu_S } = \Phi(S)
	\end{align}
	Von hier kommen wir mir der folgenden Übung weiter
	\begin{align}
		\forall n \geq 1: \Norm{ \mu_SQ^{n+1} - \mu_S Q^n }_{TV} \leq \Norm{ \mu_SQ - \mu_S } = \Phi(S) \marginnote{Übung}
	\end{align}
	Mit der Dreiecksungleichung folgt nun
	\begin{align}
		\forall m \in n: \Norm{ \mu_SQ^m-\mu_S }_{TV} &= \Norm{ \sum\limits_{n=1}^{m}\mu_SQ^n - \mu_S Q^{n-1} } \leq \sum\limits_{n = 1}^{m} \Norm{ \mu_S Q^n - \mu_S Q^{n-1} } \leq m \cdot \Phi(S)
	\end{align}
	Sei nun $S$ mit $\pi(S)\leq \frac{1}{2}$. Da $\mu_S(S^c) = 0$ gilt
	\begin{align}
		d_{TV}(\mu_S,\pi) = \max\limits_A \abs{\pi(A - \mu_S(A))} \geq \pi(S^c)- \mu_S(S^c) = \pi(S^c) \geq \frac{1}{2}
 	\end{align}
 	und mit der Dreiecksungleichung 
 	\begin{align}
 		\frac{1}{2} \leq \Norm{ \mu_S - \pi }_{TV} \leq \Norm{ \mu_S - \mu_SQ^m } + \Norm{ \mu_SQ^m-\pi } \leq t_{mix}\Phi(S) + \frac{1}{4}
  	\end{align}
  	wenn man $m = t_{mix}$ wählt. Also
  	\begin{align}
  		t_{mix} \Phi(S) \geq \frac{1}{4} \Rightarrow t_mix \geq \frac{1}{4\Phi(S)}
  	\end{align}
  	Minimieren über die $S$ mit $\pi(S) \leq \frac{1}{2}$ ergibt 
  	\begin{align}
  		t_{mix} \geq \frac{1}{4\Phi_*}
  	\end{align}
\end{beweis}

\begin{beispiel}[Verklebte Tori]
	\label{bsp:verklebtte-Tori}
	Ein diskreter d-dimensionaler Torus ist ein Graph $G = (V,E)$ mit $V = \underbrace{\ZZ_n \times \dots \times \ZZ_n}_{\text{d-mal}}$, und $\ZZ_n = \set{0,\dots,n-1}$ und für $x = (x_1,\dots,x_d), y = (y_1,\dots,y_d)$ sei 
	\begin{align}
		e = \set{x,y} \in E \Leftrightarrow \exists j \in \set{1,\dots,d} :\forall i \neq j: x_i = y_i  \text{ und } x_j = (y_j \pm 1 \mod{n}).
	\end{align} 
	Seien nun $T_1$ und $T_2$ zwei d-dimensionale n-Tori. Diese verkleben wir in genau einem Punkt $v^*$. Sei mim $V_1 = V(T_1)$, $V_2 = V(T_2) \Rightarrow V_1 \cap V_2 = \set{v^*}$. Wähle für die einfache Irrfahrt auf diesen Gebilden $S = V_1\backslash \set{v^*}$. Da jeder Knoten aus $T_1$ oder $T_2$ mit genau $2d$ Knoten aus diesem Torus verbunden ist, ist $\abs{\delta S} = 2d$. Nun ist 
	\begin{align}
		\sum\limits_{x \in S} \deg{x} = 2d(n^d-1). 
	\end{align}
	Damit ist $\Phi(S) = \frac{1}{n^d - 1}$. Wählt man $d = 2$ ist $\Phi(S) \approx \frac{1}{n^2}$. Damit erhält man in diesem Fall $t_{mix} \geq \frac{\enb{n^2-1}}{4}$. Die wahre Mischzeit ist $\mathcal{O}(n^2 \log n)$
\end{beispiel}

\begin{beispiel}[Graphenfärben]
	\label{bsp:graphenfärben}
	Sei $G = (V,E)$ ein Graph. Eine Färbung von $G$ mit $q$ Farben $\set{1,\dots,q}$ ist eine Abbildung $f: V \to \set{1,\dots,q}$, mit $f(v) \neq f(w)$, falls $\set {v,w} \in E$. Es gibt zwei offensichtliche Fragen: 
	\begin{enumerate}
		\item Wie groß muss $q$ minimal sein? 
		\item Wie viele Färbungen gibt es? 
	\end{enumerate}Die zweite Frage versucht man approximativ mithilfe eines MCMC-Verfahrens zu lösen.\marginnote{MCMC = Monte Carlo Markov Chain} Dazu sucht man eine Markovkette auf $\Omega = \set{f:V \to \set{1,\dots,q} \given f \text{ zulässige Färbung}}$, die als stationäres Maß die Gleichverteilung hat. Nun bastelt man Graph $G = G_{\abs{E}} \supseteq G_{\abs{E}-1} \supset \dots \supset G_0 = \enb{V,E_0}, E_0 = \emptyset$, indem man sukzessive Kanten weglässt. Sei $\Omega_i$ die Menge der Färbungen von $G_i \Rightarrow G_0 = q^{\abs{V}}$. Kann man die Gleichverteilung auf allen $\Omega_i$ gut simulieren, so kann man auch $\frac{\abs{\Omega_i}}{\abs{\Omega_{i-1}}}$ gut schätzen. Also kann man auch $\abs{\Omega}$ gut schätzen. 

Wie sampelt man die Gleichverteilung auf $\Omega$? \\
	\enquote{Glauberdynamik} - Beginne mit irgendeiner zulässigen Färbung.
	\begin{itemize}
		\item wähle zufällig $v \in V$
		\item wähle zufällige Farbe
		\item ist diese nicht zulässig, behält $v$ seine Farbe, ansonsten streiche $v$ in der neuen Farbe an
	\end{itemize}
	\underline{Mitteilung:} Wenn $q > \Delta = \max\limits_{v \in V} \deg{v}$, dann ist die Kette aperiodisch und irreduzibel, und $\pi = \frac{1}{\abs{\Omega}}$ ist das stationäre Maß \\
	\underline{Mitteilung:} Ist $q > 2\Delta$, dann mischt die Kette in Zeit $\abs{V} \log \abs{V}$. Es kann aber auch schief gehen, wenn $\Delta$ groß wird. Sei dazu $G = (V,E)$ der $n$-Stern, d.h. $G$ ist ein Baum der Tiefe 1 mit Wurzel $v^*$ und $n-1$ Blättern.
	Sei $\Omega \supseteq S = \set{f \in \Omega \given f(v^*) =  1}$. Sei $(x,y) \in S \times S^c$ und es gelte c(x,y) > 0 genau dann, wenn
	\begin{itemize}
		\item $x(v^*) = 1, y(v^*) \neq 1$
		\item $x(v) =  y(v) \forall v \neq v^* $
		\item $x(v) \notin \set{1,y(v^*)} \forall v \neq v^*$ 
	\end{itemize}
	Also gilt für 
	\begin{align}
		A = \set{(x,y) \in S\times S^c, c(x,y)> 0} \Rightarrow \abs{A} = (q-1)(q-2)^{n-1} \marginnote{Es gibt $(q-1)$ Färbungen von $v^*$ unter $y$}
	\end{align}
	Für alle $(x,y) \in A$ ist außerdem $c(x,y) = \pi(x)Q(x,y) \leq \frac{1}{\abs{\Omega}} \frac{1}{n}$. Daraus folgt
	\begin{align}
		C (S,S^c) \leq \frac{1}{\abs{\Omega} n} \enb{q-1} \enb{q-2}^{n-1}.
	\end{align}
	Umgekehrt gilt $x \in S \Leftrightarrow x(v^*) = 1 \land x(v) \neq 1, \forall v \neq v^*$. Das heißt $\abs{S} = \enb{q-1}^{n-1}$
	\begin{align}
		\Rightarrow \frac{C(S,S^c)}{\pi(S)} &\leq \frac{1}{\abs{\Omega} n}(q-1)(q-2)^{n-1} \frac{\abs{\Omega}}{(q-1)^{n-1}} \\
		&= \frac{\enb{q-1}^2}{n(q-2)} \enb{1  - \frac{1}{q-1}}^n \\
		&\leq \frac{(q-1)^2}{n(q-2)} e^{-\frac{n}{q-1}} 
	\end{align}
	Und daraus folgt
	\begin{gather}
		t_{mix} \geq \frac{n(q-2)}{4(q-1)^2} e^{\frac{n}{q-1}}
	\end{gather}
	Wählt man nun $n$ und $q$ so, dass $\frac{n}{q \log q} \to \infty$ dann wächst $t_{mix}$ super-polynomiell.
\end{beispiel}
 
\marginnote{Vorlesungsbeginn 20.06.2016}
Ein Beispiel (zu $t_{mix} \geq \frac{1}{4 \Phi_*}$), in dem alles gut geht: 
\begin{beispiel}[Der Binärbaum]
	\label{bsp:Binärbaum}
	Sei $T=(V,E)$ der Binärbaum der Tiefe $k$, d.h. ein Baum mit Wurzel $v^*$, wobei $\deg(v^*) = 2, \deg(v) = 3$ für alle $v \neq v^*$, die keine Blätter sind und $\deg(v) = 1$ für alle Blätter. Sei $n = \abs{V} = 2^{k+1} -1$. Dann ist $\abs{E} = n-1, d(v,v^*) = k$ für alle Blätter.
	
\end{beispiel}
\begin{uebung}
	Ist $G = (V,E)$ ein einfacher Graph, dann ist $G$ ein Baum genau dann, wenn $\abs{V} = \abs{E} +1$. 
\end{uebung}
 
 Auf $T$ lassen wir die einfache Irrfahrt laufen. \\
 \underline{Behauptung:} $t_{mix} \geq \frac{k-2}{4}$. \\
 Um das zu zeigen, seien $v_l$ und $v_r$ die direkten Nachbarn von $v^*$ und wir nennen $w$ einen Abkömmling von $v$ (oder $v$ einen Vorgänger von $w$), wenn der kürzeste Weg von $v^*$ nach $w$ durch $v$ geht. Wir wählen als $S$ die Menge der Abkömmlinge von $v_r$ inklusive $v_r$ selbst. Um $\Phi(S)$ zu berechnen, müssen wir zunächst $\pi(x), x \in V$ ausrechnen. 
 \begin{align}
 	\pi(x) = \begin{cases}
			 	\frac{2}{2n-2}, & x = v^* \\
			 	\frac{1}{2n-2}, & x \text{ ist ein Blatt}\\
			 	\frac{3}{2n -2}, & x \text{ ist ein innerer Knoten } \neq v^*	
		 	\end{cases}
 \end{align}
 Damit ist
\begin{align}
	\pi(S) = \sum\limits_{v: v \leftarrow v^*}\pi(v) = \sum\limits_{\substack{v \leftarrow v^* \\ v \text{ inn. Knoten}}} \pi(v) + \sum\limits_{\substack{v \leftarrow v^* \\ v \text{ Blatt}}} \pi(v) \overset{(*)}{=} \frac{1}{2} - \frac{1}{n-2} = \frac{n-2}{2n-2} \marginnote{$(*)$ aus Symmetriegründen}
\end{align}
 
 Man berechnet 
 \begin{align}
 	C =(S,S^c) = \sum\limits_{x \in S, y\in S^c} \pi(x) Q(x,y) = \pi(v^*) \frac{1}{3} = \frac{3}{2n-2} \cdot \frac{1}{3} = \frac{1}{2n-2}
 \end{align}
 Somit:
 \begin{align}
 	\Phi(S) = \frac{1}{2n-2} \frac{2n-2}{n-2} = \frac{1}{n-2}.
 \end{align}
 Damit $t_{mix} \geq \frac{n-2}{4}$. Diese Abschätzung ist ziemlich gut, denn ,man kann auch zeigen dass $t_{mix} \leq 4n$ gilt, d.h. untere und obere Schranke unterscheiden sich lediglich um eine Konstante. \\
 Curie-Weiß: $\beta =1$ kritische Temperatur $m_N = \frac{1}{N} \sum\limits_{i=1}^{N}\delta_i, m_N \sim 0$, für $\beta \leq 1:$ konzentriert sich $m_N$ um zwei Werte. Auch sind Simulationsalgorithmen für $\beta > 1$ exponentiell langsam.
 
 \enquote{Man kann zeigen, dass $t_{mix} \leq \dots$ gilt} haben wir in allen Beispielen \ref{bsp:verklebtte-Tori}, \ref{bsp:graphenfärben} und \ref{bsp:Binärbaum} gesagt. Aber wie zeigt man so etwas? Wir wollen eine Technik kennenlernen, die solche Abschätzungen aus der Verteilung der Eigenwerte von $Q$ generiert. 
 
 Die Situation ist wieder die, dass $Q$ eine bezüglich $\pi$ reversible Markovkette auf einem endlichen Zustandsraum $\Omega$ mit $\abs{\Omega} = m$ ist. Wir werden sehen, dass $Q$ nur reelle Eigenwerte hat. Diese ordnen wir an: $1 = \beta_0 > \beta_1 \dots \geq \beta_{m-1} \overset{*}{>} -1$\marginnote{* die Kette sei aperiodisch}. Man kann einen Operator zu $Q$ assoziieren, der vermöge
 \begin{align}
 	\benb{Q\phi}(x) = \sum\limits_{z \in \Omega} Q(x,z)\phi(z)
 \end{align}
 auf $L^2(\pi)$ (also alle Funktionen auf $\Omega$) wirkt. Intuitiv ist klar, dass $t_{mix}$ etwas mit $1 - \max\set{\abs{\beta_1},\abs{\beta_{m-1}}}$ zu tun haben muss. 
 	\begin{align}
 		Q^n = S^{-1} D^n S = S^{-1} \cdot 
 		\begin{psmallmatrix}
	 		1	&	&	&	 \\
	 			&\beta_1	&	&0	 \\
	 			&	& \diagdown	&	 \\
	 			&	0 & 	& \beta_{m-1}
 		\end{psmallmatrix}^n \cdot S
  	\end{align}
  	D.h. die Konvergenz findet in $D^n$ statt. Diese Matrix konvergiert gegen $\begin{psmallmatrix}
  	1 & & & \\
  	& 0 & & \\
  	& & \diagdown & \\
  	& & & 0
  	\end{psmallmatrix}$ und die Konvergenzgeschwindigkeit richtet sich nach dem betragsmäßig zweitgrößten Eigenwert.
  	
\begin{satz}
	\label{satz:11-17}
	In der obigen Situation sei $\beta_* = \max\set{\abs{\beta_i},i\neq 0}, \gamma* = 1- \beta_*$ die absolute Spektrallücke, $\gamma = 1 - \beta_1$ die Spektrallücke und $t_{rel} = \frac{1}{\gamma^*}$ die Relaxationszeit. Dann gilt für alle $\epsilon > 0$ 
	\begin{align}
		(t_{rel} - 1) \log \frac{1}{2 \epsilon} \leq t_{mix} (\epsilon) \leq \log\enb{\frac{1}{2\epsilon \sqrt{\pi_{min}}}} t_{rel}
	\end{align}
	wobei $\pi_{min} = \min\limits_{x} \pi(x) > 0$.
\end{satz}
\begin{beweis}
	\begin{enumerate}
		\item $t_{rel}$ gibt es bis auf Faktoren Auskunft über $t_{mix}$.
		\item Durch Übergang zur faulen Version der Markovkette, d.h. $\hat{Q} = \frac{1}{2}(Q + \id)$ genügt es \enquote{immer} $\gamma$ statt $\gamma*$ zu betrachten.
	\end{enumerate}
	Wir beginnen mit einem Lemma
	\begin{lemma}
		Sei $Q$ reversibel bezüglich $\pi$. Dann gilt:
		\begin{enumerate}[(i)]
			\item $Q$ hat nur reelle Eigenwerte $(\beta_0,\dots,\beta_m)$
			\item $(R^m,<\cdot,\cdot>)$ hat eine Orthonormalbasis aus Eigenfunktionen zu $(\beta_i)$
			\item $\forall t \in \NN$ gilt $\frac{Q^t(x,y)}{\pi(y)}  = \sum\limits_{j = 0}^{m-1} f_j(x) f_j(y)\beta_j^t$
			\item $f_0 \equiv 1$ ist zulässig also \begin{align}
				\frac{Q^t(x,y)}{\pi(y)} = 1+ \sum\limits_{j = 1}^{m-1} f_j(x) fj(y) \beta^t_j
					\end{align}
		\end{enumerate}
	\end{lemma}
	\begin{beweis}
		Sei $A(x,y) = \sqrt{\frac{\pi(x)}{\pi(y)}} Q(x,y).$ $A$ ist symmetrisch, denn $Q$ ist reversibel bezüglich $\pi$. Also hat $A$ lauter reelle Eigenwerte, nennen wir sie auch $\beta_0, \dots \beta_m-1$. Außerdem gibt es eine ONB des $\RR^m$ bezüglich $\sprod{\cdot,\cdot}$ (euklidisch) aus Eigenfunktionen $(\phi_j)_{j=0}^{m-1}$ von $A$. Man rechnet nach, dass $(\sqrt{\pi(x)})_x$ ein Eigenvektor zum größten Eigenwert $\beta_0 = 1$ ist. Mit $D_\pi = \text{diag}(\sqrt{\pi(x))}_x$ ist weiter $A = D^{1/2}_{\pi}  Q D^{1/2}_{\pi}$. Weiter sei $f_j =  D^{-1/2} \phi_j$. Dann sind $f_j$ Eigenfunktionen zu Eigenwert $\beta_j$ und der Matrix $Q$. 
		\begin{align}
			Qf_j = Q D^{-1/2}_{\pi} \phi_j = D^{-1/2}_\pi (D^{1/2}_\pi Q D^{-1/2}_{\pi}) \phi_j = D^{-1/2}_{\pi} A \phi_j = D^{-1/2}_{\pi} \beta_j \phi_j = \beta_j f_j
		\end{align}
		$f_j$ sind orthogonal (orthonormal) bezüglich $<\cdot,\cdot>_{\pi}$.
		\begin{align}
			\delta_{ij} = <\phi_i,\phi_j> = <D^{1/2}_{\pi} f_i, D_{\pi}^{1/2} f_j > = \sum\limits_{x} f_i(x) f_j(x) \pi(x) = <f_i,f_j>_{\pi}
		\end{align}
		Sei $\delta_y(x)ß = \begin{cases}
			1, &y = x \\
			0, &\text{sonst}
		\end{cases}$ Dann hat die Zerlegung 
		\begin{align}
			\delta_y = \sum\limits_{j = 0}^{m-1} <\delta_y,f_j>_{\pi} f_j = \sum\limits_{j = 0}^{m-1} f_j(y) \pi(y) f_j
		\end{align}
		Da $Q^t f_j = \beta^t_j f_j$ und $Q^t(x,y) = \benb{Q^t\delta_y} (x)$ folgt $Q^t(x,y) = \sum\limits_{j}f_j(y) \pi(y) \beta_j^t f(x) \Rightarrow $ Division durch $\pi(y)$ gibt das Ergebnis.
	\end{beweis}
	\underline{Beweis von \autoref{satz:11-17}}
	
	Wir zeigen nur die obere Schranke, weil uns diese hilft $t_{mix}$ von oben zu beschränken. Nach Definition von TV:
	\begin{align}
		\Norm{Q_t(x, \cdot) - \pi(\cdot)}_{TV} &= \frac{1}{2} \sum\limits_{y} \abs{Q^t(x,y) - \pi(y)} = \frac{1}{2} \sum\limits_{y} \pi(y) \abs{\frac{Q^t(x,y)}{\pi(y)} -1} \\
						&\leq \frac{1}{2} \Norm{\frac{Q^t(x,\cdot)}{\pi(\cdot)} -1 }_1  \marginnote{$\Norm{\cdot}_1$ 1-Norm in $L^1(\pi)$} 
	\end{align}
	d.h. $4 \Norm{Q^t(x,\cdot) - \pi(\cdot )}^2_{TV} \leq \Norm{\frac{Q^t(x,\cdot)}{\pi(\cdot)} -1}^2_2$. \marginnote{Mit Jensenscher Ungleichung und $\Norm{\cdot}_2$ die Norm in $L^2(\pi)$} Nach dem Lemma gilt:
	\begin{align}
		\abs{\frac{Q^t(x,y)}{\pi(y)} -1} &\overset{\Delta-\text{Ungl.}}{\leq} \sum\limits_{j =  1}^{m-1} \abs{f_j(x) f_j(y)} \beta^t_j \leq \beta^t_* \sum\limits_{j=1}^{m-1} \abs{f_j(x) f_j(y)} \\
			&\overset{C.S.}{\leq} \beta^t_* \enb{\sum\limits_{j = 0}^{m-1} f_j^2(x)}^{1/2} \enb{\sum\limits_{j = 0}^{m-1} f_j^2(y)}^{1/2}
	\end{align}
	Da \begin{align}
		\pi(x) &= <\delta_x(\cdot),\delta_x(\cdot)> = <\sum\limits_{j = 0}^{m-1}f_j(x \pi(x) f_j(\cdot), \sum\limits_{j = 0}^{m-1} f_j(x) \pi(x) f_j(\cdot)>_\pi \\
			&=\sum\limits_{j = 0}^{m-1} f_j^2(x)\pi^2(x) = \sum\limits_{j=0}^{m-1} f_j^2(x) = \frac{1}{\pi(x)}
	\end{align}
	Also
	\begin{align}
		4 \Norm{ Q^t (x,\cdot) - \pi(\cdot) }^2_{TV} \leq \beta^{2t}_* \sum\limits_{y}\frac{1}{\pi(x)} \frac{1}{\pi(y)} \pi(y) = \frac{m \beta^{2t}_*}{\pi(x)} &\leq m \beta^{2t}_* \frac{1}{\pi_{min}} \\
		&= (1-\gamma^*)^{2t} \frac{1}{\pi_{min}} \leq e^{-2jt} \frac{1}{\pi_{min}} \\
	\end{align}
	\begin{align}
		\Rightarrow d(t) \leq \epsilon \text{ wenn } t \geq \frac{1}{\gamma^*} \log \frac{1}{2\epsilon \sqrt{\pi_{min}}} = \log \enb{\frac{1}{2\epsilon\pi_{min}}} t_{rel}
	\end{align}
\end{beweis}
\todo{Zusammenfassung sollte passen}

\marginnote{Vorlesungsbeginn 23.06.2016}
\underline{Moral:} Es genügt $1-\beta_1$ abzuschätzen, um $t_{mix}$ zu kontrollieren. Eine Möglichkeit geht über den mit $Q$ assoziierten Laplace-Operator: $L = \id -Q$. Die Spektrallücke ist dann der erste von 0 verschiedene Eigenwert.

\begin{definition}
	Sei $Q$ reversibel bezüglich $\pi$, $c(x,y) = \pi(x) Q(x,y)$ die zu $(Q,\pi)$ assoziierte Dirichletform ist definiert durch
	\begin{align}
		\mathcal{E}(f,g) = \frac{1}{2}\sum\limits_{x,y}c(x,y)\enb{f(y) - f(x)}\enb{g(y)- g(x)}.
	\end{align}
	Also insbesondere
	\begin{align}
		\mathcal{E}(f,f) = \frac{x,y} c(x,y) \enb{f(y)-f(x)}^2
	\end{align}
\end{definition}

Man kann mittels $\mathcal{E}$ eine Abschätzung für $1-\beta_1$	erreichen.
\begin{satz}
	\label{satz:11-20}
	Sei $\lambda_1 = 1-\beta_1$, dann ist  
	\begin{align}
		\lambda = \min\limits_{\substack{f:S\to \RR \\ \EW{\relax}_{\pi}(f) = 0 \\ \Var{\relax}_{\pi}(f) = 1}} \mathcal{E}(f,f) = \min\limits_{\substack{f:S\to \RR \\ \EW{\relax}_{\pi} (f) = 0}} \frac{\mathcal{E}(f,f)}{\Norm{f}^2_{2,\pi}} = \min\limits_{\substack{f:S \to \RR \\ f \neq const.}} \frac{\mathcal{E}(f,f)}{\Var{\relax}_{\pi}(f)}
	\end{align}
\end{satz}
\begin{bemerkung}
	Das heißt in der Physik auch manchmal Raleigh-Ritz-Prinzip. Man sucht in gewissem Sinne möglichst glatte Funktionen, die aber nicht konstant sind.
\end{bemerkung}
\begin{beweis}
	Das letzte Gleichheitszeichen erhält man, indem man $f$ durch $f- \EW{\relax}_{\pi}(f)$ ersetzt, das vorletzte Gleichheitszeichen erhählt man, indem man $f$ durch $\frac{f}{\Norm{f}_{2,\pi}}$ ersetzt.
	Für die erste Gleichheit, sei $f: \EW{\relax}_{\pi}(f) = 0, \Norm{f}^{2,\pi} = 1$. Dann gilt
	\begin{align}
		0 = \EW{\relax}_{\pi}(f)= \sum\limits_{x}f(x)\pi(x) = \sum\limits_{x} f(x) \mathds{x} \pi(x) ) <f,\mathds{1}>_{pi} \marginnote{$\mathds{1}$-konstante 1 Fkt}
	\end{align}
	Da $\mathds{1}$ Eigenfunktion zu $\beta_0$ ist, ist $f orthogonal$ Eigenraum zu $\beta_0$. Also, mit $f_j$ Eigenfunktion von $\beta_j$
	\begin{align}
		f = \sum\limits_{j = 0}^{m-1}<f,f_j>_{\pi}f_j 0 \sum\limits_{j = 1}^{m-1}<f,f_j>_{\pi} f_j 
	\end{align}
	Andererseits
	\begin{align}
		\mathcal{E}(f,f) &= \frac{1}{2} \sum\limits_{x,y}c(x,y) \enb{f(x) - f(y)}^2 = \frac{1}{2} \sum\limits_{x,y}f^2(x) \pi(x) Q(x,y) + \frac{1}{2} \sum\limits_{x,y} f^2(y) \pi(x) Q(x,y) - \sum\limits_{x,y} f(x)f(y)\pi(x) Q(x,y) \\
		&= \frac{1}{2} \sum\limits_{x}f^2(x) \pi(x) \sum\limits_{y} Q (x,y) + \frac{1}{2} \sum\limits_{y} f^2(y) \pi(y) - \sum\limits_{x,y}f(x) f(y) \pi(x) Q(x,y) \\
		&= <(I-Q)f,f >_{\pi} = <Lf,f>_{\pi} = <L\sum\limits_{j=1}^{m-1}<f_i,f_j>_{pi},f_j, \sum\limits_{j=1}^{m-1}<f,f_j>_{pi}f_j>_{\pi} \\
		&= <\sum\limits_{j=1}^{m-1}(1-\beta_j)<f,f_j>_{\pi}f_j,\sum\limits_{j=1}^{m-1}<f,f_j>_{\pi}f_j>_{\pi} \marginnote{$f_j$ sind Eigenfkt von $L$} \\
		&= \sum\limits_{j = 1}^{m-1}(1-\beta_j)<f,f_j>^2_{\pi}\geq \enb{1-\beta_1} \sum\limits_{j = 1}^{m-1}<f,f_j>^2_{\pi}  =\enb{1-\beta_1} \sum\limits_{j = 0}^{m-1}<f,f_j>^2_n \marginnote{$f_j$ ONB}
		&= \enb{1- \beta_1} \Norm{f}^2_{2,\pi} = (1-\beta_)
	\end{align}
\end{beweis}

Um \autoref{satz:11-20} anzuwenden, sei nun ein Random Walk auf einem zusammenhängenden Netzwerk $G = (V,E,c)$ mit Übergangsmatrix $Q$ und stationären Maß $\pi$ gegeben. F+r je zwei Punkte $x \neq y \in V$ sei $\gamma_{xy}$ ein Pfad von $x$ nach $y$ und 
\begin{align}
	\Gamma = \set{\gamma_{x,y} \given x,y \in V}
\end{align}
Weiter sei
\begin{align}
	\abs{\gamma_{xy}}_Q = \sum\limits_{e \in \gamma_{xy}}\frac{1}{c(e)}
\end{align}
Weiter sei 
\begin{align}
	\kappa = \kappa(\Gamma) = \max\limits_{e} \sum\limits_{\gamma_{xy}}\abs{\gamma_{xy}}_Q.
\end{align}
Dann gilt 
\begin{satz}[Poincaré Ungleichung]
	\label{satz:poincare-ungleichung}
	In der obigen Situation gilt $\beta_1 \leq 1 - \frac{1}{\kappa}$ oder $\lambda_1 \geq \frac{1}{\kappa}$
\end{satz}
\begin{beweis}
	Für eine Funktion (Zufallsvariable) $f: V \to \RR$ gilt:
	\begin{align}
		\Var{f}_{\pi} &= \EW{f^2}_{\pi} - \enb{\EW{f}_{\pi}}^2 = \sum\limits_{x}f^2(x) \pi(x) - \enb{\sum\limits_{x}f(x) \pi(x) }^2 \\
				&=\frac{1}{2}\sum\limits_{x} f^2(x) \pi(x) + \frac{1}{2} \sum\limits_{y}f^2(y) \pi(y) - \enb{\sum\limits_{x}f(x) \pi(x) }\enb{\sum\limits_{y}f(y)\pi(y) } \\
				&= \frac{1}{2} \sum\limits_{x,y} \enb{f(x) - f(y)}^2 \pi(x) \pi(y) \\
				&= \frac{1}{2} \sum\limits_{x,y} \enb{\sum\limits_{e \in \gamma_{xy}} \sqrt{\frac{c(e)}{c(e)}}  f(e) }^2 \pi(x)\pi(y) \marginnote{Teleskopsumme. $f(e) = f(a) - f(b),  c(e) = c(a,b)$ für $e = (a,b)$} \\
				&\leq \frac{1}{2} \sum\limits_{x,y} \sum\limits_{e' \in \gamma_{xy}} \frac{1}{c(e')} \sum\limits_{e \in \gamma_{xy}} f^2(e) \pi(x) \pi(y) \\
				&= \frac{1}{2}\sum\limits_{e}c(e) f^2(e) \underbrace{\sum\limits_{\gamma_{xy \ni e}} \abs{\gamma_{xy}}_Q \pi(x) \pi(y)}_{\kappa} \\
				&\leq \frac{1}{2} \kappa \sum\limits_{e} c(e) f^2(e) \\
				&= \frac{1}{2} \kappa \sum\limits_{x,y} c(x,y) \enb{f(x) - f(y) }^2 = \kappa \mathcal{E}(f,f)
	\end{align}
	Mit \autoref{satz:11-20} folgt die Behauptung.
\end{beweis}

\underline{Ähnlich:} 
\begin{satz}
	\label{satz:11-23}
	Sei $\abs{\gamma_{xy}}$ in obiger Situation die Länge von $\gamma_{xy}$ und $k = \max_e \set{\frac{1}{c(e)} \sum\limits_{\gamma_{xy} \ni e} \abs{\gamma_{xy}} \pi(x) \pi(y) }$. Dann
	\begin{align}
		\beta_1 \leq 1- \frac{1}{k}
	\end{align}
\end{satz}
\begin{beweis}
	Sei $f: V \to \RR$.
	\begin{align}
		\Var{f}_{\pi} &= \frac{1}{2} \sum\limits_{x,y} \enb{\sum\limits_{e \in \gamma_{xy}} f(e) }^2\pi(x) \pi(y) \\
			&\leq \sum\limits_{e} f^2(e) \sum\limits{\gamma_{xy} \ni e} \abs{\gamma_{xy}}\pi(x)\pi(y) \marginnote{C.S.} \\
			&" \frac{1}{2} \sum\limits_{e} f^2(e) c(e) \sum\limits_{\gamma_{xy}} \frac{1}{c(e)} \abs{\gamma_{xy}} \pi(x) \pi(y)
	\end{align}
\end{beweis}
\begin{korollar}
	Für alle $\epsilon > 0$ gilt
	\begin{align}
		t_{mix} &\leq \log\enb{\frac{1}{2 \epsilon \pi_{min}}} \kappa(\Gamma) \\
		t_{mix} &\leq \log \enb{\frac{1}{2\epsilon\pi_{min}}} K
	\end{align}
\end{korollar}
\begin{beweis}
	folgt unmittelbar aus \autoref{satz:11-17}, \autoref{satz:poincare-ungleichung} und \autoref{satz:11-23}.
\end{beweis}
\begin{korollar}
	Für die einfache Irrfahrt auf $G$ gilt $\beta_1 \leq 1 - \frac{2\abs{E}}{d_*^2\gamma_* b}$. Dabei ist $d_* = \max\limits_x \deg(x), \gamma_* = \max\set{\abs{\gamma_{xy}} \given x,y \in V }$ und $b = \max\limits_{e} \abs{\set{\gamma \in \Gamma \given e \in  \gamma}}$.
\end{korollar}
\begin{beweis}
	Hier ist $Q(x,y) = \frac{1}{\deg (x)}$, falls $x \sim y$, sonst 0. Sei $\pi(x) = \frac{deg (x)}{2\abs{E}}$ und daher $c(e) = \frac{1}{2\abs{E}}$, für alle $e \in E$.
	\begin{align}
		\Rightarrow \kappa(\Gamma) = \max\limits_x \sum\limits_{\gamma_{xy}} \abs{\gamma_{xy}}_Q \pi(x)\pi(y) \leq \frac{d^2_*}{(2\abs{E})^2} 2 \abs{E} \gamma_* b \marginnote{$b$ ist entschiedende Größe, kluge Pfadwahl}
	\end{align}
	weil $\frac{1}{\epsilon(e)} = 2\abs{E}$ jeder Pfad höchstens $\gamma_*$ lang ist und jede Kante höchstens $b$ mal vorkommt folgt die Behauptung.
\end{beweis}
%
\marginnote{Vorlesungsbeginn 27.06.2016}
%\todo[inline]{weg lassen wiederhilung?}
%\underline{Poincaré:} Es genügt $1- \beta$ abzuschätzen; das hieß gelegentlich auch $\lambda_1$.
%\begin{align}
%	1 - \beta \geq \frac{1}{\kappa} \\
%	\kappa = \max\limits_e \sum\limits_{\gamma_{xy} \ni e} \abs{\gamma_{xy}}_Q \pi(x) \pi(y)
%\end{align}
%wobei
%\begin{align}
%	\Gamma 
%\end{align}
%\todo{missing part erklärung/wiederholung}
%Insbesondere für SRW auf einem zusammenhängenden Graphen
%\begin{align}
%	1- \beta_1 \geq \frac{2 \abs{E}}{d^2_*} && d_* = \max\limits_x \deg(x), \gamma_* := \max\limits
%\end{align}
%\todo[inline]{ende der wiederholung}
%
\begin{beispiel}
	Irrfahrt auf dem $p$-Kreis $\ZZ_p$. Sei $p$ ungerade und wir betrachten die gewöhnliche Irrfahrt auf $\ZZ_p$. Es ist $\pi(x) = \frac{1}{p} \forall x \in \ZZ_p$ das invariante Maß. \todo{missing figure} Außerdem gilt $d_* = 2$. Wir wählen (typisch für Simple Random Walk) $\gamma_{xy}$ als den kürzesten Pfad von $x$ nach $y$. Dann ist $\gamma_* = \frac{p-1}{2}$. Nun berechnen wir $b$. Aus Symmetriegründen wird jede Kante von gleich vielen Pfaden benutzt. Nehmen wir die Kante $(0,1)$. \todo{missing picture} $\gamma_{xy}$, die diese Kante benutzen haben $x$ \enquote{links} von $0$ und $y$ \enquote{rechts} von 1. Sei $d(x,0) = i$. Dann laufen alle Pfade von $\gamma_{xy}$ über $(0,1)$ mit $j \neq i \leq \frac{p-1}{2}$ mit $j = d(0,y)$ über $(0,1)$. Das heißt für ein festes $i$ gibt es $\frac{p-1}{2} - i$ solche Pfade ($i = 0, \dots, \frac{p-3}{2}$). Also ist 
	\begin{align}
		b = \sum\limits_{i = 0}^{\frac{p-3}{2}} \frac{p-1}{2} -i = \enb{\frac{p-1}{2}}^2 - \enb{\frac{p-3}{2} - \frac{p-1}{2} \frac{1}{2}} = \frac{p^2-1}{8}
	\end{align}
	Also ist nach unserer Abschätzung:
	\begin{align}
		\beta_1 \leq 1-\frac{2p}{4 \frac{p-1}{2} \frac{p^2-1}{2} } = 1 - \frac{8p}{\enb{p-1}^2 (p+1) }
	\end{align}
	Wie gut ist das? Mit Darstellungstheorem kann man in diesem Fall die $(\beta_j)$ persönlich ausrechnen:
	\begin{align}
		\beta_j = \cos(\frac{2\pi}{p}j), j = 0, \dots, p-1
	\end{align}
	d.h.
	\begin{align}
		\beta_1 = \cos(\frac{2\pi}{p}) \approx 1 - \frac{2 \pi^2}{p^2} + \mathcal{O}\enb{\frac{1}{p^3}}
	\end{align}
	für große $p$. Unsere Abschätzung $\beta_1 = 1 - \frac{8}{p^2}\dots$ das ist brillant. Wir haben die richtige Ordnung und fast die richtige Konstante.
\end{beispiel}
\todo[inline]{mathbb undefined control sequence in title...}
\begin{beispiel}[Irrfahrt auf dem Torus $Z_2^d$]
	\marginnote{$\ZZ^d_2 = \set{0,1}^d$} Wir berechnen: $\abs{E} = d 2^d, d_* = d$ und wir wählen $\gamma_{xy}$ wie folgt: Wenn $x \neq y$, z.B. $d_H(x,y) = k$, dann gibt es $i_1 < \dots < i_k$, sodass $x_{i_j} \neq y_{i_j}, \forall j = 1, \dots, k$ und in allen anderen Koordinaten sind $x$ und $y$ gleich. $\gamma_{xy}$ besteht nun darin, dass man sukzessiv $x_{i_1} \to y_{i_1}, x(i_2) \to y_{i_2}, \dots$ usw. verändert.b Dann ist $\gamma_* = d$.

	Nun zu $b$. Sei $e = (w,z) \in E$. Da bedeutet, dass $w_i = z_i$ für alle $i$ bis auf eines, beispielsweise das $j$'te. Die Kante $e$ verbindet alle $x$ und $y$, sodass $x$ mit $w$ in den Koordinaten $j,j+1,\dots,d$ übereinstimmt und $y$ mit $z$ in der Koordinaten $1, \dots, j$. Beispiel:
	\begin{gather}
		w = (0,0,0,0,0), z = (0,0,1,0,0), \text{ ist } x = (1,0,0,1,0), y = (0,0,1,0,1) \\
		x\to y: (0,0,0,1,0)
	\end{gather}
	Also gibt es für die Kante $b$ $2^{j-1} 2^{d-j} = 2^{d-1}$ Möglichkeiten. \underline{Also:}
	\begin{align}
		\beta_1 \leq 1 . \frac{d2^{d}}{d^2 \cdot d \cdot d^{d-1}} = 1 - \frac{2}{d^2}
	\end{align}
	Der wahre Wert ist $\beta_1 = 1 - \frac{2}{d}$. 
	\begin{bemerkung}
		Mann kann sich fragen, ob das an unserer Pfadwahl liegt.
		\begin{enumerate}
			\item Nein, weil alle Kanten gleich oft benutzt werden. 
			\item Nein, denn es gibt zu jedem Punkt $v \in \ZZ^d_2$ Punkte im Abstand $i$. Um $v$ mit solch einem Punkt zu verbinden, muss man also mindestens einen Pfad der Länge $i$ wählen. Das ergibt insgesamt $2^d \sum_{i = 1}^{d} i \cdot (d nCr i) = 2^d \cdot d \cdot 2^{d-1}$ Kanten. Da $\abs{E} = d 2^d$ muss man mindestens eine Kante $2^{d-1}$-mal benutzen.
		\end{enumerate}
	\end{bemerkung}
\end{beispiel}

\begin{beispiel}[Binärbaum]
	Sei $T = (V,E)$ der Binärbaum mit Wurzel $v^*$ der Tiefe $d$. Betrachte wieder einen Simple Random Walk. Für $x \neq y$ wähle $\gamma_{xy}$ als den direkten Pfad von $x$ nach $y$. Dann:
	\begin{align}
		\abs{E} = \abs{V} -1 = 2^{d+1} - 2, d* = 3, \gamma_* = 2d
	\end{align}
	Offenbar werden $(v^*,v_l)$ und $(v^*,v_r)$ am meisten benutzt \marginnote{$v_r,v_l$ die Kinder links bzw. rechts von $v^*$} und zwar $2^d \cdot \enb{2^d-1}$ mal (Alle Knoten (links + Wurzel) mal alle Knoten (rechts + Wurzel))
	\todo[inline]{missing figure tree}
	\begin{align}
		b = 2^d(2^d-1) \Rightarrow \beta_1 \leq 1- \frac{2(2^{d+1} -2)}{q \cdot 2d \cdot 2^d(2^d-1)}) = \frac{2}{q d \cdot 2^d} = \frac{1}{q d 2^{d-1}}
	\end{align}
	Tatsächlich $\beta_1 = 1- \frac{1+ \mathcal{o}(1)}{2^{d+1}}$. Also ist unsere Abschätzung um eine Ordnung $d$ verkehrt. Das ist trotzdem nicht so schlecht, wenn man bedenkt, dass die Spektrallücke exponentiell klein ist.	
\end{beispiel}
	\todo[inline]{missing figure stern}
\begin{beispiel}[Der Stern]

	Sei $G = (V,E)$ der Stern mit $n$ Spitzen und Zentrum $v^*$. Betrachte wieder SRW. Man kann das Spektrum der einfachen Irrfahrt ausrechnen:
	\begin{align}
		\beta_0  = 1 && \beta_1 = \dots = \beta_n = 0 &&  \beta_{n+1} = -1
	\end{align}
	Für die faule Irrfahrt ist $\beta_0, \beta_1 = \dots = \beta_n = \frac{1}{2}, \beta_{max} = 0$. Die Irrfahrt mischt sehr schnell: $t_{rel} = 2$ Es ist
	\begin{align}
		\abs{E} = n && d_* = n && \gamma_* = 2 && b = n 
	\end{align}
	Daraus folgt
	\begin{align}
		1 - \beta_1 \geq \frac{2n}{2n^3}= \frac{1}{n^2}
	\end{align}
	Das ist für großes $n$ eine Katastrophe. Übung: Poincaré ist unschuldig.
\end{beispiel}

\marginnote{Vorlesungsbeginn 30.06.2016}

\begin{satz}[Cheeger-Ungleichung]
	\label{satz:Cheeger}
	Sei $Q$ die Übergangsmatrix einer ergodischen Markovkette auf $V$ (endlich) und $\pi$ sei das reversible Wahrscheinlichkeitsmaß zu $Q$. Sei wieder 
	\begin{align}
		h = \min\limits_{S:\pi(S)<1/2} \set{\frac{C(S,S^c)}{\pi(S)}} && \text{mit } C(S,S^C) = \sum\limits_{x \in S, y \notin S} c(x,y) = \sum\limits_{x \in S, y \notin S} \pi(x) Q(x,y)
	\end{align}
	Dann gilt:
	\begin{align}
		1-2h \leq \beta_1 \leq 1-\frac{h^2}{2} && \beta_1 \text{ist 2. größter Eigenwert}
	\end{align}
\end{satz}
\begin{beweis}
	Setze $\lambda_1 := 1 - \beta_1$ und zeige $\frac{h^2}{2} \leq \lambda_1 \leq 2h$. Sei $S$ mit $0 < \pi(S) \leq \frac{1}{2}$. Setze
	\begin{align}
		\Psi_S(x) = \begin{cases}
						\pi(S^C, & x\in S) \\
						\pi(S), & x \notin S
					\end{cases}
	\end{align}
	Es gilt $\EW{\Psi_S} = 0$. \underline{Also:}
	\begin{align}
		\lambda_1 \leq \frac{\mathcal{E}(\Psi_S,\Psi_S)}{\Var{\Psi_S}} &= \frac{\frac{1}{2}\sum\limits_{x,y}\enb{\Psi_S(x) - \Psi_S(y)}^2 c(x,y)}{\frac{1}{2} \sum\limits_{x,y} \enb{\Psi_S(x - \Psi_S(y))}} \\
							&= \frac{\sum\limits_{x\in S} \sum\limits_{y \in S} 1 c(x,y) }{\sum\limits_{x \in S}\sum\limits_{y \in S} 1 \pi(x) \pi(y) } = \frac{C(S,S^c)}{\pi(S) \pi(S^c)} \leq \frac{2C(S,S^c)}{\pi(S)}
	\end{align}

	\underline{Infimum bilden:} $\lambda_1 \leq 2h$ \\
	U.S: \underline{1. Beobachtung:} $\Psi \in L^2(\pi), \Psi^+ = \max(\Psi,0),	S(\Psi) = \set{ x : \Psi (x) > 0 }$. Dann gilt $\forall S(\Psi) \neq 0$ und alle $\lambda \geq 0$
	\begin{align}
		\lambda \Norm{\Psi^+}^2_{L^2(\pi)} \geq \mathcal{E} (\Psi^+,\Psi^+)
	\end{align}
	falls $L\Psi := (\id - Q) \Psi \leq \lambda\Psi$ auf $S(\Psi) \ (*)$. Denn
	\begin{align}
		\lambda \Norm{\Psi^+}_{L^2(\pi)} = \sprod{\lambda\Psi^+, \Psi^+}_{\pi} \overset{(*)}{\geq} \sprod{L_{\Psi^+}, \Psi^+} = \mathcal{E} (\Psi^+,\Psi^+)
	\end{align}

	\underline{2. Beobachtung:} $\forall \Psi \in L^2(\pi)$ mit $S(\Psi) \neq \emptyset$ gilt
	\begin{align}
		\mathcal{E} (\Psi^+,\Psi^+) \geq \frac{h^2(\Psi) \Norm{\Psi}^2_{L^2(\pi)}}{2} \\
		\text{wo } h(\Psi) = \inf \set{ \frac{C(S,S^)}{\pi(S)} - t, \emptyset \neq S \subseteq S(\Psi)  }
	\end{align}
	denn einerseits: O.B.d.A. $\Psi \geq 0$ überall, dann gilt
	
	\begin{align}
		\sum\limits_{x,y} \abs{\Psi^2(x) - \Psi^2(y)} c(x,y) &= \sum\limits_{x,y} \abs{\Psi(x) - \Psi(y)} \abs{\Psi(x) - \Psi(y)} \sqrt{c(x,y)} \sqrt{c(x,y)} \\
		&\leq \sqrt{2} \mathcal{E}^{1/2} (\Psi,\Psi) \enb{\sum\limits_{x,y}\enb{\Psi(x) + \Psi(y)}^2 c(x,y) }^{1/2} \marginnote{Siehe Nebenrechnung}\\
		&\leq 2 \mathcal{E}^{1/2}(\Psi,\Psi) \enb{\sum\limits_{x,y} \enb{\Psi^2(x) + \Psi^2(y) } c(x,y) }^{1/2} \\
		&\leq 2^{3/2} \mathcal{E}^{1/2} (\Psi,\Psi) \Norm{ \Psi }_{L^2(\pi)}
	\end{align}
	
	Nebenrechnung: \todo{nebenrechnung unsicher}
	\begin{align}
		\Psi^2(x) + \Psi^2(y) c(x,y) = \sum\limits_{x,y} \Psi^2(x) \pi(x) Q(x,y) + \sum\limits_{x,y} \Psi^2(y) \pi(x) Q(x,y) = 2 \Norm{\Psi^2}_{L^2(\pi)}
	\end{align}
	
	\underline{Andererseits:} 
	\begin{align}
		\sum\limits_{x,y} \abs{\Psi^2(x) - \Psi^2(y)}c(x,y) &= 2 \sum\limits_{\substack{x,y \\ \Psi(y) > \Psi(x)}} \enb{\Psi^2(y) - \Psi^2(x)} c(x,y) = 4 \sum\limits_{\substack{x,y \\ \Psi(y) > \Psi(x)}} \int_{\Psi(x)}^{\Psi(y)} t \diff t \cdot c(x,y) \\
		&= \marginnote{Fubini} 4 \int_{0}^{\infty} t\enb{\sum\limits_{\Psi(x) \leq t \leq \Psi(y)} c(x,y) } \diff t \\
		&= 4 \int_{0}^{\infty} t C(S,S^c) \diff t = 4 \int_{0}^{\infty} t \frac{C(S,S^c)}{\pi(S)} \pi(S) \diff t \marginnote{Sei $S := \set{y \given \Psi(y) > t} \subseteq S(\Psi)$} \\
		&\geq 4h(\Psi) \int_{0}^{\infty} t \pi(y : \Psi(y) > t) \diff t \\
		&= 4 \frac{h(\Psi)}{2} \Norm{ \Psi }_{L^2(\pi)}
	\end{align}		

	Zusammen: 
	\begin{align}
		2^{3/2} \mathcal{E}^{1/2}(\Psi,\Psi) \Norm{\Psi}_{L^2(\pi)} \geq \sum\limits_{x,y} \abs{\Psi^2(x) - \Psi^2(y)} c(x,y) \geq 2 h(\Psi) \Norm{\Psi}^2_{L^2(\pi)}
	\end{align}
	Zusammen mit \autoref{satz:6-4}
	\begin{gather}
		\lambda \Norm{\Psi_+}^2_{L^2(\pi)} \geq \mathcal{E}(\Psi^+,\Psi^+) \frac{h^2(\Psi)}{2} \Norm{\Psi_+}^2_{L^2(\pi)} \\
		\Rightarrow \lambda \geq \frac{h^2(\Psi)}{2}
	\end{gather}	
	wobei $h = \inf\limits_{S: \pi(S) \leq 1/2} \frac{C(S,S^c)}{\pi(S)}$. \\
	Das betrachten wir für $\lambda = \lambda_1$. Sei $\Psi$ eine Eigenfunktion von $L$ zu $\lambda_1$. Dann stimmt $(*)$ ($L\Psi \leq \lambda\Psi $), d.h. 
	\begin{align}
		\lambda_1 \geq \frac{h^2(\Psi)}{2} \geq \frac{h^2}{2} &&  h^2 = \inf\limits_{S: \pi(S) \leq \frac{1}{2}} \frac{C(S,S^c)}{\pi(S)} && h^2(\Psi) = \inf\limits_{S \subseteq S(\Psi)} \frac{C(S,S^c)}{\pi(S)}
	\end{align}
	solange $\pi\Big(S(\Psi)\Big) \leq \frac{1}{2}$ geht alles gut, denn dann ist das Infimum in $h(\Psi)$ über weniger Mengen $S$ als das in $h$. Nun ist $\EW{\Psi}_{\pi} = \sprod{\Psi, \mathds{1}}_{\pi} = 0$, denn $\Psi \perp \mathds{1}$, d.h. wenn $S(\Psi)$ so sein sollte, dass $\pi(S(\Psi)) > \frac{1}{2}$, dann gehen wir zu $\Psi$ über und erhalten $\pi(S(-\Psi)) \leq \frac{1}{2}$. Somit sind wir fertig.
\end{beweis}
\todo[inline]{undefined Control sequence titel $Z_p$ und $Z^d_2$}
\begin{beispiel}%[$\ZZ_p$]
	Sei $p$ ungerade und $\ZZ_p$ der $p$-Kreis. $\pi(x) = \frac{1}{p}, \forall x$, $h$ wird angenommen, wenn man für $S$ ein Intervall der Länge $\frac{p-1}{2}$ wählt. Dann gilt $h = \frac{2}{p-1}$. Also folgt mit Cheeger (\autoref{satz:Cheeger})
	\begin{align}
	1 - \frac{\Psi}{p-1} \leq p_1 \leq 1 - \frac{2}{\enb{p-1}^2}
	\end{align}
	\underline{Erinnerung:} $\beta_1 = \cos(\frac{2\pi}{p}) = 1 - \frac{2\pi^2}{p^2} + \mathcal{O}\enb{\frac{1}{p^4}}$. Hier ist für $p$ groß die untere Schranke schlecht und die obere Schranke bis auf einen Faktor $\pi^2$ gut.
\end{beispiel}

\begin{beispiel}%[$Z^d_2$]
	Wir betrachten $\ZZ^d_2$. Sei $V = \set{0,1}^d, \pi(x) = \frac{1}{2^d}, \forall x$.$h$ wird angenommen für die $S$, welche genau eine Koordinate festhalten, z.B.
	\begin{align}
		S = \set{x \given x_1 = 0} && \pi(S) = \frac{1}{2}
	\end{align}
	\begin{align}
		h = \frac{C(S,S^c)}{\pi(S)} = 2 \sum\limits_{x \in S, y \notin S} c(x,y) = \frac{2 \abs{S} }{2 \abs{E}} = \frac{2 \cdot 2^{d-1}}{2 (d/2) 2^d} = \frac{1}{d} \marginnote{SRW hat $c(x,y) = \frac{1}{2\abs{E}}$}
	\end{align}
	Es gilt $p_1 =  1 - \frac{2}{d}$ und wir erhalten mit Cheeger (\autoref{satz:Cheeger}):
	\begin{align}
		1 - \frac{2}{d} \leq p_1 \leq 1 - \frac{1}{2d^2}
	\end{align}
	Die untere Schranke ist perfekt, während die obere Schranke um die Ordnung $d$ falsch ist (wie bei Pointcaré).	
\end{beispiel}

\begin{beispiel}[Binärbäume]
	
	Sei $T$ der Binärbaum mit Wurzel $v^*$ und der Tiefe $d$. \todo{missing Tree?} Wähle als $S$ die linken oder rechten Nachfolger von $v^*$ ohne die Wurzel. Dann gilt $\pi(S) = \frac{2^d-1}{2^{d+1}-1}$ Man rechnet aus
	\begin{align}
		h = \frac{1}{4 \cdot 2^{d-1} - 1} = \frac{1}{2\abs{E}} = \frac{1}{(2d^{d+2}) -4 } \marginnote{\enquote{Das ist noch eins der Dinge, die ich ganz gut kann: 2 mal 2}, Löwe}
	\end{align}
	mit Cheeger (\autoref{satz:Cheeger})
	\begin{align}
		1 - \frac{1}{2^{d+1} -2} \leq \beta^1 \leq 1- \frac{1}{2} \enb{\frac{1}{2^{d+2} -4 }}^2
	\end{align}
	Der wahre Wert für $d$ groß ist $1 - \frac{1 + o(1)}{2^{d-2}}$. Pointcaré: $1- \frac{const}{d2^d}$
\end{beispiel}

\begin{beispiel}[Stern]
	Sei $Q$ die Übergangsmatrix für einen Random Walk auf dem $n$-Stern ($n$ Spitzen) mit holding $\Theta$. Man rechnet aus $h = 1 - \Theta$, $2 \Theta - 1 \leq \beta_1 \leq 1 - \frac{(1- \Theta)^2}{2}$. Z.B. für $\Theta = \frac{1}{2}$  $\beta_1 \leq \frac{7}{8}$. (Der wahre Wert $\beta_1 = \frac{1}{2}$)
\end{beispiel}
